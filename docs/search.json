[
  {
    "objectID": "syllabus.html#class-meetings",
    "href": "syllabus.html#class-meetings",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Class Meetings",
    "text": "Class Meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nLSN 111\nMon & Wed 3:30 - 4:45pm"
  },
  {
    "objectID": "syllabus.html#teaching-team",
    "href": "syllabus.html#teaching-team",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nSergio Rey\nThu 3:30 - 4:30pm (by appointment)\nPSFA 361G\n\n\nJin Huang\nFri 10:30am\nPSFA 361F"
  },
  {
    "objectID": "syllabus.html#introduction",
    "href": "syllabus.html#introduction",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Introduction",
    "text": "Introduction\nWelcome to 385: Spatial Data Analysis!\nThe purpose of this course is to introduce you to methods of spatial data analysis. The focus is on both the conceptual and applied aspects of spatial statistical methods. We will place particular emphasis on the computational aspects of Exploratory Spatial Data Analysis (ESDA) methods for diﬀerent types of spatial data including point processes, lattice data, geostatistical data, network data, and spatial interaction. Throughout the course you will gain valuable hands-on experience with several specialized software packages for spatial data analysis. The overriding goal of the course is for you to acquire familiarity with the fundamental methodological and operational issues in the statistical analysis of geographic information and the ability to extend these methods in your own research.\nThe course takes an explicitly computational thinking approach to its pedagogy. Students are introduced to computational concepts and tools that are increasingly important to research that engages with geospatial data. By adopting these tools, students acquire a deeper engagement with, and mastery of, the substantive concepts. Put differently, students will learn to code. But this is a means to the end goal: students will code to learn spatial data analysis.\nIn the scope of a 15-week semester course we can only introduce a handful of the key concepts and methods relevant to the field of spatial data analysis. As such, the course is not intended as an exhaustive treatment. Instead, the goal is that students will acquire an understanding of the more common and useful methods and practices, and use the course as an entry point for further engagement with the field."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nGEOG 101 or GEOG 102\nSTAT 250 or comparable course in statistics.\n\nAll students are required to complete the prerequisite assessment quiz before 2023-08-23 3:00pm."
  },
  {
    "objectID": "syllabus.html#computational-learning",
    "href": "syllabus.html#computational-learning",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Computational Learning",
    "text": "Computational Learning\nWe will be using open source geospatial software throughout the course together with Jupyter Notebooks, and Python as our scripting language.\nAll software for the course will be made available through JupyterHub a web-based framework. Students wishing to install these materials on their own machines will be given instructions to do so, but this is not required."
  },
  {
    "objectID": "syllabus.html#readings",
    "href": "syllabus.html#readings",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Readings",
    "text": "Readings\nAll required readings are available through the links listed below. Assigned readings should be completed before the date listed in the schedule (see below). Readings are a critical part of the discussions we will hold in class, and therefore being prepared for class means having completed the readings and thought about the content. It will be difficult to do well in this course without having completed the readings.\n\n\n\nAbbrevation\nSource\n\n\n\n\nGDA\nTenkanen, H., V. Heikinheimo, D. Whipp (2023) Python for Geographic Data Analysis. CRC Press.\n\n\nGDS\nRey, S.J., D. Arribas-Bel, L.J. Wolf (2023) Geographic Data Science with Python. CRC Press."
  },
  {
    "objectID": "syllabus.html#schedule-planned",
    "href": "syllabus.html#schedule-planned",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)\n\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopic\nReading\nDue\n\n\n\n\n1\nAug-21\nCourse Introduction\n\n\n\n\n\nAug-23\nJupyter Hub\nGDA 1 GDS 2\n\n\n\n2\nAug-28\nPython: Programming Concepts\nGDA 2\nQuiz 1\n\n\n\nAug-30\nPython: Scripting\nGDA 2\n\n\n\n3\nSep-04\nLabor Day Holiday\n\n\n\n\n\nSep-06\nPython: Data Analysis/Visualization\nGDA 3,4\n\n\n\n4\nSep-11\nPython: Geographic Data\nGDA 5\nQuiz 2\n\n\n\nSep-13\nGeopandas\nGDA 6\nExercise 1\n\n\n5\nSep-18\nPySAL\nGDS 3\nQuiz 3\n\n\n\nSep-20\nGeoVisualization\nGDS 5\n\n\n\n6\nSep-25\nSpatial Weights\nGDS 4\nQuiz 4\n\n\n\nSep-27\nSpatial Dependence\nGDS 6\n\n\n\n7\nOct-02\nGlobal Autocorrelation\nGDS 6\nQuiz 5\n\n\n\nOct-04\nGlobal Autocorrelation Tests\nGDS 6\n\n\n\n8\nOct-09\nLocal Autocorrelation\nGDS 7\nQuiz 6\n\n\n\nOct-11\nLocal Autocorrelation Tests\nGDS 7\nExercise 2\n\n\n9\nOct-16\nPoint Pattern Data\nGDS 8\nQuiz 7\n\n\n\nOct-18\nCentrography\nGDS 8\n\n\n\n10\nOct-23\nPoint Processes\nGDS 8\nQuiz 8\n\n\n\nOct-25\nQuadrat Statistics\nGDS 8\n\n\n\n11\nOct-30\nNearest Neighbor Statistics\nGDS 8\nQuiz 9\n\n\n\nNov-01\nDistance Based Statistics\nGDS 8\n\n\n\n12\nNov-06\nGeostatistical Data\nTBA\nQuiz 10\n\n\n\nNov-08\nSpatial Interpolation\nTBA\nExercise 3\n\n\n13\nNov-13\nKriging\nTBA\nQuiz 11\n\n\n\nNov-15\nNetwork Data\nTBA\n\n\n\n14\nNov-20\nSpatial Interaction Data\nTBA\nQuiz 12\n\n\n\nNov-22\nGravity Models\nTBA\n\n\n\n15\nNov-27\nMeasuring Spatial Disparities\nTBA\nQuiz 13\n\n\n\nNov-29\nSpatial Segregation\nTBA\n\n\n\n16\nDec-04\nChange of Support Problems\nTBA\nQuiz 14\n\n\n\nDec-06\nNext Steps in SDA\n\nExercise 4\n\n\n17\nDec-11\nFinal Review\n\nQuiz 15\n\n\n18\nDec-18\nFinal Exam (13:00-15:00)"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Grading",
    "text": "Grading\nGEOG385 uses specification grading in evaluating student work and in determining your final course grade. Your course grade will be based on the quality and quantity of the work that you submit that is evaluated to be of an acceptable level of quality. The acceptable level of quality demonstrates competency in the concepts and methods covered in the course.\nThere is a two-step process for determination of your final course grade at the end of the quarter:\n\nUsing your quizzes and exercises, your base grade is determined.\nUsing your final exam results, determine if your base grade includes a \"plus\", \"minus\", or level drop to form the course grade.\n\nFor Step 1, the base grade is determined using the following specification:\n\n\n\nLevel\nHurdles\n\n\n\n\nA\nPass at least 13 of 15 quizzes and earn \"Demonstrates Competency\" on 4 of 4 exercises,\n\n\nB\nPass at least 11 of 15 quizzes and earn \"Demonstrates Competency\" on 3 of 4 exercises\n\n\nC\nPass at least 9 of 15 quizzes and earn \"Demonstrates Competency\" on 2 of 4 exercises\n\n\nD\nPass at least 7 of 15 quizzes and earn \"Demonstrates Competency\" on 1 of 4 exercises\n\n\nF\nFail to clear D-level hurdles\n\n\n\nFor Step 2, your final course grade is determined as follows:\n\nIf you earn at least 85% on the final exam, you will obtain a “+” for your grade. So a B base grade would become a B+ course grade, and so on (Note: SDSU does not record A+ grades).\nIf you score between 70-85% on the final exam, your base grade becomes your course grade.\nIf you score between 50% and 69% on the final exam, you will obtain a “-” for your grade. So an A base grade becomes an A- course grade, a B base grade becomes a B- course grade, and so on.\nIf you score less than 50% on the final exam, your course grade will drop one level: An A base grade becomes a final B course grade."
  },
  {
    "objectID": "syllabus.html#quizzes",
    "href": "syllabus.html#quizzes",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Quizzes",
    "text": "Quizzes\nQuizzes are graded on a pass/fail basis. Starting in week two, there will be a quiz due before a session that pertains to the background reading that is required before our work in class."
  },
  {
    "objectID": "syllabus.html#exercises",
    "href": "syllabus.html#exercises",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Exercises",
    "text": "Exercises\nFour exercises will be introduced in class and are to be completed outside of class meetings.\nEach exercise is graded using a CRN rubric that classifies work with marks of C (\"Demonstrates Competence\"), R (\"Needs Revision\"), or N (\"Not assessable\"):\nOf each exercise the following questions will be asked: Does the work demonstrate that the student understands the concepts? Does the work demonstrate competence and meet the expectations outlined in the exercise?\nIf the answer is \"yes\" to both of the questions, a student passes the hurdle for that exercise.\nIf the initial submission does not clear the hurdle, then a second question is asked: Is there evidence of partial understanding of the concepts? If the answer to this question is \"Yes\" the student can exchange one token to attempt a revision of their work. If the answer is \"No\", the student does not clear the hurdle for this exercise and will not have the opportunity to revise their work."
  },
  {
    "objectID": "syllabus.html#final-exam",
    "href": "syllabus.html#final-exam",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Final Exam",
    "text": "Final Exam\nA closed book, closed note, timed final exam will be given on December 18 (13:00-15:00). The exam will be based on a blend of previous quiz questions and additional questions that pertain to material covered in class."
  },
  {
    "objectID": "syllabus.html#tokens",
    "href": "syllabus.html#tokens",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Tokens",
    "text": "Tokens\nEach student is provided with three tokens at the beginning of the semester.\nUsing Tokens\n\nOne token can be used for a one-day extension for an exercise.\nOne token can be used to revise an exercise that was submitted on-time but evaluated as \"Needing Revision\".\nTwo tokens can be used to request a make-up date for the final exam. (Requests required by 2023-11-18 17:00.)\n\nRemaining Tokens\nEach token that remains unused after 2023-12-18 will be counted as a passed quiz. Tokens cannot be exchanged with other students."
  },
  {
    "objectID": "syllabus.html#policies",
    "href": "syllabus.html#policies",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Policies",
    "text": "Policies\n\nAccommodations\nIf you are a student with a disability and are in need of accommodations for this class, please contact Student Ability Success Center at (619) 594-6473 as soon as possible. Please know accommodations are not retroactive, and I cannot provide accommodations based upon disability until I have received an accommodation letter from Student Ability Success Center.\n\n\nPrivacy and Intellectual Property\nStudent Privacy and Intellectual Property: The Family Educational Rights and Privacy Act (FERPA) mandates the protection of student information, including contact information, grades, and graded assignments. I will use Canvas to communicate with you, and I will not post grades or leave graded assignments in public places. Students will be notified at the time of an assignment if copies of student work will be retained beyond the end of the semester or used as examples for future students or the wider public. Students maintain intellectual property rights to work products they create as part of this course unless they are formally notified otherwise.\n\n\nAcademic Integrity\nThe SDSU student academic integrity policy lists violations in detail. These violations fall into eight broad areas that include but are not limited to: cheating, fabrication, plagiarism, facilitating academic misconduct, unauthorized collaboration, interference or sabotage, non-compliance with research regulations and retaliation. For more information about the SDSU student academic integrity policy, please see the following: https://sacd.sdsu.edu/student-rights/academic-dishonesty.\n\n\nCode of Conduct\nAs course instructor, I am dedicated to providing a harassment-free learning experience for all students, regardless of gender, sexual orientation, disability, physical appearance, body size, race, religion, or choice of operating system. All course participants are expected to show respect and courtesy to other students throughout the semester. As a learning community we do not tolerate harassment of participants in any form.\n\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery are not appropriate in this course.\nBe kind to others. Do not insult or put down other students. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for this course.\nStudents violating these rules may be asked to leave the course, and their violations will be reported to the SDSU administration.\n\nThis code of conduct is an adaptation of the SciPy 2018 Code of Conduct."
  },
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "",
    "text": "Week 01\n\n08-21 Course Introduction\n08-23 Jupyter Introduction (ipynb)\n\nWeek 02\n\n08-28 Python Introduction (ipynb)\n08-30 Functions and Scripts (ipynb) (temp_converter.py)\n\nWeek 03\n\n09-06 Python Introduction to Data Analysis (ipynb)\n\nWeek 04\n\n09-11 Spatial Data\n09-13 GeoPandas (ipynb)\n\nWeek 05\n\n09-18 GeoPandas Spatial Queries (ipynb)\n09-20 Geovisualization (ipynb)\n\nWeek 06\n\n09-25 Spatial Weights (ipynb)"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#data-definitions",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#data-definitions",
    "title": "Spatial Data",
    "section": "Data Definitions",
    "text": "Data Definitions\n\nfacts and statistics collected together for reference or analysis\n\n\nthe quantities, characters, or symbols on which operations are performed by a computer, being stored and transmitted in the form of electrical signals and recorded on magnetic, optical, or mechanical recording media.\n\n\nthings known or assumed as facts, making the basis of reasoning or calculate\n\nSource: Oxford languages"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#datas-place",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#datas-place",
    "title": "Spatial Data",
    "section": "Data’s Place",
    "text": "Data’s Place\n\n\n\n\n\nDIKW Pyramid\n\n\n\n\ndata: discrete facts, unorganized and lacking context or information\ninformation: data imbued with meaning - what is in the data\nknowledge: perception of the world seen through information synthesis\nwisdom: “knowing the right things to do”"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#data-sets",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#data-sets",
    "title": "Spatial Data",
    "section": "Data Sets",
    "text": "Data Sets\nA data set is a collection of observations recorded for individual units on a set of variables.\nVariables are sometimes referred to as attributes or features (in machine learning parlance)."
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#measurement-scales",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#measurement-scales",
    "title": "Spatial Data",
    "section": "Measurement Scales",
    "text": "Measurement Scales\n\n\n\nScale\nOperations\nExample\n\n\n\n\nnominal\nmode, frequencies\nZip Code\n\n\nordinal\nA > B\nRanks, Primary, Intermediate\n\n\ninterval\n+ -\nTime\n\n\nratio\n+ - * /\nWeight, Kelvin"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-data-is-special",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-data-is-special",
    "title": "Spatial Data",
    "section": "Spatial Data is Special",
    "text": "Spatial Data is Special\n\nSpatial data comes in many varieties and it is not easy to arrive at a system of classification that is simultaneously exclusive, exhaustive, imaginative, and satisfying.\n\n– G. Upton & B. Fingleton"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#what-is-special-about-spatial-data-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#what-is-special-about-spatial-data-1",
    "title": "Spatial Data",
    "section": "What is special about spatial data?",
    "text": "What is special about spatial data?\nLocation, Location, Location\nwhere matters\nDependence is the rule, not the exception\n\nspatial interaction, contagion, spill-overs\nspatial externalities\n\nSpatial Scale\n\nInference can change with scale"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#nature-of-spatial-data",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#nature-of-spatial-data",
    "title": "Spatial Data",
    "section": "Nature of Spatial Data",
    "text": "Nature of Spatial Data\nGeoreferences\nattribute data together with location\nGeocoding\n\nassociate observations with location\npoint: latitude-longtitude (GPS)\nareal unit: spatial reference"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geocoding-on-line",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geocoding-on-line",
    "title": "Spatial Data",
    "section": "Geocoding on-line",
    "text": "Geocoding on-line\n\nGeocode Input"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geocoding-on-line-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geocoding-on-line-1",
    "title": "Spatial Data",
    "section": "Geocoding on-line",
    "text": "Geocoding on-line\n\nGeocode Output"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#on-the-map",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#on-the-map",
    "title": "Spatial Data",
    "section": "On the Map?",
    "text": "On the Map?\n\nMap of Geocode Output"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#on-the-map-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#on-the-map-1",
    "title": "Spatial Data",
    "section": "On the Map?",
    "text": "On the Map?\n\nErrors in Geocode Output"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#location",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#location",
    "title": "Spatial Data",
    "section": "Location",
    "text": "Location\n\nGiven: in most spatial data analysis, no choice in location\nNo sampling in the usual sense\nData = attributes augmented with locational information"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-effects",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-effects",
    "title": "Spatial Data",
    "section": "Spatial Effects",
    "text": "Spatial Effects\nThe Trilogy\n\nSpatial Dependence\nSpatial Heterogeneity\nSpatial Scale"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-dependence",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-dependence",
    "title": "Spatial Data",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nTobler’s First Law of Geography\n\n“everything depends on everything else, but closer things more so”\n\n\nStructure of spatial dependence\nDistance Decay\nCloseness = Similarity"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-heterogenety",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-heterogenety",
    "title": "Spatial Data",
    "section": "Spatial Heterogenety",
    "text": "Spatial Heterogenety\nSpatial Instability\n\nProcess varies in some way over spatial units\nMultiple forms\n\nDiscrete = regimes\nContinuous = expansion method, GWR\n\nTrade-off\n\nSpatial homogeneity = stationary process\nUniqueness = extreme heterogeneity"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-scale-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-scale-1",
    "title": "Spatial Data",
    "section": "Spatial Scale",
    "text": "Spatial Scale\nMismatch\n\nSpatial scale of the process\nSpatial scale of our measurement\n\nIssues\n\npoints too far apart = miss small distance variation\narea aggregates cannot provide information on individual behavior\nEcological Fallacy"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#modifiable-areal-unit-problem-maup",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#modifiable-areal-unit-problem-maup",
    "title": "Spatial Data",
    "section": "Modifiable Areal Unit Problem (MAUP)",
    "text": "Modifiable Areal Unit Problem (MAUP)\nAggregation Problem\n\nspecial case of ecological fallacy\na million correlation coefficients\n\nZonation Problem\n\nsize\narangement\nHow many ways could you partition the coterminous US land area into 48 polygons?"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#maup-zonation-problem",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#maup-zonation-problem",
    "title": "Spatial Data",
    "section": "MAUP Zonation Problem",
    "text": "MAUP Zonation Problem\n\nhttp://en.wikipedia.org/wiki/Modifiable_areal_unit_problem"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#maup-aggregation-problem",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#maup-aggregation-problem",
    "title": "Spatial Data",
    "section": "MAUP Aggregation Problem",
    "text": "MAUP Aggregation Problem\n\n\n\n\n\nhttp://en.wikipedia.org/wiki/Modifiable_areal_unit_problem\n\n\n\n\nTrue rate = 1/3 = 33%\nA’s rate = (0 +1/2) /2 = 25%\nA’s weighted rate = 1/3 * 0 + 2/3 * 50 = 33%\nB’s rate = (0 + 100) /2 = 50%\nB’s weighted rate = 2/3 * 0 + 1/3 * 100 = 33%"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-process",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-process",
    "title": "Spatial Data",
    "section": "Spatial Process",
    "text": "Spatial Process\nSpatial Random Field\na mathemtical construct to capture randomness of values distributed over space\n\\[\\{Z(s):s \\in D \\} \\]\n\n\\(s \\in R^d:\\) location (e.g., lat-lon)\n\\(D \\in R^d:\\) index set = possible locations\n\\(Z(s):\\) random variable at location \\(s\\)"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#types-of-spatial-data-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#types-of-spatial-data-1",
    "title": "Spatial Data",
    "section": "Types of Spatial Data",
    "text": "Types of Spatial Data\n\nEvents\n\naddresses of crimes\n\nDiscrete Spatial Objects\n\ncounty crime rates\n\nContinuous surfaces\n\nair quality\nrainfall"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis",
    "title": "Spatial Data",
    "section": "Point Pattern Analysis",
    "text": "Point Pattern Analysis\nData\n\nmapped pattern = all the values\nnot a sample in the usual sense\n\nSpatial Process\n\nobservations as a realization of a random point process\npoints occur in space according to a mathematical model"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#point-patterns",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#point-patterns",
    "title": "Spatial Data",
    "section": "Point Patterns",
    "text": "Point Patterns\nUnmarked Point Pattern\n\nonly location is recorded\nno other attribute information\n\nMarked Point Pattern\n\nLocation is recorded\nStochastic attributes are also recorded\ne.g., sales price at address, DBH of a tree"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis-quadrat-methods",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis-quadrat-methods",
    "title": "Spatial Data",
    "section": "Point Pattern Analysis: Quadrat Methods",
    "text": "Point Pattern Analysis: Quadrat Methods\n\nQuadrat Analysis"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis-distance-based-methods",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis-distance-based-methods",
    "title": "Spatial Data",
    "section": "Point Pattern Analysis: Distance Based Methods",
    "text": "Point Pattern Analysis: Distance Based Methods\n\nDistance Distributions"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#areal-unit-data-lattice",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#areal-unit-data-lattice",
    "title": "Spatial Data",
    "section": "Areal Unit Data (Lattice)",
    "text": "Areal Unit Data (Lattice)\n\nSpatial Domain: \\(D\\)\n\nDiscrete and fixed\nLocations nonrandom\nLocations countable\n\n\n\nExamples of lattice data\n\nAttributes collected by ZIP code\ncensus tract"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#lattice-data-indexing",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#lattice-data-indexing",
    "title": "Spatial Data",
    "section": "Lattice Data: Indexing",
    "text": "Lattice Data: Indexing\n\nSite\n\nEach location is now an area or site\nOne observation on \\(Z\\) for each site\nNeed a spatial index: \\(Z(s_i)\\)\n\n\n\n\\(Z(s_i)\\)\n\n\\(s_i\\) is a representative location within the site\ne.g., centroid, largest city\nAllows for measuring distances between sites"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#lattice-data-county-per-capita-incomes",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#lattice-data-county-per-capita-incomes",
    "title": "Spatial Data",
    "section": "Lattice Data: County Per Capita Incomes",
    "text": "Lattice Data: County Per Capita Incomes\n\n1969"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-analysis",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-analysis",
    "title": "Spatial Data",
    "section": "Geostatistical Analysis",
    "text": "Geostatistical Analysis\n\nSpatial Domain: \\(D\\)\n\nA continuous and fixed set.\nMeaning \\(Z(s)\\) can be observed everywhere within \\(D\\).\nBetween any two sample locations \\(s_i\\) and \\(s_j\\) you can theoretically place an infinite number of other samples.\nBy fixed: the points in \\(D\\) are non-stochastic"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data",
    "title": "Spatial Data",
    "section": "Geostatistical Data",
    "text": "Geostatistical Data\n\nContinuous Variation\n\nBecause of the continuity of \\(D\\)\nGeostatistical data is referred to as “spatial data with continuous variation.”\nContinuity is associated with \\(D\\).\nAttribute \\(Z\\) may, or may not, be continuous."
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-monitoring-sites",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-monitoring-sites",
    "title": "Spatial Data",
    "section": "Geostatistical Data: Monitoring Sites",
    "text": "Geostatistical Data: Monitoring Sites\n\nSites"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction",
    "title": "Spatial Data",
    "section": "Geostatistical Data: Surface Reconstruction",
    "text": "Geostatistical Data: Surface Reconstruction\n\nTessellation"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction-1",
    "title": "Spatial Data",
    "section": "Geostatistical Data: Surface Reconstruction",
    "text": "Geostatistical Data: Surface Reconstruction\n\nInterpolation"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction-2",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction-2",
    "title": "Spatial Data",
    "section": "Geostatistical Data: Surface Reconstruction",
    "text": "Geostatistical Data: Surface Reconstruction\n\nKriging"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#network-data",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#network-data",
    "title": "Spatial Data",
    "section": "Network Data",
    "text": "Network Data\n\n\nA network is a system of linear features connected at intersections and interchanges.\nThese intersections and interchanges are called nodes.\nThe linear feature connecting any given pair of nodes is called an arc.\nFormally, a network is defined as a directed graph \\(G = (N,  A)\\) consisting of an indexed set of nodes \\(N\\) with \\(n = |N|\\) and a spanning set of directed arcs \\(A\\) with \\(m = |A|\\), where \\(n\\) is the number of nodes and \\(m\\) is the number of arcs.\nEach arc on a network is represented as an ordered pair of nodes, in the form from node \\(i\\) to node \\(j\\), denoted by \\((i, j)\\)."
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#network-data-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#network-data-1",
    "title": "Spatial Data",
    "section": "Network Data",
    "text": "Network Data\n\nStreet Network"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#flow-data",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#flow-data",
    "title": "Spatial Data",
    "section": "Flow Data",
    "text": "Flow Data\n\n\n\n\nFlows"
  },
  {
    "objectID": "lectures/week-04/2023-09-13.html",
    "href": "lectures/week-04/2023-09-13.html",
    "title": "Geog385F23",
    "section": "",
    "text": "GeoPandas Structure\nWorking with GeoDataFrames and GeoSeriesf\nCarrying out project\nBasic spatial queries and attribute construction\n\n\nimport geopandas\n\n/tmp/ipykernel_2579345/1616829109.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas\n\n\n\ngdf = geopandas.read_file(\"./data/shared/covid/gz_2010_us_040_00_500k.json\")\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\n\n\n\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      NAME\n      LSAD\n      CENSUSAREA\n      geometry\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n    \n    \n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n    \n    \n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n    \n    \n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n    \n    \n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n    \n  \n\n\n\n\n\ngdf.columns.values\n\narray(['GEO_ID', 'STATE', 'NAME', 'LSAD', 'CENSUSAREA', 'geometry'],\n      dtype=object)\n\n\n\ngdf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntype(gdf)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\n\n\ngdf.geometry\n\n0     MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n1     MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n2     MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n3     POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n4     POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n5     POLYGON ((-75.52684 39.65571, -75.52634 39.656...\n6     MULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n7     MULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n8     MULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n9     POLYGON ((-75.41504 39.80179, -75.42804 39.809...\n10    MULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n11    POLYGON ((-81.67754 36.58812, -81.68014 36.585...\n12    MULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n13    POLYGON ((-114.05060 37.00040, -114.05175 37.0...\n14    MULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n15    MULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n16    MULTIPOLYGON (((-65.58733 18.38199, -65.59122 ...\n17    MULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n18    MULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n19    MULTIPOLYGON (((-164.97620 54.13459, -164.9377...\n20    POLYGON ((-109.04522 36.99908, -109.04524 36.9...\n21    POLYGON ((-94.55929 36.49950, -94.51948 36.499...\n22    MULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n23    POLYGON ((-102.04224 36.99308, -102.05450 36.9...\n24    MULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n25    MULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n26    POLYGON ((-77.03860 38.79151, -77.03890 38.800...\n27    MULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n28    POLYGON ((-81.44412 30.70971, -81.44872 30.709...\n29    MULTIPOLYGON (((-171.73761 25.79210, -171.7223...\n30    POLYGON ((-111.04669 42.00157, -111.41587 42.0...\n31    POLYGON ((-87.53233 39.99778, -87.53254 39.987...\n32    POLYGON ((-88.02803 37.79922, -88.02938 37.803...\n33    POLYGON ((-95.76565 40.58521, -95.75889 40.588...\n34    POLYGON ((-94.61808 36.99813, -94.62522 36.998...\n35    MULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n36    MULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n37    POLYGON ((-91.37161 43.50095, -91.37695 43.500...\n38    MULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n39    POLYGON ((-89.53910 36.49820, -89.53452 36.491...\n40    POLYGON ((-95.76565 40.58521, -95.76853 40.583...\n41    MULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n42    POLYGON ((-109.05004 31.33250, -109.05017 31.4...\n43    POLYGON ((-96.56328 45.93524, -96.57690 45.935...\n44    POLYGON ((-94.61792 36.49941, -94.61531 36.484...\n45    POLYGON ((-117.22007 44.30138, -117.22245 44.2...\n46    POLYGON ((-78.54109 33.85111, -78.55394 33.847...\n47    POLYGON ((-96.44341 42.48949, -96.45971 42.486...\n48    POLYGON ((-72.04008 44.15575, -72.04271 44.152...\n49    MULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n50    POLYGON ((-81.96830 37.53780, -81.96540 37.541...\n51    POLYGON ((-109.05008 41.00066, -109.17368 41.0...\nName: geometry, dtype: geometry\n\n\n\ngdf.iloc[0].geometry\n\n\n\n\n\ngdf.shape\n\n(52, 6)\n\n\n\ngdf.head(52)\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      NAME\n      LSAD\n      CENSUSAREA\n      geometry\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n    \n    \n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n    \n    \n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n    \n    \n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n    \n    \n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n    \n    \n      5\n      0400000US34\n      34\n      New Jersey\n      \n      7354.220\n      POLYGON ((-75.52684 39.65571, -75.52634 39.656...\n    \n    \n      6\n      0400000US36\n      36\n      New York\n      \n      47126.399\n      MULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n    \n    \n      7\n      0400000US37\n      37\n      North Carolina\n      \n      48617.905\n      MULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n    \n    \n      8\n      0400000US39\n      39\n      Ohio\n      \n      40860.694\n      MULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n    \n    \n      9\n      0400000US42\n      42\n      Pennsylvania\n      \n      44742.703\n      POLYGON ((-75.41504 39.80179, -75.42804 39.809...\n    \n    \n      10\n      0400000US44\n      44\n      Rhode Island\n      \n      1033.814\n      MULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n    \n    \n      11\n      0400000US47\n      47\n      Tennessee\n      \n      41234.896\n      POLYGON ((-81.67754 36.58812, -81.68014 36.585...\n    \n    \n      12\n      0400000US48\n      48\n      Texas\n      \n      261231.711\n      MULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n    \n    \n      13\n      0400000US49\n      49\n      Utah\n      \n      82169.620\n      POLYGON ((-114.05060 37.00040, -114.05175 37.0...\n    \n    \n      14\n      0400000US53\n      53\n      Washington\n      \n      66455.521\n      MULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n    \n    \n      15\n      0400000US55\n      55\n      Wisconsin\n      \n      54157.805\n      MULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n    \n    \n      16\n      0400000US72\n      72\n      Puerto Rico\n      \n      3423.775\n      MULTIPOLYGON (((-65.58733 18.38199, -65.59122 ...\n    \n    \n      17\n      0400000US24\n      24\n      Maryland\n      \n      9707.241\n      MULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n    \n    \n      18\n      0400000US01\n      01\n      Alabama\n      \n      50645.326\n      MULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n    \n    \n      19\n      0400000US02\n      02\n      Alaska\n      \n      570640.950\n      MULTIPOLYGON (((-164.97620 54.13459, -164.9377...\n    \n    \n      20\n      0400000US04\n      04\n      Arizona\n      \n      113594.084\n      POLYGON ((-109.04522 36.99908, -109.04524 36.9...\n    \n    \n      21\n      0400000US05\n      05\n      Arkansas\n      \n      52035.477\n      POLYGON ((-94.55929 36.49950, -94.51948 36.499...\n    \n    \n      22\n      0400000US06\n      06\n      California\n      \n      155779.220\n      MULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n    \n    \n      23\n      0400000US08\n      08\n      Colorado\n      \n      103641.888\n      POLYGON ((-102.04224 36.99308, -102.05450 36.9...\n    \n    \n      24\n      0400000US09\n      09\n      Connecticut\n      \n      4842.355\n      MULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n    \n    \n      25\n      0400000US10\n      10\n      Delaware\n      \n      1948.543\n      MULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n    \n    \n      26\n      0400000US11\n      11\n      District of Columbia\n      \n      61.048\n      POLYGON ((-77.03860 38.79151, -77.03890 38.800...\n    \n    \n      27\n      0400000US12\n      12\n      Florida\n      \n      53624.759\n      MULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n    \n    \n      28\n      0400000US13\n      13\n      Georgia\n      \n      57513.485\n      POLYGON ((-81.44412 30.70971, -81.44872 30.709...\n    \n    \n      29\n      0400000US15\n      15\n      Hawaii\n      \n      6422.628\n      MULTIPOLYGON (((-171.73761 25.79210, -171.7223...\n    \n    \n      30\n      0400000US16\n      16\n      Idaho\n      \n      82643.117\n      POLYGON ((-111.04669 42.00157, -111.41587 42.0...\n    \n    \n      31\n      0400000US17\n      17\n      Illinois\n      \n      55518.930\n      POLYGON ((-87.53233 39.99778, -87.53254 39.987...\n    \n    \n      32\n      0400000US18\n      18\n      Indiana\n      \n      35826.109\n      POLYGON ((-88.02803 37.79922, -88.02938 37.803...\n    \n    \n      33\n      0400000US19\n      19\n      Iowa\n      \n      55857.130\n      POLYGON ((-95.76565 40.58521, -95.75889 40.588...\n    \n    \n      34\n      0400000US20\n      20\n      Kansas\n      \n      81758.717\n      POLYGON ((-94.61808 36.99813, -94.62522 36.998...\n    \n    \n      35\n      0400000US21\n      21\n      Kentucky\n      \n      39486.338\n      MULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n    \n    \n      36\n      0400000US22\n      22\n      Louisiana\n      \n      43203.905\n      MULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n    \n    \n      37\n      0400000US27\n      27\n      Minnesota\n      \n      79626.743\n      POLYGON ((-91.37161 43.50095, -91.37695 43.500...\n    \n    \n      38\n      0400000US28\n      28\n      Mississippi\n      \n      46923.274\n      MULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n    \n    \n      39\n      0400000US29\n      29\n      Missouri\n      \n      68741.522\n      POLYGON ((-89.53910 36.49820, -89.53452 36.491...\n    \n    \n      40\n      0400000US31\n      31\n      Nebraska\n      \n      76824.171\n      POLYGON ((-95.76565 40.58521, -95.76853 40.583...\n    \n    \n      41\n      0400000US33\n      33\n      New Hampshire\n      \n      8952.651\n      MULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n    \n    \n      42\n      0400000US35\n      35\n      New Mexico\n      \n      121298.148\n      POLYGON ((-109.05004 31.33250, -109.05017 31.4...\n    \n    \n      43\n      0400000US38\n      38\n      North Dakota\n      \n      69000.798\n      POLYGON ((-96.56328 45.93524, -96.57690 45.935...\n    \n    \n      44\n      0400000US40\n      40\n      Oklahoma\n      \n      68594.921\n      POLYGON ((-94.61792 36.49941, -94.61531 36.484...\n    \n    \n      45\n      0400000US41\n      41\n      Oregon\n      \n      95988.013\n      POLYGON ((-117.22007 44.30138, -117.22245 44.2...\n    \n    \n      46\n      0400000US45\n      45\n      South Carolina\n      \n      30060.696\n      POLYGON ((-78.54109 33.85111, -78.55394 33.847...\n    \n    \n      47\n      0400000US46\n      46\n      South Dakota\n      \n      75811.000\n      POLYGON ((-96.44341 42.48949, -96.45971 42.486...\n    \n    \n      48\n      0400000US50\n      50\n      Vermont\n      \n      9216.657\n      POLYGON ((-72.04008 44.15575, -72.04271 44.152...\n    \n    \n      49\n      0400000US51\n      51\n      Virginia\n      \n      39490.086\n      MULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n    \n    \n      50\n      0400000US54\n      54\n      West Virginia\n      \n      24038.210\n      POLYGON ((-81.96830 37.53780, -81.96540 37.541...\n    \n    \n      51\n      0400000US56\n      56\n      Wyoming\n      \n      97093.141\n      POLYGON ((-109.05008 41.00066, -109.17368 41.0...\n    \n  \n\n\n\n\n\ndrop_states = ['15', '02', '72'] # HA, AK, PR\n\n\ngdf[gdf.STATE.isin(drop_states)].plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf[~gdf.STATE.isin(drop_states)].plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf = gdf[~gdf.STATE.isin(drop_states)]\n\n\ngdf.shape\n\n(49, 6)\n\n\n\n\n\n\ngdf.centroid\n\n/tmp/ipykernel_2579345/2017122361.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf.centroid\n\n\n0      POINT (-69.22532 45.36948)\n1      POINT (-71.79546 42.25229)\n2      POINT (-85.43751 44.35323)\n3     POINT (-109.64507 47.03355)\n4     POINT (-116.65540 39.35646)\n5      POINT (-74.66099 40.18393)\n6      POINT (-75.50198 42.93930)\n7      POINT (-79.35542 35.53980)\n8      POINT (-82.79018 40.29333)\n9      POINT (-77.79953 40.87382)\n10     POINT (-71.55250 41.67619)\n11     POINT (-86.34329 35.84299)\n12     POINT (-99.35528 31.49051)\n13    POINT (-111.67820 39.32379)\n14    POINT (-120.45017 47.38108)\n15     POINT (-90.01113 44.63829)\n17     POINT (-76.76446 39.03041)\n18     POINT (-86.82843 32.78969)\n20    POINT (-111.66458 34.29326)\n21     POINT (-92.43928 34.89974)\n22    POINT (-119.61077 37.24612)\n23    POINT (-105.54782 38.99855)\n24     POINT (-72.72576 41.62055)\n25     POINT (-75.50018 38.99178)\n26     POINT (-77.01630 38.90473)\n27     POINT (-82.50162 28.64096)\n28     POINT (-83.44606 32.64908)\n30    POINT (-114.65933 44.38912)\n31     POINT (-89.19828 40.06474)\n32     POINT (-86.27529 39.90853)\n33     POINT (-93.50004 42.07462)\n34     POINT (-98.38022 38.48470)\n35     POINT (-85.29049 37.52666)\n36     POINT (-91.98048 31.05264)\n37     POINT (-94.30870 46.31645)\n38     POINT (-89.66510 32.75040)\n39     POINT (-92.47742 38.36762)\n40     POINT (-99.81080 41.52715)\n41     POINT (-71.57760 43.68569)\n42    POINT (-106.10838 34.42137)\n43    POINT (-100.46931 47.44634)\n44     POINT (-97.50844 35.58350)\n45    POINT (-120.55529 43.93673)\n46     POINT (-80.89550 33.90710)\n47    POINT (-100.23049 44.43615)\n48     POINT (-72.66272 44.07518)\n49     POINT (-78.80562 37.51539)\n50     POINT (-80.61385 38.64251)\n51    POINT (-107.55145 42.99964)\ndtype: geometry\n\n\n\ngdf.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf.to_crs(gdf.estimate_utm_crs()).plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf = gdf.to_crs(gdf.estimate_utm_crs())\n\n\ngdf.centroid\n\n0      POINT (2359392.452 5305169.775)\n1      POINT (2253174.905 4901469.336)\n2      POINT (1108124.800 4935453.192)\n3      POINT (-765955.563 5348067.290)\n4     POINT (-1548246.254 4630302.336)\n5      POINT (2065285.016 4612318.097)\n6      POINT (1930049.196 4904992.641)\n7      POINT (1742974.165 4021487.098)\n8      POINT (1368634.294 4510266.503)\n9      POINT (1784134.621 4638257.315)\n10     POINT (2289313.142 4842131.086)\n11     POINT (1102080.849 3988935.477)\n12     POINT (-103892.519 3500028.226)\n13    POINT (-1115997.209 4520861.031)\n14    POINT (-1565541.918 5625943.989)\n15      POINT (738620.228 4945674.977)\n17     POINT (1910429.070 4447760.368)\n18     POINT (1078425.172 3643488.570)\n20    POINT (-1230212.721 3954699.221)\n21      POINT (550933.347 3861451.888)\n22    POINT (-1870826.275 4466915.330)\n23     POINT (-590190.342 4392344.096)\n24     POINT (2192677.610 4811150.640)\n25     POINT (2020359.310 4464239.060)\n26     POINT (1889584.643 4429716.358)\n27     POINT (1535090.511 3210901.406)\n28     POINT (1399285.825 3651528.582)\n30    POINT (-1223231.299 5144300.473)\n31      POINT (824539.285 4439350.669)\n32     POINT (1074696.801 4437810.568)\n33      POINT (458939.639 4658274.632)\n34       POINT (29865.949 4274404.492)\n35     POINT (1181577.693 4182456.487)\n36      POINT (598555.176 3435096.661)\n37      POINT (399444.786 5126179.145)\n38      POINT (812334.714 3626923.994)\n39      POINT (547395.056 4245527.933)\n40      POINT (-68074.613 4621136.163)\n41     POINT (2227578.113 5064712.956)\n42     POINT (-710483.211 3886170.809)\n43      POINT (-63437.281 5282398.408)\n44       POINT (91977.904 3947894.482)\n45    POINT (-1716081.976 5249621.330)\n46     POINT (1623336.735 3818285.055)\n47      POINT (-76107.494 4946849.414)\n48     POINT (2128860.270 5085004.659)\n49     POINT (1758189.886 4249101.361)\n50     POINT (1579306.753 4350654.685)\n51     POINT (-688676.799 4864507.482)\ndtype: geometry\n\n\n\ngdf.crs\n\n<Projected CRS: EPSG:32615>\nName: WGS 84 / UTM zone 15N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 96°W and 90°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - Manitoba; Nunavut; Ontario. Ecuador -Galapagos. Guatemala. Mexico. United States (USA).\n- bounds: (-96.0, 0.0, -90.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 15N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf['centroid'] = gdf.centroid\n\n\nbase = gdf.plot()\ngdf.centroid.plot(ax=base, color='r')\n\n<AxesSubplot:>"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#introduction",
    "href": "lectures/week-01/01-introduction.html#introduction",
    "title": "Course Introduction",
    "section": "Introduction",
    "text": "Introduction\n\nThis course introduces the fundamental concepts of spatial data analysis. Key fundamentals include spatial sampling, descriptive statistics for areal data, inferential statistics, use of maps in data analysis."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#approach",
    "href": "lectures/week-01/01-introduction.html#approach",
    "title": "Course Introduction",
    "section": "Approach",
    "text": "Approach\n\nThe course takes an explicitly computational thinking approach to its pedagogy. Students are introduced to computational concepts and tools that are increasingly important to research that engages with geospatial data. By adopting these tools, students acquire a deeper engagement with, and mastery of, the substantive concepts."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#scope",
    "href": "lectures/week-01/01-introduction.html#scope",
    "title": "Course Introduction",
    "section": "Scope",
    "text": "Scope\n\nIn the scope of a 15-week semester course we can only introduce a handful of the key concepts and methods relevant to the field of spatial data analysis. As such, the course is not intended as an exhaustive treatment. Instead, the goal is that students will acquire an understanding of the more common and useful methods and practices, and use the course as an entry point for further engagement with the field."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#prerequisites",
    "href": "lectures/week-01/01-introduction.html#prerequisites",
    "title": "Course Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#schedule-reading-and-content",
    "href": "lectures/week-01/01-introduction.html#schedule-reading-and-content",
    "title": "Course Introduction",
    "section": "Schedule, Reading, and Content",
    "text": "Schedule, Reading, and Content\nAll required readings are available through the links listed below. Assigned readings should be completed before the date listed in the schedule (see below). Readings are a critical part of the discussions we will hold in class, and therefore coming into class prepared means having completed the readings and thought about the content. It will be difficult to do well in this course without having completed the readings."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#readings",
    "href": "lectures/week-01/01-introduction.html#readings",
    "title": "Course Introduction",
    "section": "Readings",
    "text": "Readings\n\n\n\nAbbrevation\nSource\n\n\n\n\nGDA\nTenkanen, H., V. Heikinheimo, D. Whipp (2023) Python for Geographic Data Analysis. CRC Press.\n\n\nGDS\nRey, S.J., D. Arribas-Bel, L.J. Wolf (2023) Geographic Data Science with Python. CRC Press."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#schedule-planned",
    "href": "lectures/week-01/01-introduction.html#schedule-planned",
    "title": "Course Introduction",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#schedule-planned-1",
    "href": "lectures/week-01/01-introduction.html#schedule-planned-1",
    "title": "Course Introduction",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#grading",
    "href": "lectures/week-01/01-introduction.html#grading",
    "title": "Course Introduction",
    "section": "Grading",
    "text": "Grading\nGEOG385 uses specification grading in evaluating student work and in determining your final course grade. Your course grade will be based on the quality and quantity of the work that you submit that is evaluated to be of an acceptable level of quality. The acceptable level of quality demonstrates competency in the concepts and methods covered in the course."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#specification-grading",
    "href": "lectures/week-01/01-introduction.html#specification-grading",
    "title": "Course Introduction",
    "section": "Specification Grading",
    "text": "Specification Grading\nThere is a two-step process for determination of your final course grade at the end of the quarter:\n\nUsing your quizzes, and exercises, your base grade is determined.\nUsing your final exam results, determine if your base grade includes a \"plus\", \"minus\", or level drop to form the course grade."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#base-grade",
    "href": "lectures/week-01/01-introduction.html#base-grade",
    "title": "Course Introduction",
    "section": "Base Grade",
    "text": "Base Grade\n\n\n\nLevel\nHurdles\n\n\n\n\nA\nPass at least 13 of 15 quizzes and earn \"Demonstrates Competency\" on 4 of 4 exercises,\n\n\nB\nPass at least 11 of 15 quizzes and earn \"Demonstrates Competency\" on 3 of 4 exercises\n\n\nC\nPass at least 9 of 15 quizzes and earn \"Demonstrates Competency\" on 2 of 4 exercises\n\n\nD\nPass at least 7 of 15 quizzes and earn \"Demonstrates Competency\" on 1 of 4 exercises\n\n\nF\nFail to clear D-level hurdles"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#final-grade",
    "href": "lectures/week-01/01-introduction.html#final-grade",
    "title": "Course Introduction",
    "section": "Final Grade",
    "text": "Final Grade\n\nIf you earn at least 85% on the final exam, you will obtain a “+” for your grade. So a B base grade would become a B+ course grade, and so on (Note: SDSU does not record A+ grades).\nIf you score between 70-85% on the final exam, your base grade becomes your course grade.\nIf you score between 50% and 69% on the final exam, you will obtain a “-” for your grade. So an A base grade becomes an A- course grade, a B base grade becomes a B- course grade, and so on.\nIf you score less than 50% on the final exam, your course grade will drop one level: An A base grade becomes a final B course grade."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#quizzes",
    "href": "lectures/week-01/01-introduction.html#quizzes",
    "title": "Course Introduction",
    "section": "Quizzes",
    "text": "Quizzes\nQuizzes are graded on a pass/fail basis. Starting in week two, there will be a quiz due before a session that pertains to the background reading that is required before our work in class."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#exercises",
    "href": "lectures/week-01/01-introduction.html#exercises",
    "title": "Course Introduction",
    "section": "Exercises",
    "text": "Exercises\nFour exercises will be introduced in class and are to be completed outside of class meetings.\nEach exercise is graded using a CRN rubric that classifies work with marks of C (\"Demonstrates Competence\"), R (\"Needs Revision\"), or N (\"Not assessable\"):"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#exercises-1",
    "href": "lectures/week-01/01-introduction.html#exercises-1",
    "title": "Course Introduction",
    "section": "Exercises",
    "text": "Exercises\nOf each exercise the following questions will be asked: Does the work demonstrate that the student understands the concepts? Does the work demonstrate competence and meet the expectations outlined in the exercise?\nIf the answer is \"yes\" to both of the questions, a student passes the hurdle for that exercise."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#exercises-2",
    "href": "lectures/week-01/01-introduction.html#exercises-2",
    "title": "Course Introduction",
    "section": "Exercises",
    "text": "Exercises\nIf the initial submission does not clear the hurdle, then a second question is asked: Is there evidence of partial understanding of the concepts? If the answer to this question is \"Yes\" the student can exchange one token to attempt a revision of their work. If the answer is \"No\", the student does not clear the hurdle for this exercise and will not have the opportunity to revise their work."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#final-exam",
    "href": "lectures/week-01/01-introduction.html#final-exam",
    "title": "Course Introduction",
    "section": "Final Exam",
    "text": "Final Exam\nA closed book, closed note, timed final exam will be given on December 18 (13:00-15:00). The exam will be based on a blend of previous quiz questions and additional questions that pertain to material covered in class."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#tokens",
    "href": "lectures/week-01/01-introduction.html#tokens",
    "title": "Course Introduction",
    "section": "Tokens",
    "text": "Tokens\nEach student is provided with three tokens at the beginning of the semester."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#using-tokens",
    "href": "lectures/week-01/01-introduction.html#using-tokens",
    "title": "Course Introduction",
    "section": "Using Tokens",
    "text": "Using Tokens\n\nOne token can be used for a one-day extension for an exercise.\nOne token can be used to revise an exercise that was submitted on-time but evaluated as \"Needing Revision\".\nTwo tokens can be used to request a make-up date for the final exam."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#remaining-tokens",
    "href": "lectures/week-01/01-introduction.html#remaining-tokens",
    "title": "Course Introduction",
    "section": "Remaining Tokens",
    "text": "Remaining Tokens\nEach token that remains unused after 2023-12-18 will be counted as a passed quiz. Tokens cannot be exchanged with other students."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#administration",
    "href": "lectures/week-01/01-introduction.html#administration",
    "title": "Course Introduction",
    "section": "Administration",
    "text": "Administration"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#accommodations",
    "href": "lectures/week-01/01-introduction.html#accommodations",
    "title": "Course Introduction",
    "section": "Accommodations",
    "text": "Accommodations\nIf you are a student with a disability and are in need of accommodations for this class, please contact Student Ability Success Center at (619) 594-6473 as soon as possible. Please know accommodations are not retroactive, and I cannot provide accommodations based upon disability until I have received an accommodation letter from Student Ability Success Center."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#privacy-and-intellectual-property",
    "href": "lectures/week-01/01-introduction.html#privacy-and-intellectual-property",
    "title": "Course Introduction",
    "section": "Privacy and Intellectual Property",
    "text": "Privacy and Intellectual Property\nStudent Privacy and Intellectual Property: The Family Educational Rights and Privacy Act (FERPA) mandates the protection of student information, including contact information, grades, and graded assignments. I will use [Canvas / Blackboard] to communicate with you, and I will not post grades or leave graded assignments in public places. Students will be notified at the time of an assignment if copies of student work will be retained beyond the end of the semester or used as examples for future students or the wider public. Students maintain intellectual property rights to work products they create as part of this course unless they are formally notified otherwise."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#academic-integrity",
    "href": "lectures/week-01/01-introduction.html#academic-integrity",
    "title": "Course Introduction",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThe SDSU student academic integrity policy lists violations in detail. These violations fall into eight broad areas that include but are not limited to: cheating, fabrication, plagiarism, facilitating academic misconduct, unauthorized collaboration, interference or sabotage, non-compliance with research regulations and retaliation. For more information about the SDSU student academic integrity policy, please see the following: http://www.sa.sdsu.edu/srr/index.html."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#code-of-conduct",
    "href": "lectures/week-01/01-introduction.html#code-of-conduct",
    "title": "Course Introduction",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nAs course instructor, I am dedicated to providing a harassment-free learning experience for all students, regardless of gender, sexual orientation, disability, physical appearance, body size, race, religion, or choice of operating system. All course participants are expected to show respect and courtesy to other students throughout the semester. As a learning community we do not tolerate harassment of participants in any form."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#code-of-conduct-1",
    "href": "lectures/week-01/01-introduction.html#code-of-conduct-1",
    "title": "Course Introduction",
    "section": "Code of Conduct",
    "text": "Code of Conduct\n\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery are not appropriate in this course.\nBe kind to others. Do not insult or put down other students. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for this course.\nStudents violating these rules may be asked to leave the course, and their violations will be reported to the SDSU administration.\n\nThis code of conduct is an adaptation of the SciPy 2018 Code of Conduct."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#computational-learning",
    "href": "lectures/week-01/01-introduction.html#computational-learning",
    "title": "Course Introduction",
    "section": "Computational Learning",
    "text": "Computational Learning\n\n\nShow me the code\nimport libpysal.examples\nimport geopandas \n\n# get path to built-in dataset for Mexico\npth = libpysal.examples.get_path(\"mexicojoin.shp\")\n# load the file with geopandas to create a GeoDataframe\ngdf = geopandas.read_file(pth)\n# call the plot method of the GeoDataFrame\ngdf.plot(edgecolor='white');"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#open-source",
    "href": "lectures/week-01/01-introduction.html#open-source",
    "title": "Course Introduction",
    "section": "Open Source",
    "text": "Open Source"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#server-or-laptop",
    "href": "lectures/week-01/01-introduction.html#server-or-laptop",
    "title": "Course Introduction",
    "section": "Server or Laptop",
    "text": "Server or Laptop\nYou can choose to either use an account on our course JupyterHub or install the packages on your own laptop.\nEither way, you will be using Jupyter Notebooks for all computation:"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#my-program",
    "href": "lectures/week-01/01-introduction.html#my-program",
    "title": "Course Introduction",
    "section": "My Program",
    "text": "My Program\n\n\n\nurl"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#why-am-i-here",
    "href": "lectures/week-01/01-introduction.html#why-am-i-here",
    "title": "Course Introduction",
    "section": "Why am I here",
    "text": "Why am I here"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#why-am-i-here-1",
    "href": "lectures/week-01/01-introduction.html#why-am-i-here-1",
    "title": "Course Introduction",
    "section": "Why am I here",
    "text": "Why am I here"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#trump-turned-this-place-into-a-ghost-town",
    "href": "lectures/week-01/01-introduction.html#trump-turned-this-place-into-a-ghost-town",
    "title": "Course Introduction",
    "section": "‘Trump turned this place into a ghost town’",
    "text": "‘Trump turned this place into a ghost town’"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-and-atlantic-city",
    "href": "lectures/week-01/01-introduction.html#stockton-and-atlantic-city",
    "title": "Course Introduction",
    "section": "Stockton and Atlantic City",
    "text": "Stockton and Atlantic City"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-and-atlantic-city-1",
    "href": "lectures/week-01/01-introduction.html#stockton-and-atlantic-city-1",
    "title": "Course Introduction",
    "section": "Stockton and Atlantic City",
    "text": "Stockton and Atlantic City\n\nSource"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton",
    "href": "lectures/week-01/01-introduction.html#stockton",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-1",
    "href": "lectures/week-01/01-introduction.html#stockton-1",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-2",
    "href": "lectures/week-01/01-introduction.html#stockton-2",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-3",
    "href": "lectures/week-01/01-introduction.html#stockton-3",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#you",
    "href": "lectures/week-01/01-introduction.html#you",
    "title": "Course Introduction",
    "section": "You",
    "text": "You\nTake a few minutes and let us know a bit about yourself\n\nName\nProgram/Concentration\nWhy you are here"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html",
    "href": "lectures/week-01/2023-08-23.html",
    "title": "Geog385F23",
    "section": "",
    "text": "3 + 7\n\n10"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#markdown",
    "href": "lectures/week-01/2023-08-23.html#markdown",
    "title": "Geog385F23",
    "section": "Markdown",
    "text": "Markdown\nThis is a markdown cell.\nSo what?\nWell we can do things like bold or italics.\nHow about bold and italics\n\nSubsection\nNow we are in a subsection.\n\n\nLists\nHere is stuff .\nWe can do unordered lists:\n\nfirst\nsecond\nthird\n\nOr numbered lists\n\nfirst\nsecond\nthird"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#math",
    "href": "lectures/week-01/2023-08-23.html#math",
    "title": "Geog385F23",
    "section": "Math",
    "text": "Math\nThe formulae for the sample mean is:\n\\[\n\\bar{x} = 1/n \\sum_{i=1}^n x_i\n\\]"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#markdown-1",
    "href": "lectures/week-01/2023-08-23.html#markdown-1",
    "title": "Geog385F23",
    "section": "Markdown",
    "text": "Markdown\nThis is a markdown cell.\nSo what?\nWell we can do things like bold or italics.\nHow about bold and italics\n\nSubsection\nNow we are in a subsection.\n\n\nLists\nHere is stuff .\nWe can do unordered lists:\n\nfirst\nsecond\nthird\n\nOr numbered lists\n\nfirst\nsecond\nthird"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#math-1",
    "href": "lectures/week-01/2023-08-23.html#math-1",
    "title": "Geog385F23",
    "section": "Math",
    "text": "Math\nThe formulae for the sample mean is:\n\\[\n\\bar{x} = 1/n \\sum_{i=1}^n x_i\n\\]"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#links",
    "href": "lectures/week-01/2023-08-23.html#links",
    "title": "Geog385F23",
    "section": "Links",
    "text": "Links\nThe big search engine in the sky is google"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#markdown-2",
    "href": "lectures/week-01/2023-08-23.html#markdown-2",
    "title": "Geog385F23",
    "section": "Markdown",
    "text": "Markdown\nThis is a markdown cell.\nSo what?\nWell we can do things like bold or italics.\nHow about bold and italics\n\nSubsection\nNow we are in a subsection.\n\n\nLists\nHere is stuff .\nWe can do unordered lists:\n\nfirst\nsecond\nthird\n\nOr numbered lists\n\nfirst\nsecond\nthird"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#math-2",
    "href": "lectures/week-01/2023-08-23.html#math-2",
    "title": "Geog385F23",
    "section": "Math",
    "text": "Math\n\n10 * 4\n\n40"
  },
  {
    "objectID": "lectures/week-05/2023-09-18.html",
    "href": "lectures/week-05/2023-09-18.html",
    "title": "Geog385F23",
    "section": "",
    "text": "GeoPandas Structure\nWorking with GeoDataFrames and GeoSeriesf\nCarrying out project\nBasic spatial queries and attribute construction\n\n\nimport geopandas\n\n/tmp/ipykernel_3120570/1616829109.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas\n\n\n\ngdf = geopandas.read_file(\"./data/shared/covid/gz_2010_us_040_00_500k.json\")\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\n\n\n\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      NAME\n      LSAD\n      CENSUSAREA\n      geometry\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n    \n    \n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n    \n    \n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n    \n    \n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n    \n    \n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n    \n  \n\n\n\n\n\ngdf.columns.values\n\narray(['GEO_ID', 'STATE', 'NAME', 'LSAD', 'CENSUSAREA', 'geometry'],\n      dtype=object)\n\n\n\ngdf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ntype(gdf)\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\n\n\ngdf.geometry\n\n0     MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n1     MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n2     MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n3     POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n4     POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n5     POLYGON ((-75.52684 39.65571, -75.52634 39.656...\n6     MULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n7     MULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n8     MULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n9     POLYGON ((-75.41504 39.80179, -75.42804 39.809...\n10    MULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n11    POLYGON ((-81.67754 36.58812, -81.68014 36.585...\n12    MULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n13    POLYGON ((-114.05060 37.00040, -114.05175 37.0...\n14    MULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n15    MULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n16    MULTIPOLYGON (((-65.58733 18.38199, -65.59122 ...\n17    MULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n18    MULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n19    MULTIPOLYGON (((-164.97620 54.13459, -164.9377...\n20    POLYGON ((-109.04522 36.99908, -109.04524 36.9...\n21    POLYGON ((-94.55929 36.49950, -94.51948 36.499...\n22    MULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n23    POLYGON ((-102.04224 36.99308, -102.05450 36.9...\n24    MULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n25    MULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n26    POLYGON ((-77.03860 38.79151, -77.03890 38.800...\n27    MULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n28    POLYGON ((-81.44412 30.70971, -81.44872 30.709...\n29    MULTIPOLYGON (((-171.73761 25.79210, -171.7223...\n30    POLYGON ((-111.04669 42.00157, -111.41587 42.0...\n31    POLYGON ((-87.53233 39.99778, -87.53254 39.987...\n32    POLYGON ((-88.02803 37.79922, -88.02938 37.803...\n33    POLYGON ((-95.76565 40.58521, -95.75889 40.588...\n34    POLYGON ((-94.61808 36.99813, -94.62522 36.998...\n35    MULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n36    MULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n37    POLYGON ((-91.37161 43.50095, -91.37695 43.500...\n38    MULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n39    POLYGON ((-89.53910 36.49820, -89.53452 36.491...\n40    POLYGON ((-95.76565 40.58521, -95.76853 40.583...\n41    MULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n42    POLYGON ((-109.05004 31.33250, -109.05017 31.4...\n43    POLYGON ((-96.56328 45.93524, -96.57690 45.935...\n44    POLYGON ((-94.61792 36.49941, -94.61531 36.484...\n45    POLYGON ((-117.22007 44.30138, -117.22245 44.2...\n46    POLYGON ((-78.54109 33.85111, -78.55394 33.847...\n47    POLYGON ((-96.44341 42.48949, -96.45971 42.486...\n48    POLYGON ((-72.04008 44.15575, -72.04271 44.152...\n49    MULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n50    POLYGON ((-81.96830 37.53780, -81.96540 37.541...\n51    POLYGON ((-109.05008 41.00066, -109.17368 41.0...\nName: geometry, dtype: geometry\n\n\n\ngdf.iloc[0].geometry\n\n\n\n\n\ngdf.shape\n\n(52, 6)\n\n\n\ngdf.head(52)\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      NAME\n      LSAD\n      CENSUSAREA\n      geometry\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n    \n    \n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n    \n    \n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n    \n    \n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n    \n    \n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n    \n    \n      5\n      0400000US34\n      34\n      New Jersey\n      \n      7354.220\n      POLYGON ((-75.52684 39.65571, -75.52634 39.656...\n    \n    \n      6\n      0400000US36\n      36\n      New York\n      \n      47126.399\n      MULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n    \n    \n      7\n      0400000US37\n      37\n      North Carolina\n      \n      48617.905\n      MULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n    \n    \n      8\n      0400000US39\n      39\n      Ohio\n      \n      40860.694\n      MULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n    \n    \n      9\n      0400000US42\n      42\n      Pennsylvania\n      \n      44742.703\n      POLYGON ((-75.41504 39.80179, -75.42804 39.809...\n    \n    \n      10\n      0400000US44\n      44\n      Rhode Island\n      \n      1033.814\n      MULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n    \n    \n      11\n      0400000US47\n      47\n      Tennessee\n      \n      41234.896\n      POLYGON ((-81.67754 36.58812, -81.68014 36.585...\n    \n    \n      12\n      0400000US48\n      48\n      Texas\n      \n      261231.711\n      MULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n    \n    \n      13\n      0400000US49\n      49\n      Utah\n      \n      82169.620\n      POLYGON ((-114.05060 37.00040, -114.05175 37.0...\n    \n    \n      14\n      0400000US53\n      53\n      Washington\n      \n      66455.521\n      MULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n    \n    \n      15\n      0400000US55\n      55\n      Wisconsin\n      \n      54157.805\n      MULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n    \n    \n      16\n      0400000US72\n      72\n      Puerto Rico\n      \n      3423.775\n      MULTIPOLYGON (((-65.58733 18.38199, -65.59122 ...\n    \n    \n      17\n      0400000US24\n      24\n      Maryland\n      \n      9707.241\n      MULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n    \n    \n      18\n      0400000US01\n      01\n      Alabama\n      \n      50645.326\n      MULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n    \n    \n      19\n      0400000US02\n      02\n      Alaska\n      \n      570640.950\n      MULTIPOLYGON (((-164.97620 54.13459, -164.9377...\n    \n    \n      20\n      0400000US04\n      04\n      Arizona\n      \n      113594.084\n      POLYGON ((-109.04522 36.99908, -109.04524 36.9...\n    \n    \n      21\n      0400000US05\n      05\n      Arkansas\n      \n      52035.477\n      POLYGON ((-94.55929 36.49950, -94.51948 36.499...\n    \n    \n      22\n      0400000US06\n      06\n      California\n      \n      155779.220\n      MULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n    \n    \n      23\n      0400000US08\n      08\n      Colorado\n      \n      103641.888\n      POLYGON ((-102.04224 36.99308, -102.05450 36.9...\n    \n    \n      24\n      0400000US09\n      09\n      Connecticut\n      \n      4842.355\n      MULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n    \n    \n      25\n      0400000US10\n      10\n      Delaware\n      \n      1948.543\n      MULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n    \n    \n      26\n      0400000US11\n      11\n      District of Columbia\n      \n      61.048\n      POLYGON ((-77.03860 38.79151, -77.03890 38.800...\n    \n    \n      27\n      0400000US12\n      12\n      Florida\n      \n      53624.759\n      MULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n    \n    \n      28\n      0400000US13\n      13\n      Georgia\n      \n      57513.485\n      POLYGON ((-81.44412 30.70971, -81.44872 30.709...\n    \n    \n      29\n      0400000US15\n      15\n      Hawaii\n      \n      6422.628\n      MULTIPOLYGON (((-171.73761 25.79210, -171.7223...\n    \n    \n      30\n      0400000US16\n      16\n      Idaho\n      \n      82643.117\n      POLYGON ((-111.04669 42.00157, -111.41587 42.0...\n    \n    \n      31\n      0400000US17\n      17\n      Illinois\n      \n      55518.930\n      POLYGON ((-87.53233 39.99778, -87.53254 39.987...\n    \n    \n      32\n      0400000US18\n      18\n      Indiana\n      \n      35826.109\n      POLYGON ((-88.02803 37.79922, -88.02938 37.803...\n    \n    \n      33\n      0400000US19\n      19\n      Iowa\n      \n      55857.130\n      POLYGON ((-95.76565 40.58521, -95.75889 40.588...\n    \n    \n      34\n      0400000US20\n      20\n      Kansas\n      \n      81758.717\n      POLYGON ((-94.61808 36.99813, -94.62522 36.998...\n    \n    \n      35\n      0400000US21\n      21\n      Kentucky\n      \n      39486.338\n      MULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n    \n    \n      36\n      0400000US22\n      22\n      Louisiana\n      \n      43203.905\n      MULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n    \n    \n      37\n      0400000US27\n      27\n      Minnesota\n      \n      79626.743\n      POLYGON ((-91.37161 43.50095, -91.37695 43.500...\n    \n    \n      38\n      0400000US28\n      28\n      Mississippi\n      \n      46923.274\n      MULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n    \n    \n      39\n      0400000US29\n      29\n      Missouri\n      \n      68741.522\n      POLYGON ((-89.53910 36.49820, -89.53452 36.491...\n    \n    \n      40\n      0400000US31\n      31\n      Nebraska\n      \n      76824.171\n      POLYGON ((-95.76565 40.58521, -95.76853 40.583...\n    \n    \n      41\n      0400000US33\n      33\n      New Hampshire\n      \n      8952.651\n      MULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n    \n    \n      42\n      0400000US35\n      35\n      New Mexico\n      \n      121298.148\n      POLYGON ((-109.05004 31.33250, -109.05017 31.4...\n    \n    \n      43\n      0400000US38\n      38\n      North Dakota\n      \n      69000.798\n      POLYGON ((-96.56328 45.93524, -96.57690 45.935...\n    \n    \n      44\n      0400000US40\n      40\n      Oklahoma\n      \n      68594.921\n      POLYGON ((-94.61792 36.49941, -94.61531 36.484...\n    \n    \n      45\n      0400000US41\n      41\n      Oregon\n      \n      95988.013\n      POLYGON ((-117.22007 44.30138, -117.22245 44.2...\n    \n    \n      46\n      0400000US45\n      45\n      South Carolina\n      \n      30060.696\n      POLYGON ((-78.54109 33.85111, -78.55394 33.847...\n    \n    \n      47\n      0400000US46\n      46\n      South Dakota\n      \n      75811.000\n      POLYGON ((-96.44341 42.48949, -96.45971 42.486...\n    \n    \n      48\n      0400000US50\n      50\n      Vermont\n      \n      9216.657\n      POLYGON ((-72.04008 44.15575, -72.04271 44.152...\n    \n    \n      49\n      0400000US51\n      51\n      Virginia\n      \n      39490.086\n      MULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n    \n    \n      50\n      0400000US54\n      54\n      West Virginia\n      \n      24038.210\n      POLYGON ((-81.96830 37.53780, -81.96540 37.541...\n    \n    \n      51\n      0400000US56\n      56\n      Wyoming\n      \n      97093.141\n      POLYGON ((-109.05008 41.00066, -109.17368 41.0...\n    \n  \n\n\n\n\n\ndrop_states = ['15', '02', '72'] # HA, AK, PR\n\n\ngdf[gdf.STATE.isin(drop_states)].plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf[~gdf.STATE.isin(drop_states)].plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf = gdf[~gdf.STATE.isin(drop_states)]\n\n\ngdf.shape\n\n(49, 6)\n\n\n\ngdf.index\n\nIndex([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 17, 18,\n       20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51],\n      dtype='int64')\n\n\n\ngdf.reset_index(inplace=True)\n\n\ngdf.index\n\nRangeIndex(start=0, stop=49, step=1)\n\n\n\ngdf.head(49)\n\n\n\n\n\n  \n    \n      \n      index\n      GEO_ID\n      STATE\n      NAME\n      LSAD\n      CENSUSAREA\n      geometry\n    \n  \n  \n    \n      0\n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n    \n    \n      1\n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n    \n    \n      2\n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n    \n    \n      3\n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n    \n    \n      4\n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n    \n    \n      5\n      5\n      0400000US34\n      34\n      New Jersey\n      \n      7354.220\n      POLYGON ((-75.52684 39.65571, -75.52634 39.656...\n    \n    \n      6\n      6\n      0400000US36\n      36\n      New York\n      \n      47126.399\n      MULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n    \n    \n      7\n      7\n      0400000US37\n      37\n      North Carolina\n      \n      48617.905\n      MULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n    \n    \n      8\n      8\n      0400000US39\n      39\n      Ohio\n      \n      40860.694\n      MULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n    \n    \n      9\n      9\n      0400000US42\n      42\n      Pennsylvania\n      \n      44742.703\n      POLYGON ((-75.41504 39.80179, -75.42804 39.809...\n    \n    \n      10\n      10\n      0400000US44\n      44\n      Rhode Island\n      \n      1033.814\n      MULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n    \n    \n      11\n      11\n      0400000US47\n      47\n      Tennessee\n      \n      41234.896\n      POLYGON ((-81.67754 36.58812, -81.68014 36.585...\n    \n    \n      12\n      12\n      0400000US48\n      48\n      Texas\n      \n      261231.711\n      MULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n    \n    \n      13\n      13\n      0400000US49\n      49\n      Utah\n      \n      82169.620\n      POLYGON ((-114.05060 37.00040, -114.05175 37.0...\n    \n    \n      14\n      14\n      0400000US53\n      53\n      Washington\n      \n      66455.521\n      MULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n    \n    \n      15\n      15\n      0400000US55\n      55\n      Wisconsin\n      \n      54157.805\n      MULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n    \n    \n      16\n      17\n      0400000US24\n      24\n      Maryland\n      \n      9707.241\n      MULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n    \n    \n      17\n      18\n      0400000US01\n      01\n      Alabama\n      \n      50645.326\n      MULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n    \n    \n      18\n      20\n      0400000US04\n      04\n      Arizona\n      \n      113594.084\n      POLYGON ((-109.04522 36.99908, -109.04524 36.9...\n    \n    \n      19\n      21\n      0400000US05\n      05\n      Arkansas\n      \n      52035.477\n      POLYGON ((-94.55929 36.49950, -94.51948 36.499...\n    \n    \n      20\n      22\n      0400000US06\n      06\n      California\n      \n      155779.220\n      MULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n    \n    \n      21\n      23\n      0400000US08\n      08\n      Colorado\n      \n      103641.888\n      POLYGON ((-102.04224 36.99308, -102.05450 36.9...\n    \n    \n      22\n      24\n      0400000US09\n      09\n      Connecticut\n      \n      4842.355\n      MULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n    \n    \n      23\n      25\n      0400000US10\n      10\n      Delaware\n      \n      1948.543\n      MULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n    \n    \n      24\n      26\n      0400000US11\n      11\n      District of Columbia\n      \n      61.048\n      POLYGON ((-77.03860 38.79151, -77.03890 38.800...\n    \n    \n      25\n      27\n      0400000US12\n      12\n      Florida\n      \n      53624.759\n      MULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n    \n    \n      26\n      28\n      0400000US13\n      13\n      Georgia\n      \n      57513.485\n      POLYGON ((-81.44412 30.70971, -81.44872 30.709...\n    \n    \n      27\n      30\n      0400000US16\n      16\n      Idaho\n      \n      82643.117\n      POLYGON ((-111.04669 42.00157, -111.41587 42.0...\n    \n    \n      28\n      31\n      0400000US17\n      17\n      Illinois\n      \n      55518.930\n      POLYGON ((-87.53233 39.99778, -87.53254 39.987...\n    \n    \n      29\n      32\n      0400000US18\n      18\n      Indiana\n      \n      35826.109\n      POLYGON ((-88.02803 37.79922, -88.02938 37.803...\n    \n    \n      30\n      33\n      0400000US19\n      19\n      Iowa\n      \n      55857.130\n      POLYGON ((-95.76565 40.58521, -95.75889 40.588...\n    \n    \n      31\n      34\n      0400000US20\n      20\n      Kansas\n      \n      81758.717\n      POLYGON ((-94.61808 36.99813, -94.62522 36.998...\n    \n    \n      32\n      35\n      0400000US21\n      21\n      Kentucky\n      \n      39486.338\n      MULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n    \n    \n      33\n      36\n      0400000US22\n      22\n      Louisiana\n      \n      43203.905\n      MULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n    \n    \n      34\n      37\n      0400000US27\n      27\n      Minnesota\n      \n      79626.743\n      POLYGON ((-91.37161 43.50095, -91.37695 43.500...\n    \n    \n      35\n      38\n      0400000US28\n      28\n      Mississippi\n      \n      46923.274\n      MULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n    \n    \n      36\n      39\n      0400000US29\n      29\n      Missouri\n      \n      68741.522\n      POLYGON ((-89.53910 36.49820, -89.53452 36.491...\n    \n    \n      37\n      40\n      0400000US31\n      31\n      Nebraska\n      \n      76824.171\n      POLYGON ((-95.76565 40.58521, -95.76853 40.583...\n    \n    \n      38\n      41\n      0400000US33\n      33\n      New Hampshire\n      \n      8952.651\n      MULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n    \n    \n      39\n      42\n      0400000US35\n      35\n      New Mexico\n      \n      121298.148\n      POLYGON ((-109.05004 31.33250, -109.05017 31.4...\n    \n    \n      40\n      43\n      0400000US38\n      38\n      North Dakota\n      \n      69000.798\n      POLYGON ((-96.56328 45.93524, -96.57690 45.935...\n    \n    \n      41\n      44\n      0400000US40\n      40\n      Oklahoma\n      \n      68594.921\n      POLYGON ((-94.61792 36.49941, -94.61531 36.484...\n    \n    \n      42\n      45\n      0400000US41\n      41\n      Oregon\n      \n      95988.013\n      POLYGON ((-117.22007 44.30138, -117.22245 44.2...\n    \n    \n      43\n      46\n      0400000US45\n      45\n      South Carolina\n      \n      30060.696\n      POLYGON ((-78.54109 33.85111, -78.55394 33.847...\n    \n    \n      44\n      47\n      0400000US46\n      46\n      South Dakota\n      \n      75811.000\n      POLYGON ((-96.44341 42.48949, -96.45971 42.486...\n    \n    \n      45\n      48\n      0400000US50\n      50\n      Vermont\n      \n      9216.657\n      POLYGON ((-72.04008 44.15575, -72.04271 44.152...\n    \n    \n      46\n      49\n      0400000US51\n      51\n      Virginia\n      \n      39490.086\n      MULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n    \n    \n      47\n      50\n      0400000US54\n      54\n      West Virginia\n      \n      24038.210\n      POLYGON ((-81.96830 37.53780, -81.96540 37.541...\n    \n    \n      48\n      51\n      0400000US56\n      56\n      Wyoming\n      \n      97093.141\n      POLYGON ((-109.05008 41.00066, -109.17368 41.0...\n    \n  \n\n\n\n\n\n\n\n\ngdf.centroid\n\n/tmp/ipykernel_3120570/2017122361.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf.centroid\n\n\n0      POINT (-69.22532 45.36948)\n1      POINT (-71.79546 42.25229)\n2      POINT (-85.43751 44.35323)\n3     POINT (-109.64507 47.03355)\n4     POINT (-116.65540 39.35646)\n5      POINT (-74.66099 40.18393)\n6      POINT (-75.50198 42.93930)\n7      POINT (-79.35542 35.53980)\n8      POINT (-82.79018 40.29333)\n9      POINT (-77.79953 40.87382)\n10     POINT (-71.55250 41.67619)\n11     POINT (-86.34329 35.84299)\n12     POINT (-99.35528 31.49051)\n13    POINT (-111.67820 39.32379)\n14    POINT (-120.45017 47.38108)\n15     POINT (-90.01113 44.63829)\n16     POINT (-76.76446 39.03041)\n17     POINT (-86.82843 32.78969)\n18    POINT (-111.66458 34.29326)\n19     POINT (-92.43928 34.89974)\n20    POINT (-119.61077 37.24612)\n21    POINT (-105.54782 38.99855)\n22     POINT (-72.72576 41.62055)\n23     POINT (-75.50018 38.99178)\n24     POINT (-77.01630 38.90473)\n25     POINT (-82.50162 28.64096)\n26     POINT (-83.44606 32.64908)\n27    POINT (-114.65933 44.38912)\n28     POINT (-89.19828 40.06474)\n29     POINT (-86.27529 39.90853)\n30     POINT (-93.50004 42.07462)\n31     POINT (-98.38022 38.48470)\n32     POINT (-85.29049 37.52666)\n33     POINT (-91.98048 31.05264)\n34     POINT (-94.30870 46.31645)\n35     POINT (-89.66510 32.75040)\n36     POINT (-92.47742 38.36762)\n37     POINT (-99.81080 41.52715)\n38     POINT (-71.57760 43.68569)\n39    POINT (-106.10838 34.42137)\n40    POINT (-100.46931 47.44634)\n41     POINT (-97.50844 35.58350)\n42    POINT (-120.55529 43.93673)\n43     POINT (-80.89550 33.90710)\n44    POINT (-100.23049 44.43615)\n45     POINT (-72.66272 44.07518)\n46     POINT (-78.80562 37.51539)\n47     POINT (-80.61385 38.64251)\n48    POINT (-107.55145 42.99964)\ndtype: geometry\n\n\n\ngdf.crs\n\n<Geographic 2D CRS: EPSG:4326>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf.to_crs(gdf.estimate_utm_crs()).plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf = gdf.to_crs(gdf.estimate_utm_crs())\n\n\ngdf.centroid\n\n0      POINT (2359392.452 5305169.775)\n1      POINT (2253174.905 4901469.336)\n2      POINT (1108124.800 4935453.192)\n3      POINT (-765955.563 5348067.290)\n4     POINT (-1548246.254 4630302.336)\n5      POINT (2065285.016 4612318.097)\n6      POINT (1930049.196 4904992.641)\n7      POINT (1742974.165 4021487.098)\n8      POINT (1368634.294 4510266.503)\n9      POINT (1784134.621 4638257.315)\n10     POINT (2289313.142 4842131.086)\n11     POINT (1102080.849 3988935.477)\n12     POINT (-103892.519 3500028.226)\n13    POINT (-1115997.209 4520861.031)\n14    POINT (-1565541.918 5625943.989)\n15      POINT (738620.228 4945674.977)\n16     POINT (1910429.070 4447760.368)\n17     POINT (1078425.172 3643488.570)\n18    POINT (-1230212.721 3954699.221)\n19      POINT (550933.347 3861451.888)\n20    POINT (-1870826.275 4466915.330)\n21     POINT (-590190.342 4392344.096)\n22     POINT (2192677.610 4811150.640)\n23     POINT (2020359.310 4464239.060)\n24     POINT (1889584.643 4429716.358)\n25     POINT (1535090.511 3210901.406)\n26     POINT (1399285.825 3651528.582)\n27    POINT (-1223231.299 5144300.473)\n28      POINT (824539.285 4439350.669)\n29     POINT (1074696.801 4437810.568)\n30      POINT (458939.639 4658274.632)\n31       POINT (29865.949 4274404.492)\n32     POINT (1181577.693 4182456.487)\n33      POINT (598555.176 3435096.661)\n34      POINT (399444.786 5126179.145)\n35      POINT (812334.714 3626923.994)\n36      POINT (547395.056 4245527.933)\n37      POINT (-68074.613 4621136.163)\n38     POINT (2227578.113 5064712.956)\n39     POINT (-710483.211 3886170.809)\n40      POINT (-63437.281 5282398.408)\n41       POINT (91977.904 3947894.482)\n42    POINT (-1716081.976 5249621.330)\n43     POINT (1623336.735 3818285.055)\n44      POINT (-76107.494 4946849.414)\n45     POINT (2128860.270 5085004.659)\n46     POINT (1758189.886 4249101.361)\n47     POINT (1579306.753 4350654.685)\n48     POINT (-688676.799 4864507.482)\ndtype: geometry\n\n\n\ngdf.crs\n\n<Projected CRS: EPSG:32615>\nName: WGS 84 / UTM zone 15N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 96°W and 90°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - Manitoba; Nunavut; Ontario. Ecuador -Galapagos. Guatemala. Mexico. United States (USA).\n- bounds: (-96.0, 0.0, -90.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 15N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf['centroid'] = gdf.centroid\n\n\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      index\n      GEO_ID\n      STATE\n      NAME\n      LSAD\n      CENSUSAREA\n      geometry\n      centroid\n    \n  \n  \n    \n      0\n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((2516172.424 5253443.650, 25164...\n      POINT (2359392.452 5305169.775)\n    \n    \n      1\n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((2351724.597 4850457.653, 23526...\n      POINT (2253174.905 4901469.336)\n    \n    \n      2\n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((821167.988 5338182.388, 821794...\n      POINT (1108124.800 4935453.192)\n    \n    \n      3\n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-371533.418 5042503.702, -386687.255...\n      POINT (-765955.563 5348067.290)\n    \n    \n      4\n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-1384104.733 4308747.816, -1385163.9...\n      POINT (-1548246.254 4630302.336)\n    \n  \n\n\n\n\n\nbase = gdf.plot()\ngdf.centroid.plot(ax=base, color='r')\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\n\ngdf.geometry\n\n0     MULTIPOLYGON (((2516172.424 5253443.650, 25164...\n1     MULTIPOLYGON (((2351724.597 4850457.653, 23526...\n2     MULTIPOLYGON (((821167.988 5338182.388, 821794...\n3     POLYGON ((-371533.418 5042503.702, -386687.255...\n4     POLYGON ((-1384104.733 4308747.816, -1385163.9...\n5     POLYGON ((2003056.105 4538350.496, 2003083.601...\n6     MULTIPOLYGON (((2267648.358 4790204.443, 22689...\n7     MULTIPOLYGON (((1438177.023 4038770.483, 14383...\n8     MULTIPOLYGON (((1347764.520 4669512.453, 13480...\n9     POLYGON ((2009412.336 4556637.291, 2008122.809...\n10    MULTIPOLYGON (((2312801.421 4845029.912, 23131...\n11    POLYGON ((1514737.024 4109398.060, 1514537.561...\n12    MULTIPOLYGON (((92911.351 3092597.856, 92990.0...\n13    POLYGON ((-1384104.733 4308747.816, -1381904.7...\n14    MULTIPOLYGON (((-1682205.672 5879501.588, -167...\n15    MULTIPOLYGON (((693374.944 5210974.031, 693159...\n16    MULTIPOLYGON (((1987068.430 4366526.262, 19890...\n17    MULTIPOLYGON (((1264668.058 3457276.553, 12625...\n18    POLYGON ((-932699.253 4217338.169, -933275.202...\n19    POLYGON ((360351.380 4040482.637, 363916.663 4...\n20    MULTIPOLYGON (((-2114916.187 4624534.270, -211...\n21    POLYGON ((-305537.860 4132535.682, -306632.528...\n22    MULTIPOLYGON (((2273709.589 4796000.071, 22727...\n23    MULTIPOLYGON (((2000818.277 4534887.709, 20008...\n24    POLYGON ((1889905.340 4416707.434, 1889692.665...\n25    MULTIPOLYGON (((1260129.872 3309151.351, 12619...\n26    POLYGON ((1610127.048 3455093.398, 1609687.082...\n27    POLYGON ((-996830.450 4810504.086, -1027552.17...\n28    POLYGON ((966857.622 4441852.185, 966910.074 4...\n29    POLYGON ((937850.748 4195199.124, 937705.699 4...\n30    POLYGON ((265938.864 4496389.554, 266523.302 4...\n31    POLYGON ((356020.762 4095889.131, 355386.048 4...\n32    MULTIPOLYGON (((1335046.637 4091312.569, 13350...\n33    MULTIPOLYGON (((899967.144 3298554.678, 897713...\n34    POLYGON ((631645.161 4817734.258, 631214.291 4...\n35    MULTIPOLYGON (((912838.208 3354370.602, 918103...\n36    POLYGON ((810004.565 4044781.403, 810441.658 4...\n37    POLYGON ((265938.864 4496389.554, 265688.248 4...\n38    MULTIPOLYGON (((2183943.003 4939995.423, 21838...\n39    POLYGON ((-1036062.528 3580182.026, -1033579.7...\n40    POLYGON ((223770.899 5093027.974, 222715.445 5...\n41    POLYGON ((355100.062 4040560.166, 355306.819 4...\n42    POLYGON ((-1431865.838 5199448.159, -1432139.7...\n43    POLYGON ((1842940.161 3841258.252, 1841789.432...\n44    POLYGON ((217005.965 4709874.192, 215650.454 4...\n45    POLYGON ((2176439.257 5107755.756, 2176332.418...\n46    MULTIPOLYGON (((1994538.922 4338939.700, 19949...\n47    POLYGON ((1476144.128 4212196.934, 1476356.502...\n48    POLYGON ((-852129.655 4664855.469, -862571.421...\nName: geometry, dtype: geometry\n\n\n\ngdf.set_geometry('centroid').plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf = gdf.set_geometry('centroid')\n\n\ngdf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf = gdf.set_geometry('geometry')\ngdf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\n\n\n\ngdf['dissolve_var'] = 1 # all observations have same value for this variable\nus = gdf.dissolve(by='dissolve_var')\n\n\nus.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\nus.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      index\n      GEO_ID\n      STATE\n      NAME\n      LSAD\n      CENSUSAREA\n      centroid\n    \n    \n      dissolve_var\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      MULTIPOLYGON (((-1922552.872 3962305.133, -192...\n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      POINT (2359392.452 5305169.775)\n    \n  \n\n\n\n\n\nus_cent = us.centroid\nbase = gdf.plot()\nus_cent.plot(ax=base, color='r')\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\n\n\nbuffer = us_cent.buffer(804.672*1000)\nbuffer.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\nbase = gdf.plot()\nbuffer.plot(ax=base, color='grey')\nus_cent.plot(ax=base, color='r')\n\n<AxesSubplot:>\n\n\n\n\n\n\nbase = gdf.plot()\nbuffer.plot(ax=base, color='grey', alpha=0.4)\nus_cent.plot(ax=base, color='r')\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf.sindex.query(buffer, predicate='intersects')\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [19, 36, 12, 39, 41, 31, 21, 37, 28, 30, 15, 34, 48, 44, 40,  3]])\n\n\n\nrfirst, states_intersecting = gdf.sindex.query(buffer, predicate='intersects')\n\n\ngdf.iloc[states_intersecting].plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf[~gdf.index.isin(states_intersecting)].plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\nus.envelope\n\ndissolve_var\n1    POLYGON ((-2186034.766 2758545.507, 2557956.87...\ndtype: geometry\n\n\n\nbb = us.envelope\nbb.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\nbase = bb.plot(color='b')\nus.plot(ax=base, color='w') # 2 layers\n\n<AxesSubplot:>\n\n\n\n\n\n\nbb_less_49 = bb.difference(us.geometry) # 1 layer\nbb_less_49.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf.sindex.query(bb_less_49, predicate='touches')\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0],\n       [25, 33, 35, 17, 26, 43, 12, 39, 18, 20,  7, 46, 16, 23,  5,  9,\n        22, 10, 28, 29,  8, 15,  2, 34,  1,  6, 45, 38,  0, 27, 42, 40,\n         3, 14]])\n\n\n\n_,border_states = gdf.sindex.query(bb_less_49, predicate='touches')\ngdf[gdf.index.isin(border_states)].plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\n\nimport numpy as np\n\nborder_dummy = np.zeros(49, 'int')\nborder_dummy[border_states]=1\nborder_dummy\n\narray([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n       0, 1, 1, 0, 0])\n\n\n\ngdf['border'] = border_dummy\ngdf.plot(column='border', categorical=True, legend=True)\n\n<AxesSubplot:>\n\n\n\n\n\n\ngdf.explore(column='border', categorical=True, legend=False)\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\ngdf.explore(column='border', categorical=True)\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\ngdf.explore?\n\n\nSignature: gdf.explore(*args, **kwargs)\nDocstring:\nInteractive map based on GeoPandas and folium/leaflet.js\nGenerate an interactive leaflet map based on :class:`~geopandas.GeoDataFrame`\nParameters\n----------\ncolumn : str, np.array, pd.Series (default None)\n    The name of the dataframe column, :class:`numpy.array`,\n    or :class:`pandas.Series` to be plotted. If :class:`numpy.array` or\n    :class:`pandas.Series` are used then it must have same length as dataframe.\ncmap : str, matplotlib.Colormap, branca.colormap or function (default None)\n    The name of a colormap recognized by ``matplotlib``, a list-like of colors,\n    :class:`matplotlib.colors.Colormap`, a :class:`branca.colormap.ColorMap` or\n    function that returns a named color or hex based on the column\n    value, e.g.::\n        def my_colormap(value):  # scalar value defined in 'column'\n            if value > 1:\n                return \"green\"\n            return \"red\"\ncolor : str, array-like (default None)\n    Named color or a list-like of colors (named or hex).\nm : folium.Map (default None)\n    Existing map instance on which to draw the plot.\ntiles : str, xyzservices.TileProvider (default 'OpenStreetMap Mapnik')\n    Map tileset to use. Can choose from the list supported by folium, query a\n    :class:`xyzservices.TileProvider` by a name from ``xyzservices.providers``,\n    pass :class:`xyzservices.TileProvider` object or pass custom XYZ URL.\n    The current list of built-in providers (when ``xyzservices`` is not available):\n    ``[\"OpenStreetMap\", \"Stamen Terrain\", “Stamen Toner\", “Stamen Watercolor\"\n    \"CartoDB positron\", “CartoDB dark_matter\"]``\n    You can pass a custom tileset to Folium by passing a Leaflet-style URL\n    to the tiles parameter: ``http://{s}.yourtiles.com/{z}/{x}/{y}.png``.\n    Be sure to check their terms and conditions and to provide attribution with\n    the ``attr`` keyword.\nattr : str (default None)\n    Map tile attribution; only required if passing custom tile URL.\ntooltip : bool, str, int, list (default True)\n    Display GeoDataFrame attributes when hovering over the object.\n    ``True`` includes all columns. ``False`` removes tooltip. Pass string or list of\n    strings to specify a column(s). Integer specifies first n columns to be\n    included. Defaults to ``True``.\npopup : bool, str, int, list (default False)\n    Input GeoDataFrame attributes for object displayed when clicking.\n    ``True`` includes all columns. ``False`` removes popup. Pass string or list of\n    strings to specify a column(s). Integer specifies first n columns to be\n    included. Defaults to ``False``.\nhighlight : bool (default True)\n    Enable highlight functionality when hovering over a geometry.\ncategorical : bool (default False)\n    If ``False``, ``cmap`` will reflect numerical values of the\n    column being plotted. For non-numerical columns, this\n    will be set to True.\nlegend : bool (default True)\n    Plot a legend in choropleth plots.\n    Ignored if no ``column`` is given.\nscheme : str (default None)\n    Name of a choropleth classification scheme (requires ``mapclassify`` >= 2.4.0).\n    A :func:`mapclassify.classify` will be used\n    under the hood. Supported are all schemes provided by ``mapclassify`` (e.g.\n    ``'BoxPlot'``, ``'EqualInterval'``, ``'FisherJenks'``, ``'FisherJenksSampled'``,\n    ``'HeadTailBreaks'``, ``'JenksCaspall'``, ``'JenksCaspallForced'``,\n    ``'JenksCaspallSampled'``, ``'MaxP'``, ``'MaximumBreaks'``,\n    ``'NaturalBreaks'``, ``'Quantiles'``, ``'Percentiles'``, ``'StdMean'``,\n    ``'UserDefined'``). Arguments can be passed in ``classification_kwds``.\nk : int (default 5)\n    Number of classes\nvmin : None or float (default None)\n    Minimum value of ``cmap``. If ``None``, the minimum data value\n    in the column to be plotted is used.\nvmax : None or float (default None)\n    Maximum value of ``cmap``. If ``None``, the maximum data value\n    in the column to be plotted is used.\nwidth : pixel int or percentage string (default: '100%')\n    Width of the folium :class:`~folium.folium.Map`. If the argument\n    m is given explicitly, width is ignored.\nheight : pixel int or percentage string (default: '100%')\n    Height of the folium :class:`~folium.folium.Map`. If the argument\n    m is given explicitly, height is ignored.\ncategories : list-like\n    Ordered list-like object of categories to be used for categorical plot.\nclassification_kwds : dict (default None)\n    Keyword arguments to pass to mapclassify\ncontrol_scale : bool, (default True)\n    Whether to add a control scale on the map.\nmarker_type : str, folium.Circle, folium.CircleMarker, folium.Marker (default None)\n    Allowed string options are ('marker', 'circle', 'circle_marker'). Defaults to\n    folium.CircleMarker.\nmarker_kwds: dict (default {})\n    Additional keywords to be passed to the selected ``marker_type``, e.g.:\n    radius : float (default 2 for ``circle_marker`` and 50 for ``circle``))\n        Radius of the circle, in meters (for ``circle``) or pixels\n        (for ``circle_marker``).\n    fill : bool (default True)\n        Whether to fill the ``circle`` or ``circle_marker`` with color.\n    icon : folium.map.Icon\n        the :class:`folium.map.Icon` object to use to render the marker.\n    draggable : bool (default False)\n        Set to True to be able to drag the marker around the map.\nstyle_kwds : dict (default {})\n    Additional style to be passed to folium ``style_function``:\n    stroke : bool (default True)\n        Whether to draw stroke along the path. Set it to ``False`` to\n        disable borders on polygons or circles.\n    color : str\n        Stroke color\n    weight : int\n        Stroke width in pixels\n    opacity : float (default 1.0)\n        Stroke opacity\n    fill : boolean (default True)\n        Whether to fill the path with color. Set it to ``False`` to\n        disable filling on polygons or circles.\n    fillColor : str\n        Fill color. Defaults to the value of the color option\n    fillOpacity : float (default 0.5)\n        Fill opacity.\n    style_function : callable\n        Function mapping a GeoJson Feature to a style ``dict``.\n        * Style properties :func:`folium.vector_layers.path_options`\n        * GeoJson features :class:`GeoDataFrame.__geo_interface__`\n        e.g.::\n            lambda x: {\"color\":\"red\" if x[\"properties\"][\"gdp_md_est\"]<10**6\n                                         else \"blue\"}\n    Plus all supported by :func:`folium.vector_layers.path_options`. See the\n    documentation of :class:`folium.features.GeoJson` for details.\nhighlight_kwds : dict (default {})\n    Style to be passed to folium highlight_function. Uses the same keywords\n    as ``style_kwds``. When empty, defaults to ``{\"fillOpacity\": 0.75}``.\ntooltip_kwds : dict (default {})\n    Additional keywords to be passed to :class:`folium.features.GeoJsonTooltip`,\n    e.g. ``aliases``, ``labels``, or ``sticky``.\npopup_kwds : dict (default {})\n    Additional keywords to be passed to :class:`folium.features.GeoJsonPopup`,\n    e.g. ``aliases`` or ``labels``.\nlegend_kwds : dict (default {})\n    Additional keywords to be passed to the legend.\n    Currently supported customisation:\n    caption : string\n        Custom caption of the legend. Defaults to the column name.\n    Additional accepted keywords when ``scheme`` is specified:\n    colorbar : bool (default True)\n        An option to control the style of the legend. If True, continuous\n        colorbar will be used. If False, categorical legend will be used for bins.\n    scale : bool (default True)\n        Scale bins along the colorbar axis according to the bin edges (True)\n        or use the equal length for each bin (False)\n    fmt : string (default \"{:.2f}\")\n        A formatting specification for the bin edges of the classes in the\n        legend. For example, to have no decimals: ``{\"fmt\": \"{:.0f}\"}``. Applies\n        if ``colorbar=False``.\n    labels : list-like\n        A list of legend labels to override the auto-generated labels.\n        Needs to have the same number of elements as the number of\n        classes (`k`). Applies if ``colorbar=False``.\n    interval : boolean (default False)\n        An option to control brackets from mapclassify legend.\n        If True, open/closed interval brackets are shown in the legend.\n        Applies if ``colorbar=False``.\n    max_labels : int, default 10\n        Maximum number of colorbar tick labels (requires branca>=0.5.0)\nmap_kwds : dict (default {})\n    Additional keywords to be passed to folium :class:`~folium.folium.Map`,\n    e.g. ``dragging``, or ``scrollWheelZoom``.\n**kwargs : dict\n    Additional options to be passed on to the folium object.\nReturns\n-------\nm : folium.folium.Map\n    folium :class:`~folium.folium.Map` instance\nExamples\n--------\n>>> import geodatasets\n>>> df = geopandas.read_file(\n...     geodatasets.get_path(\"geoda.chicago_health\")\n... )\n>>> df.head(2)  # doctest: +SKIP\n   ComAreaID  ...                                           geometry\n0         35  ...  POLYGON ((-87.60914 41.84469, -87.60915 41.844...\n1         36  ...  POLYGON ((-87.59215 41.81693, -87.59231 41.816...\n[2 rows x 87 columns]\n>>> df.explore(\"Pop2012\", cmap=\"Blues\")  # doctest: +SKIP\nFile:      /opt/tljh/user/lib/python3.9/site-packages/geopandas/geodataframe.py\nType:      method"
  },
  {
    "objectID": "lectures/week-05/2023-09-20.html",
    "href": "lectures/week-05/2023-09-20.html",
    "title": "Geog385F23",
    "section": "",
    "text": "import pandas\n\n\ndf = pandas.read_csv(\"./data/shared/covid/covid_combined.csv\", index_col=\"date\",\n                    parse_dates=True)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      state\n      fips\n      cases\n      deaths\n      dtc100\n      population\n      deaths100k\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-21\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-22\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-23\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-24\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-25\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n  \n\n\n\n\n\nlast_df = df.loc['2020-08-02']\n\n\nlast_df.shape\n\n(54, 7)\n\n\n\nimport geopandas\n\n\ngdf = geopandas.read_file(\"./data/shared/covid/gz_2010_us_040_00_500k.json\")\n\n\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      NAME\n      LSAD\n      CENSUSAREA\n      geometry\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n    \n    \n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n    \n    \n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n    \n    \n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n    \n    \n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n    \n  \n\n\n\n\n\ngdf.columns.values\n\narray(['GEO_ID', 'STATE', 'NAME', 'LSAD', 'CENSUSAREA', 'geometry'],\n      dtype=object)\n\n\n\ngdf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      state\n      fips\n      cases\n      deaths\n      dtc100\n      population\n      deaths100k\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-21\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-22\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-23\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-24\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-25\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n  \n\n\n\n\n\n\n\ngdf.rename(columns={'NAME': 'state'}, inplace=True)\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n    \n    \n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n    \n    \n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n    \n    \n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n    \n    \n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n    \n  \n\n\n\n\n\ndf['date'] = df.index.values\njoin_gdf = gdf.merge(df, on='state')\njoin_gdf.head()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n      fips\n      cases\n      deaths\n      dtc100\n      population\n      deaths100k\n      date\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      1\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-12\n    \n    \n      1\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      2\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-13\n    \n    \n      2\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      3\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-14\n    \n    \n      3\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      12\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-15\n    \n    \n      4\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      17\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-16\n    \n  \n\n\n\n\n\nlast_gdf = join_gdf[join_gdf.date=='2020-08-02']\nlast_gdf.shape\n\n(52, 13)\n\n\n\ndrop = ['Puerto Rico', 'Alaska', 'Hawaii']\nlast_gdf = last_gdf[~last_gdf['state'].isin(drop)]\n\n\nlast_gdf.shape\n\n(49, 13)\n\n\n\nlast_gdf.plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\nlast_gdf.plot(column='dtc100')\n\n<AxesSubplot:>\n\n\n\n\n\n\nlast_gdf.plot(column='dtc100', legend=True, figsize=(16,9))\n\n<AxesSubplot:>\n\n\n\n\n\n\nlast_gdf.plot(column='dtc100', legend=True, figsize=(16,9),\n               scheme='quantiles', k=10)\n\n<AxesSubplot:>\n\n\n\n\n\n\nlast_gdf.plot(column='dtc100', legend=True, figsize=(16,9),\n               scheme='quantiles', k=10,\n               legend_kwds={'loc': 'lower right'})\n\n<AxesSubplot:>\n\n\n\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='quantiles',k=10, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\n\n\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='EqualInterval',k=5, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\ny = last_gdf.dtc100\nimport mapclassify\n\n\nea5 = mapclassify.EqualInterval(y, k=5)\n\n\nea5\n\nEqualInterval\n\n  Interval     Count\n--------------------\n[0.76, 2.39] |    25\n(2.39, 4.02] |    13\n(4.02, 5.64] |     4\n(5.64, 7.27] |     3\n(7.27, 8.90] |     4\n\n\n\n4.02-2.39\n\n1.6299999999999994\n\n\n\n5.64-4.02\n\n1.62\n\n\n\n\n\n\neq5 = mapclassify.Quantiles(y, k=5)\n\n\neq5\n\nQuantiles\n\n  Interval     Count\n--------------------\n[0.76, 1.49] |    10\n(1.49, 1.84] |    10\n(1.84, 2.93] |     9\n(2.93, 4.23] |    10\n(4.23, 8.90] |    10\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='Quantiles',k=5, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\n\n\n\nmb5 = mapclassify.MaximumBreaks(y, k=5)\nmb5\n\nMaximumBreaks\n\n  Interval     Count\n--------------------\n[0.76, 5.03] |    41\n(5.03, 5.73] |     1\n(5.73, 6.66] |     2\n(6.66, 8.15] |     3\n(8.15, 8.90] |     2\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='MaximumBreaks',k=5, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\n\n\n\nmapclassify.BoxPlot(y)\n\nBoxPlot\n\n   Interval      Count\n----------------------\n( -inf, -1.82] |     0\n(-1.82,  1.62] |    13\n( 1.62,  2.37] |    12\n( 2.37,  3.91] |    12\n( 3.91,  7.35] |     9\n( 7.35,  8.90] |     3\n\n\n\ny.median()\n\n2.3743977976600137\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='BoxPlot', legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\nmapclassify.BoxPlot?\n\n\nInit signature: mapclassify.BoxPlot(y, hinge=1.5)\nDocstring:     \nBoxPlot Map Classification.\nParameters\n----------\ny : numpy.array\n    Attribute to classify\nhinge : float (default 1.5)\n    Multiplier for *IQR*.\nAttributes\n----------\nyb : numpy.array\n    :math:`(n,1)`, bin ids for observations.\nbins : array\n    :math:`(n,1)`, the upper bounds of each class  (monotonic).\nk : int\n    The number of classes.\ncounts : numpy.array\n    :math:`(k,1)`, the number of observations falling in each class.\nlow_outlier_ids : numpy.array\n    Indices of observations that are low outliers.\nhigh_outlier_ids : numpy.array\n    Indices of observations that are high outliers.\nNotes\n-----\nThe bins are set as follows::\n    bins[0] = q[0]-hinge*IQR\n    bins[1] = q[0]\n    bins[2] = q[1]\n    bins[3] = q[2]\n    bins[4] = q[2]+hinge*IQR\n    bins[5] = inf  (see Notes)\nwhere :math:`q` is an array of the first three quartiles of :math:`y` and\n:math:`IQR=q[2]-q[0]`.\nIf :math:`q[2]+hinge*IQR > max(y)` there will only be 5 classes and no high\noutliers, otherwise, there will be 6 classes and at least one high\noutlier.\nExamples\n--------\n>>> import mapclassify\n>>> import numpy\n>>> cal = mapclassify.load_example()\n>>> bp = mapclassify.BoxPlot(cal)\n>>> bp.bins\narray([-5.287625e+01,  2.567500e+00,  9.365000e+00,  3.953000e+01,\n        9.497375e+01,  4.111450e+03])\n>>> list(bp.counts)\n[0, 15, 14, 14, 6, 9]\n>>> list(bp.high_outlier_ids)\n[0, 6, 18, 29, 33, 36, 37, 40, 42]\n>>> cal[bp.high_outlier_ids].values\narray([ 329.92,  181.27,  370.5 ,  722.85,  192.05,  110.74, 4111.45,\n        317.11,  264.93])\n>>> bx = mapclassify.BoxPlot(numpy.arange(100))\n>>> bx.bins\narray([-49.5 ,  24.75,  49.5 ,  74.25, 148.5 ])\nInit docstring:\nParameters\n----------\ny : numpy.array\n    :math:`(n,1)`, attribute to classify\nhinge : float (default 1.5)\n    Multiple of inter-quartile range.\nFile:           /opt/tljh/user/lib/python3.9/site-packages/mapclassify/classifiers.py\nType:           type\nSubclasses:     \n\n\n\n\n\n\n\nmapclassify.StdMean(y)\n\nStdMean\n\n   Interval      Count\n----------------------\n( -inf, -1.15] |     0\n(-1.15,  0.97] |     3\n( 0.97,  5.22] |    38\n( 5.22,  7.34] |     5\n( 7.34,  8.90] |     3\n\n\n\ny.mean()- y.std()\n\n0.9741967014843262\n\n\n\ny.mean()+y.std()\n\n5.217609367021282\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='StdMean', legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\n\n\n\nmapclassify.FisherJenks(y, k=5)\n\nFisherJenks\n\n  Interval     Count\n--------------------\n[0.76, 2.12] |    24\n(2.12, 3.34] |     9\n(3.34, 5.29] |     9\n(5.29, 7.29] |     4\n(7.29, 8.90] |     3\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='FisherJenks', k=5, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='FisherJenks', k=5, legend=True,\n             figsize=(16,9), cmap='Blues',edgecolor='k',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\nfj5 = mapclassify.FisherJenks(y,5)\nq5 = mapclassify.Quantiles(y, 5)\nea5 = mapclassify.EqualInterval(y, 5)\nfj5.adcm, q5.adcm, ea5.adcm\n\n(15.68079626959761, 21.603825374669483, 18.970554166382374)\n\n\n\n\n\n\n\nc_gdf = last_gdf.to_crs(last_gdf.estimate_utm_crs())\n\n\ncentroids =c_gdf.centroid\n\n\neast = c_gdf.centroid.x > c_gdf.centroid.x.median()\nnorth = c_gdf.centroid.y > c_gdf.centroid.y.median()\n\n\nc_gdf['east'] = c_gdf.centroid.x < centroids.x.median()\nc_gdf['west'] = c_gdf.centroid.x >= centroids.x.median()\nc_gdf['south'] = c_gdf.centroid.y < centroids.y.median()\nc_gdf['north'] = c_gdf.centroid.y >= centroids.y.median()\nc_gdf['NE'] = c_gdf.north * c_gdf.east\nc_gdf['SE'] = c_gdf.south * c_gdf.east\nc_gdf['NW'] = c_gdf.north * c_gdf.west\nc_gdf['SW'] = c_gdf.south * c_gdf.west\n    \n\n\n\nc_gdf['region'] = (1 * c_gdf.NE) + (2 * c_gdf.NW) + (3* c_gdf.SW) + (4*c_gdf.SE)\n\n\nc_gdf.groupby(by='region').count()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n      fips\n      cases\n      deaths\n      dtc100\n      ...\n      date\n      east\n      west\n      south\n      north\n      NE\n      SE\n      NW\n      SW\n      usgreedy\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      ...\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n    \n    \n      2\n      11\n      11\n      11\n      11\n      11\n      11\n      11\n      11\n      11\n      11\n      ...\n      11\n      11\n      11\n      11\n      11\n      11\n      11\n      11\n      11\n      11\n    \n    \n      3\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      ...\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n      14\n    \n    \n      4\n      10\n      10\n      10\n      10\n      10\n      10\n      10\n      10\n      10\n      10\n      ...\n      10\n      10\n      10\n      10\n      10\n      10\n      10\n      10\n      10\n      10\n    \n  \n\n4 rows × 22 columns\n\n\n\n\nc_gdf.plot(column='region', categorical=True)\n\n<AxesSubplot:>\n\n\n\n\n\n\nc_gdf.plot(column='region')\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nusgreedy = mapclassify.greedy(c_gdf)\n\n\nc_gdf['usgreedy'] = usgreedy\nc_gdf.plot(column='usgreedy', categorical=True)\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\n\n\njoin_gdf.head()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n      fips\n      cases\n      deaths\n      dtc100\n      population\n      deaths100k\n      date\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      1\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-12\n    \n    \n      1\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      2\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-13\n    \n    \n      2\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      3\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-14\n    \n    \n      3\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      12\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-15\n    \n    \n      4\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      17\n      0\n      0.0\n      1344212\n      0.0\n      2020-03-16\n    \n  \n\n\n\n\n\ndates = [f\"2020-{month:02}-01\"  for month  in range(5,9)]\n\ndates\n\n['2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01']\n\n\n\nlast4 = join_gdf[join_gdf.date.isin(dates)]\nlast4 = last4[~last4['state'].isin(drop)]\nlast4.shape\n\n(196, 13)\n\n\n\nfor day in dates:\n    last4[last4.date==day].plot(column='dtc100', scheme='Quantiles', k=5, \n                                legend=True, legend_kwds={'loc': 'lower right'})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport mapclassify\n\nq5 = mapclassify.Quantiles(last4.dtc100, k=5)\n\nq5\n\nQuantiles\n\n  Interval     Count\n--------------------\n[0.76, 1.87] |    40\n(1.87, 3.12] |    39\n(3.12, 4.20] |    39\n(4.20, 5.44] |    39\n(5.44, 9.45] |    39\n\n\n\nq5.bins\n\narray([1.86813187, 3.12420625, 4.20334682, 5.44027239, 9.45494994])\n\n\n\nlast4['pooled5'] = q5.yb\n\n\nfor day in dates:\n    last4[last4.date==day].plot(column='pooled5', scheme='equalinterval',\n                                legend=True, legend_kwds={'loc': 'lower right'})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlast_gdf.columns\n\nIndex(['GEO_ID', 'STATE', 'state', 'LSAD', 'CENSUSAREA', 'geometry', 'fips',\n       'cases', 'deaths', 'dtc100', 'population', 'deaths100k', 'date'],\n      dtype='object')\n\n\n\nlast_gdf = last_gdf[['GEO_ID', 'STATE', 'state', 'LSAD', 'CENSUSAREA', 'geometry', 'fips',\n       'cases', 'deaths', 'dtc100', 'population', 'deaths100k']] # omit the date column\n\n\nlast_gdf.explore()\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nlast_gdf.explore(column='dtc100')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nlast_gdf.explore(column='dtc100', scheme='quantiles', k=5)\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nlast_gdf.head()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n      fips\n      cases\n      deaths\n      dtc100\n      population\n      deaths100k\n    \n  \n  \n    \n      143\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      23\n      3958\n      123\n      3.107630\n      1344212\n      9.150342\n    \n    \n      327\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n      25\n      118458\n      8638\n      7.292036\n      6949503\n      124.296658\n    \n    \n      473\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n      26\n      91857\n      6460\n      7.032670\n      9986857\n      64.685016\n    \n    \n      616\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n      30\n      4193\n      61\n      1.454806\n      1068778\n      5.707453\n    \n    \n      767\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n      32\n      50270\n      833\n      1.657052\n      3080156\n      27.044085\n    \n  \n\n\n\n\n\nlast_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'])\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nimport folium\n\n\ncentroids = last_gdf.centroid\n\n/tmp/ipykernel_3348826/2951951418.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  centroids = last_gdf.centroid\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'])\ncentroids.explore(m=m)\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon')\ncentroids.explore(m=m, name='centroid')\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon', tiles='Stamen Toner')\ncentroids.explore(m=m, name='centroid')\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon', tiles='Stamen Terrain')\ncentroids.explore(m=m, name='centroid')\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon', tiles='CartoDB positron')\ncentroids.explore(m=m, name='centroid')\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nm.crs\n\n'EPSG3857'\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon', tiles='CartoDB positron')\ncentroids.explore(m=m, name='centroid')\nfolium.Marker([32.7774, -117.0714],\n              popup='SDSU',\n              icon=folium.Icon(color='red', icon='ok-sign'),).add_to(m)\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "lectures/0821/0821.html",
    "href": "lectures/0821/0821.html",
    "title": "Course Introduction",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "lectures/week-06/2023-09-25.html",
    "href": "lectures/week-06/2023-09-25.html",
    "title": "Geog385F23",
    "section": "",
    "text": "import pandas"
  },
  {
    "objectID": "lectures/week-06/2023-09-25.html#table-join-with-remote-data",
    "href": "lectures/week-06/2023-09-25.html#table-join-with-remote-data",
    "title": "Geog385F23",
    "section": "Table Join with remote data",
    "text": "Table Join with remote data\n\ngdf.rename(columns={'NAME': 'state'}, inplace=True)\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n    \n    \n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n    \n    \n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n    \n    \n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n    \n    \n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n    \n  \n\n\n\n\n\ntable = pandas.read_html('https://www.pewresearch.org/religion/religious-landscape-study/compare/party-affiliation/by/state/')\n\n\ntable\n\n[                   State Republican/lean Rep. No lean Democrat/lean Dem.  \\\n 0                Alabama                  52%     13%                35%   \n 1                 Alaska                  39%     29%                32%   \n 2                Arizona                  40%     21%                39%   \n 3               Arkansas                  46%     16%                38%   \n 4             California                  30%     21%                49%   \n 5               Colorado                  41%     17%                42%   \n 6            Connecticut                  32%     18%                50%   \n 7               Delaware                  29%     17%                55%   \n 8   District of Columbia                  11%     15%                73%   \n 9                Florida                  37%     19%                44%   \n 10               Georgia                  41%     18%                41%   \n 11                Hawaii                  28%     20%                51%   \n 12                 Idaho                  49%     19%                32%   \n 13              Illinois                  33%     19%                48%   \n 14               Indiana                  42%     20%                37%   \n 15                  Iowa                  41%     19%                40%   \n 16                Kansas                  46%     23%                31%   \n 17              Kentucky                  44%     13%                43%   \n 18             Louisiana                  41%     16%                43%   \n 19                 Maine                  36%     17%                47%   \n 20              Maryland                  31%     14%                55%   \n 21         Massachusetts                  27%     17%                56%   \n 22              Michigan                  34%     19%                47%   \n 23             Minnesota                  39%     15%                46%   \n 24           Mississippi                  44%     14%                42%   \n 25              Missouri                  41%     18%                42%   \n 26               Montana                  49%     21%                30%   \n 27              Nebraska                  47%     17%                36%   \n 28                Nevada                  37%     18%                46%   \n 29         New Hampshire                  35%     20%                44%   \n 30            New Jersey                  30%     19%                51%   \n 31            New Mexico                  37%     15%                48%   \n 32              New York                  28%     19%                53%   \n 33        North Carolina                  41%     17%                43%   \n 34          North Dakota                  50%     18%                33%   \n 35                  Ohio                  42%     18%                40%   \n 36              Oklahoma                  45%     15%                40%   \n 37                Oregon                  32%     21%                47%   \n 38          Pennsylvania                  39%     15%                46%   \n 39          Rhode Island                  30%     22%                48%   \n 40        South Carolina                  43%     18%                39%   \n 41          South Dakota                  53%     10%                37%   \n 42             Tennessee                  48%     15%                36%   \n 43                 Texas                  39%     21%                40%   \n 44                  Utah                  54%     16%                30%   \n 45               Vermont                  29%     14%                57%   \n 46              Virginia                  43%     18%                39%   \n 47            Washington                  33%     23%                44%   \n 48         West Virginia                  43%     16%                41%   \n 49             Wisconsin                  42%     16%                42%   \n 50               Wyoming                  57%     18%                25%   \n \n     Sample\\tsize  \n 0            511  \n 1            310  \n 2            653  \n 3            311  \n 4           3697  \n 5            504  \n 6            377  \n 7            301  \n 8            303  \n 9           2020  \n 10           968  \n 11           312  \n 12           320  \n 13          1326  \n 14           654  \n 15           330  \n 16           307  \n 17           439  \n 18           465  \n 19           303  \n 20           644  \n 21           704  \n 22           982  \n 23           563  \n 24           309  \n 25           642  \n 26           312  \n 27           312  \n 28           314  \n 29           303  \n 30           886  \n 31           312  \n 32          1966  \n 33          1022  \n 34           338  \n 35          1132  \n 36           391  \n 37           419  \n 38          1366  \n 39           305  \n 40           495  \n 41           305  \n 42           661  \n 43          2535  \n 44           315  \n 45           306  \n 46           882  \n 47           714  \n 48           309  \n 49           600  \n 50           316  ]\n\n\n\nlen(table)\n\n1\n\n\n\ndf = table[0]\ndf.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Republican/lean Rep.\n      No lean\n      Democrat/lean Dem.\n      Sample\\tsize\n    \n  \n  \n    \n      0\n      Alabama\n      52%\n      13%\n      35%\n      511\n    \n    \n      1\n      Alaska\n      39%\n      29%\n      32%\n      310\n    \n    \n      2\n      Arizona\n      40%\n      21%\n      39%\n      653\n    \n    \n      3\n      Arkansas\n      46%\n      16%\n      38%\n      311\n    \n    \n      4\n      California\n      30%\n      21%\n      49%\n      3697\n    \n  \n\n\n\n\n\ndf.columns\n\nIndex(['State', 'Republican/lean Rep.', 'No lean', 'Democrat/lean Dem.',\n       'Sample\\tsize'],\n      dtype='object')\n\n\n\ndf = df.rename(columns={df.columns[1]: 'Rep'})\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Rep\n      No lean\n      Democrat/lean Dem.\n      Sample\\tsize\n    \n  \n  \n    \n      0\n      Alabama\n      52%\n      13%\n      35%\n      511\n    \n    \n      1\n      Alaska\n      39%\n      29%\n      32%\n      310\n    \n    \n      2\n      Arizona\n      40%\n      21%\n      39%\n      653\n    \n    \n      3\n      Arkansas\n      46%\n      16%\n      38%\n      311\n    \n    \n      4\n      California\n      30%\n      21%\n      49%\n      3697\n    \n  \n\n\n\n\n\ndf['rep_int']= df.Rep.str.replace(\"%\",\"\").astype(int)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Rep\n      No lean\n      Democrat/lean Dem.\n      Sample\\tsize\n      rep_int\n    \n  \n  \n    \n      0\n      Alabama\n      52%\n      13%\n      35%\n      511\n      52\n    \n    \n      1\n      Alaska\n      39%\n      29%\n      32%\n      310\n      39\n    \n    \n      2\n      Arizona\n      40%\n      21%\n      39%\n      653\n      40\n    \n    \n      3\n      Arkansas\n      46%\n      16%\n      38%\n      311\n      46\n    \n    \n      4\n      California\n      30%\n      21%\n      49%\n      3697\n      30\n    \n  \n\n\n\n\n\ndf = df.rename(columns={'State':'state'})\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      state\n      Rep\n      No lean\n      Democrat/lean Dem.\n      Sample\\tsize\n      rep_int\n    \n  \n  \n    \n      0\n      Alabama\n      52%\n      13%\n      35%\n      511\n      52\n    \n    \n      1\n      Alaska\n      39%\n      29%\n      32%\n      310\n      39\n    \n    \n      2\n      Arizona\n      40%\n      21%\n      39%\n      653\n      40\n    \n    \n      3\n      Arkansas\n      46%\n      16%\n      38%\n      311\n      46\n    \n    \n      4\n      California\n      30%\n      21%\n      49%\n      3697\n      30\n    \n    \n      5\n      Colorado\n      41%\n      17%\n      42%\n      504\n      41\n    \n    \n      6\n      Connecticut\n      32%\n      18%\n      50%\n      377\n      32\n    \n    \n      7\n      Delaware\n      29%\n      17%\n      55%\n      301\n      29\n    \n    \n      8\n      District of Columbia\n      11%\n      15%\n      73%\n      303\n      11\n    \n    \n      9\n      Florida\n      37%\n      19%\n      44%\n      2020\n      37\n    \n    \n      10\n      Georgia\n      41%\n      18%\n      41%\n      968\n      41\n    \n    \n      11\n      Hawaii\n      28%\n      20%\n      51%\n      312\n      28\n    \n    \n      12\n      Idaho\n      49%\n      19%\n      32%\n      320\n      49\n    \n    \n      13\n      Illinois\n      33%\n      19%\n      48%\n      1326\n      33\n    \n    \n      14\n      Indiana\n      42%\n      20%\n      37%\n      654\n      42\n    \n    \n      15\n      Iowa\n      41%\n      19%\n      40%\n      330\n      41\n    \n    \n      16\n      Kansas\n      46%\n      23%\n      31%\n      307\n      46\n    \n    \n      17\n      Kentucky\n      44%\n      13%\n      43%\n      439\n      44\n    \n    \n      18\n      Louisiana\n      41%\n      16%\n      43%\n      465\n      41\n    \n    \n      19\n      Maine\n      36%\n      17%\n      47%\n      303\n      36\n    \n    \n      20\n      Maryland\n      31%\n      14%\n      55%\n      644\n      31\n    \n    \n      21\n      Massachusetts\n      27%\n      17%\n      56%\n      704\n      27\n    \n    \n      22\n      Michigan\n      34%\n      19%\n      47%\n      982\n      34\n    \n    \n      23\n      Minnesota\n      39%\n      15%\n      46%\n      563\n      39\n    \n    \n      24\n      Mississippi\n      44%\n      14%\n      42%\n      309\n      44\n    \n    \n      25\n      Missouri\n      41%\n      18%\n      42%\n      642\n      41\n    \n    \n      26\n      Montana\n      49%\n      21%\n      30%\n      312\n      49\n    \n    \n      27\n      Nebraska\n      47%\n      17%\n      36%\n      312\n      47\n    \n    \n      28\n      Nevada\n      37%\n      18%\n      46%\n      314\n      37\n    \n    \n      29\n      New Hampshire\n      35%\n      20%\n      44%\n      303\n      35\n    \n    \n      30\n      New Jersey\n      30%\n      19%\n      51%\n      886\n      30\n    \n    \n      31\n      New Mexico\n      37%\n      15%\n      48%\n      312\n      37\n    \n    \n      32\n      New York\n      28%\n      19%\n      53%\n      1966\n      28\n    \n    \n      33\n      North Carolina\n      41%\n      17%\n      43%\n      1022\n      41\n    \n    \n      34\n      North Dakota\n      50%\n      18%\n      33%\n      338\n      50\n    \n    \n      35\n      Ohio\n      42%\n      18%\n      40%\n      1132\n      42\n    \n    \n      36\n      Oklahoma\n      45%\n      15%\n      40%\n      391\n      45\n    \n    \n      37\n      Oregon\n      32%\n      21%\n      47%\n      419\n      32\n    \n    \n      38\n      Pennsylvania\n      39%\n      15%\n      46%\n      1366\n      39\n    \n    \n      39\n      Rhode Island\n      30%\n      22%\n      48%\n      305\n      30\n    \n    \n      40\n      South Carolina\n      43%\n      18%\n      39%\n      495\n      43\n    \n    \n      41\n      South Dakota\n      53%\n      10%\n      37%\n      305\n      53\n    \n    \n      42\n      Tennessee\n      48%\n      15%\n      36%\n      661\n      48\n    \n    \n      43\n      Texas\n      39%\n      21%\n      40%\n      2535\n      39\n    \n    \n      44\n      Utah\n      54%\n      16%\n      30%\n      315\n      54\n    \n    \n      45\n      Vermont\n      29%\n      14%\n      57%\n      306\n      29\n    \n    \n      46\n      Virginia\n      43%\n      18%\n      39%\n      882\n      43\n    \n    \n      47\n      Washington\n      33%\n      23%\n      44%\n      714\n      33\n    \n    \n      48\n      West Virginia\n      43%\n      16%\n      41%\n      309\n      43\n    \n    \n      49\n      Wisconsin\n      42%\n      16%\n      42%\n      600\n      42\n    \n    \n      50\n      Wyoming\n      57%\n      18%\n      25%\n      316\n      57\n    \n  \n\n\n\n\n\njoin_gdf = gdf.merge(df, on='state')\n\n\njoin_gdf.head()\n\n\n\n\n\n  \n    \n      \n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n      Rep\n      No lean\n      Democrat/lean Dem.\n      Sample\\tsize\n      rep_int\n    \n  \n  \n    \n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n      36%\n      17%\n      47%\n      303\n      36\n    \n    \n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n      27%\n      17%\n      56%\n      704\n      27\n    \n    \n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n      34%\n      19%\n      47%\n      982\n      34\n    \n    \n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n      49%\n      21%\n      30%\n      312\n      49\n    \n    \n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n      37%\n      18%\n      46%\n      314\n      37\n    \n  \n\n\n\n\n\ndrop = ['Alaska', 'Hawaii']\njoin_gdf = join_gdf[~join_gdf['state'].isin(drop)]\njoin_gdf.reset_index(inplace=True)\n\n\njoin_gdf.plot(column='rep_int', legend=True)\n\n<AxesSubplot:>\n\n\n\n\n\n\njoin_gdf = join_gdf.to_crs(join_gdf.estimate_utm_crs())\n\n\njoin_gdf.plot(column='rep_int', scheme='Quantiles', k=5, legend=True)\n\n<AxesSubplot:>\n\n\n\n\n\n\njoin_gdf.plot(column='rep_int', scheme='Quantiles', k=5, legend=True,\n             figsize=(16,9), legend_kwds={'loc': 'lower right'},\n             cmap='Reds')\n\n<AxesSubplot:>\n\n\n\n\n\n\njoin_gdf.plot(column='rep_int', scheme='Quantiles', k=5, legend=True,\n             figsize=(16,9), legend_kwds={'loc': 'lower right'},\n             edgecolor='lightgrey',\n             cmap='Reds')\n\n<AxesSubplot:>\n\n\n\n\n\n\nimport libpysal\n\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:39: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_dist(x, y):\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:165: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def get_faces(triangle):\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:199: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:261: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_mask_faces(mask, faces):\n\n\n\nw = libpysal.weights.Queen.from_dataframe(join_gdf)\n\n\nw.n\n\n49\n\n\n\nw.pct_nonzero\n\n9.079550187421907\n\n\n\nfrom splot.libpysal import plot_spatial_weights\n\n\n\nw.n\n\n49\n\n\n\nw.cardinalities\n\n{0: 1,\n 1: 5,\n 2: 3,\n 3: 4,\n 4: 5,\n 5: 3,\n 6: 5,\n 7: 4,\n 8: 5,\n 9: 6,\n 10: 2,\n 11: 8,\n 12: 4,\n 13: 6,\n 14: 2,\n 15: 4,\n 16: 5,\n 17: 4,\n 18: 5,\n 19: 6,\n 20: 3,\n 21: 7,\n 22: 3,\n 23: 3,\n 24: 2,\n 25: 2,\n 26: 5,\n 27: 6,\n 28: 5,\n 29: 4,\n 30: 6,\n 31: 4,\n 32: 7,\n 33: 3,\n 34: 4,\n 35: 4,\n 36: 8,\n 37: 6,\n 38: 3,\n 39: 5,\n 40: 3,\n 41: 6,\n 42: 4,\n 43: 2,\n 44: 6,\n 45: 3,\n 46: 6,\n 47: 5,\n 48: 6}\n\n\n\nw.histogram\n\n[(1, 1), (2, 5), (3, 9), (4, 10), (5, 10), (6, 10), (7, 2), (8, 2)]\n\n\n\ns = pandas.Series(w.cardinalities)\ns.plot.hist(bins=s.unique().shape[0]);\n\n\n\n\n\n_ = plot_spatial_weights(w,join_gdf, figsize=(16,9))\n\n\n\n\n\njoin_gdf['queen_neighbors'] = w.cardinalities\n\n\njoin_gdf.plot(column='queen_neighbors', legend=True, figsize=(16,9))\n\n<AxesSubplot:>\n\n\n\n\n\n\njoin_gdf.head()\n\n\n\n\n\n  \n    \n      \n      index\n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n      Rep\n      No lean\n      Democrat/lean Dem.\n      Sample\\tsize\n      rep_int\n      queen_neighbors\n    \n  \n  \n    \n      0\n      0\n      0400000US23\n      23\n      Maine\n      \n      30842.923\n      MULTIPOLYGON (((2516172.424 5253443.650, 25164...\n      36%\n      17%\n      47%\n      303\n      36\n      1\n    \n    \n      1\n      1\n      0400000US25\n      25\n      Massachusetts\n      \n      7800.058\n      MULTIPOLYGON (((2351724.597 4850457.653, 23526...\n      27%\n      17%\n      56%\n      704\n      27\n      5\n    \n    \n      2\n      2\n      0400000US26\n      26\n      Michigan\n      \n      56538.901\n      MULTIPOLYGON (((821167.988 5338182.388, 821794...\n      34%\n      19%\n      47%\n      982\n      34\n      3\n    \n    \n      3\n      3\n      0400000US30\n      30\n      Montana\n      \n      145545.801\n      POLYGON ((-371533.418 5042503.702, -386687.255...\n      49%\n      21%\n      30%\n      312\n      49\n      4\n    \n    \n      4\n      4\n      0400000US32\n      32\n      Nevada\n      \n      109781.180\n      POLYGON ((-1384104.733 4308747.816, -1385163.9...\n      37%\n      18%\n      46%\n      314\n      37\n      5\n    \n  \n\n\n\n\n\njoin_gdf[join_gdf.state=='California']\n\n\n\n\n\n  \n    \n      \n      index\n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n      Rep\n      No lean\n      Democrat/lean Dem.\n      Sample\\tsize\n      rep_int\n      queen_neighbors\n    \n  \n  \n    \n      20\n      21\n      0400000US06\n      06\n      California\n      \n      155779.22\n      MULTIPOLYGON (((-2114916.187 4624534.270, -211...\n      30%\n      21%\n      49%\n      3697\n      30\n      3\n    \n  \n\n\n\n\n\njoin_gdf[join_gdf.index.isin(w.neighbors[20])].state\n\n4      Nevada\n18    Arizona\n42     Oregon\nName: state, dtype: object\n\n\n\njoin_gdf[join_gdf.index.isin(w.neighbors[18])].state\n\n4         Nevada\n13          Utah\n20    California\n21      Colorado\n39    New Mexico\nName: state, dtype: object"
  },
  {
    "objectID": "lectures/week-06/2023-09-25.html#rook",
    "href": "lectures/week-06/2023-09-25.html#rook",
    "title": "Geog385F23",
    "section": "Rook",
    "text": "Rook\n\nwrook = libpysal.weights.Rook.from_dataframe(join_gdf)\n\n\nwrook.n\n\n49\n\n\n\nwrook.pct_nonzero\n\n8.912952936276552\n\n\n\n_ = plot_spatial_weights(wrook,join_gdf, figsize=(16,9))\n\n\n\n\n\njoin_gdf[join_gdf.index.isin(wrook.neighbors[18])].state\n\n4         Nevada\n13          Utah\n20    California\n39    New Mexico\nName: state, dtype: object\n\n\n\nwrook.histogram\n\n[(1, 1), (2, 5), (3, 9), (4, 12), (5, 9), (6, 10), (7, 1), (8, 2)]\n\n\n\ns = pandas.Series(wrook.cardinalities)\ns.plot.hist(bins=s.unique().shape[0]);"
  },
  {
    "objectID": "lectures/week-06/2023-09-25.html#distance-based-weights",
    "href": "lectures/week-06/2023-09-25.html#distance-based-weights",
    "title": "Geog385F23",
    "section": "Distance Based Weights",
    "text": "Distance Based Weights\n\nK-Nearest Neighbor\n\nwk4 = libpysal.weights.distance.KNN.from_dataframe(join_gdf, k=4)\n\n\nwk4.histogram\n\n[(4, 49)]\n\n\n\n_ = plot_spatial_weights(wk4,join_gdf, figsize=(16,9))\n\n\n\n\n\nwk4.pct_nonzero\n\n8.16326530612245\n\n\n\n\nDistance Bands\n\nw_bdb = libpysal.weights.distance.DistanceBand.from_dataframe(join_gdf, 1000000, binary=True) # 1 million meters = 621.37 miles\n\n\nw_bdb.histogram\n\n[(4, 1),\n (5, 2),\n (6, 1),\n (7, 3),\n (8, 4),\n (9, 1),\n (10, 4),\n (11, 3),\n (12, 1),\n (13, 4),\n (14, 3),\n (15, 5),\n (16, 2),\n (17, 2),\n (18, 3),\n (19, 1),\n (20, 1),\n (21, 3),\n (22, 4),\n (23, 0),\n (24, 1)]\n\n\n\n_ = plot_spatial_weights(w_bdb,join_gdf, figsize=(16,9))"
  },
  {
    "objectID": "lectures/week-06/2023-09-25.html#uses-of-w",
    "href": "lectures/week-06/2023-09-25.html#uses-of-w",
    "title": "Geog385F23",
    "section": "Uses of W",
    "text": "Uses of W\n\nFind largest difference between neighbors\n\nw_adjlist = w.to_adjlist()\n\n\nw_adjlist.head()\n\n\n\n\n\n  \n    \n      \n      focal\n      neighbor\n      weight\n    \n  \n  \n    \n      0\n      0\n      38\n      1.0\n    \n    \n      1\n      1\n      6\n      1.0\n    \n    \n      2\n      1\n      10\n      1.0\n    \n    \n      3\n      1\n      22\n      1.0\n    \n    \n      4\n      1\n      38\n      1.0\n    \n  \n\n\n\n\n\ny = join_gdf.rep_int.values\n\n\ny\n\narray([36, 27, 34, 49, 37, 30, 28, 41, 42, 39, 30, 48, 39, 54, 33, 42, 31,\n       52, 40, 46, 30, 41, 32, 29, 11, 37, 41, 49, 33, 42, 41, 46, 44, 41,\n       39, 44, 41, 47, 35, 37, 50, 45, 32, 43, 53, 29, 43, 43, 57])\n\n\n\nimport numpy as np\nw_adjlist['pairdiff'] = np.abs(y[w_adjlist.focal] - y[w_adjlist.neighbor])\n\n\nw_adjlist\n\n\n\n\n\n  \n    \n      \n      focal\n      neighbor\n      weight\n      pairdiff\n    \n  \n  \n    \n      0\n      0\n      38\n      1.0\n      1\n    \n    \n      1\n      1\n      6\n      1.0\n      1\n    \n    \n      2\n      1\n      10\n      1.0\n      3\n    \n    \n      3\n      1\n      22\n      1.0\n      5\n    \n    \n      4\n      1\n      38\n      1.0\n      8\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      213\n      48\n      13\n      1.0\n      3\n    \n    \n      214\n      48\n      21\n      1.0\n      16\n    \n    \n      215\n      48\n      27\n      1.0\n      8\n    \n    \n      216\n      48\n      37\n      1.0\n      10\n    \n    \n      217\n      48\n      44\n      1.0\n      4\n    \n  \n\n218 rows × 4 columns\n\n\n\n\nw_adjlist.pairdiff.max()\n\n32\n\n\n\nw_adjlist[['pairdiff']].idxmax()\n\npairdiff    104\ndtype: int64\n\n\n\nw_adjlist.iloc[w_adjlist[['pairdiff']].idxmax()]\n\n\n\n\n\n  \n    \n      \n      focal\n      neighbor\n      weight\n      pairdiff\n    \n  \n  \n    \n      104\n      24\n      46\n      1.0\n      32\n    \n  \n\n\n\n\n\njoin_gdf.iloc[[24, 46]]\n\n\n\n\n\n  \n    \n      \n      index\n      GEO_ID\n      STATE\n      state\n      LSAD\n      CENSUSAREA\n      geometry\n      Rep\n      No lean\n      Democrat/lean Dem.\n      Sample\\tsize\n      rep_int\n      queen_neighbors\n    \n  \n  \n    \n      24\n      25\n      0400000US11\n      11\n      District of Columbia\n      \n      61.048\n      POLYGON ((1889905.340 4416707.434, 1889692.665...\n      11%\n      15%\n      73%\n      303\n      11\n      2\n    \n    \n      46\n      48\n      0400000US51\n      51\n      Virginia\n      \n      39490.086\n      MULTIPOLYGON (((1994538.922 4338939.700, 19949...\n      43%\n      18%\n      39%\n      882\n      43\n      6\n    \n  \n\n\n\n\n\n\nSpatial Lag\n\nfrom libpysal.weights.spatial_lag import lag_spatial\n\n\nw.transform = 'r'\n\n\nlag_spatial?\n\n\nSignature: lag_spatial(w, y)\nDocstring:\nSpatial lag operator.\nIf w is row standardized, returns the average of each observation's neighbors;\nif not, returns the weighted sum of each observation's neighbors.\nParameters\n----------\nw                   : W\n                      libpysal spatial weightsobject\ny                   : array\n                      numpy array with dimensionality conforming to w (see examples)\nReturns\n-------\nwy                  : array\n                      array of numeric values for the spatial lag\nExamples\n--------\nSetup a 9x9 binary spatial weights matrix and vector of data; compute the\nspatial lag of the vector.\n>>> import libpysal\n>>> import numpy as np\n>>> w = libpysal.weights.lat2W(3, 3)\n>>> y = np.arange(9)\n>>> yl = libpysal.weights.lag_spatial(w, y)\n>>> yl\narray([ 4.,  6.,  6., 10., 16., 14., 10., 18., 12.])\nRow standardize the weights matrix and recompute the spatial lag\n>>> w.transform = 'r'\n>>> yl = libpysal.weights.lag_spatial(w, y)\n>>> yl\narray([2.        , 2.        , 3.        , 3.33333333, 4.        ,\n       4.66666667, 5.        , 6.        , 6.        ])\nExplicitly define data vector as 9x1 and recompute the spatial lag\n>>> y.shape = (9, 1)\n>>> yl = libpysal.weights.lag_spatial(w, y)\n>>> yl\narray([[2.        ],\n       [2.        ],\n       [3.        ],\n       [3.33333333],\n       [4.        ],\n       [4.66666667],\n       [5.        ],\n       [6.        ],\n       [6.        ]])\nTake the spatial lag of a 9x2 data matrix\n>>> yr = np.arange(8, -1, -1)\n>>> yr.shape = (9, 1)\n>>> x = np.hstack((y, yr))\n>>> yl = libpysal.weights.lag_spatial(w, x)\n>>> yl\narray([[2.        , 6.        ],\n       [2.        , 6.        ],\n       [3.        , 5.        ],\n       [3.33333333, 4.66666667],\n       [4.        , 4.        ],\n       [4.66666667, 3.33333333],\n       [5.        , 3.        ],\n       [6.        , 2.        ],\n       [6.        , 2.        ]])\nFile:      /opt/tljh/user/lib/python3.9/site-packages/libpysal/weights/spatial_lag.py\nType:      function\n\n\n\n\nlag_rep_int = lag_spatial(w, y)\n\n\nlag_rep_int\n\narray([35.        , 30.8       , 42.        , 52.25      , 41.        ,\n       32.        , 31.4       , 43.75      , 40.4       , 33.83333333,\n       29.5       , 44.        , 42.25      , 43.5       , 40.5       ,\n       36.75      , 33.        , 42.5       , 39.8       , 43.        ,\n       36.33333333, 46.57142857, 28.33333333, 33.33333333, 37.        ,\n       46.5       , 44.2       , 43.66666667, 42.        , 38.25      ,\n       42.5       , 43.5       , 41.71428571, 43.        , 46.5       ,\n       46.75      , 43.75      , 46.5       , 30.66666667, 43.8       ,\n       47.        , 41.66666667, 37.25      , 41.        , 47.16666667,\n       30.        , 36.33333333, 39.8       , 48.83333333])\n\n\n\njoin_gdf['lag_rep_int'] = lag_rep_int\n\n\nimport matplotlib.pyplot as plt\n\n\nf, axs = plt.subplots(1, 2, figsize=(16, 9))\nax1, ax2 = axs\n\ngdf = join_gdf\n\ngdf.plot(column='rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican\")\n\n\ngdf.plot(column='lag_rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\nax2.set_title(\"Spatial Lag Leaning Republican\")\n\nText(0.5, 1.0, 'Spatial Lag Leaning Republican')\n\n\n\n\n\n\ngdf.plot.scatter(x='rep_int', y='lag_rep_int')\n\n<AxesSubplot:xlabel='rep_int', ylabel='lag_rep_int'>\n\n\n\n\n\n\nimport seaborn as sns\n_ = sns.regplot(x='rep_int', y='lag_rep_int', data=gdf)\n\n\n\n\n\nimport seaborn as sns\n_ = sns.regplot(x='rep_int', y='lag_rep_int', data=gdf)\nplt.axhline(y=gdf.lag_rep_int.mean(), color='g', linestyle='--')\nplt.axvline(x=gdf.rep_int.mean(), color='g', linestyle='--')\n\n<matplotlib.lines.Line2D at 0x7fc5d2669af0>\n\n\n\n\n\n\nf, axs = plt.subplots(1, 2, figsize=(16, 9))\nax1, ax2 = axs\n\ngdf = join_gdf\n\ngdf.plot(column='rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican\")\n\n\ngdf.plot(column='lag_rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\n_= ax2.set_title(\"Spatial Lag Leaning Republican\")\n\n\n\n\n\ngdf.explore(column='rep_int', cmap='Reds')\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\ngdf.to_parquet('repub_lean.parquet')"
  },
  {
    "objectID": "lectures/week-02/2023-08-30-functions-scripts.html",
    "href": "lectures/week-02/2023-08-30-functions-scripts.html",
    "title": "Geog385F23",
    "section": "",
    "text": "def celsius_to_fahr(temp):\n    return 9 / 5 * temp + 32 \n\n\ncelsius_to_fahr(40)\n\n104.0\n\n\n\ncelsius_to_fahr(0)\n\n32.0\n\n\n\ndef how_long(a_string):\n    print(len(a_string))\n\n\nhow_long('abcd')\n\n4\n\n\n\ndef how_long(a_string):\n    len(a_string)\n\n\nhow_long('abcde')\n\n\ndef kelvins_to_celsius(temp_kelvins):\n    return temp_kelvins - 273.15\n\n\nkelvins_to_celsius(temp_kelvins=0)\n\n-273.15\n\n\n\nprint(\"Absolute zero in Celsius is:\", kelvins_to_celsius(0))\n\nAbsolute zero in Celsius is: -273.15\n\n\n\ndef kelvins_to_fahr(temp_kelvins):\n    temp_celsius = kelvins_to_celsius(temp_kelvins)\n    temp_fahr = celsius_to_fahr(temp_celsius)\n    return temp_fahr\n\n\nkelvins_to_fahr(0)\n\n-459.66999999999996\n\n\n\ndef kelvins_to_celsius(temp_kelvins):\n    \"\"\"Convers temperature in Kelvins to degrees celsius.\"\"\"\n    return temp_kelvins - 273.15\n\n\nkelvins_to_celsius?\n\n\nSignature: kelvins_to_celsius(temp_kelvins)\nDocstring: Convers temperature in Kelvins to degrees celsius.\nFile:      /tmp/ipykernel_1023841/522041369.py\nType:      function\n\n\n\n\nkelvins_to_celsius??\n\n\nSignature: kelvins_to_celsius(temp_kelvins)\nSource:   \ndef kelvins_to_celsius(temp_kelvins):\n    \"\"\"Convers temperature in Kelvins to degrees celsius.\"\"\"\n    return temp_kelvins - 273.15\nFile:      /tmp/ipykernel_1023841/522041369.py\nType:      function\n\n\n\n\ndef kelvins_to_celsius(temp_kelvins):\n    \"\"\"Convers temperature in Kelvins to degrees celsius.\n    \n    Parameters\n    ----------\n    temp_kelvins: <numerical>\n        Temperature in Kelvins\n        \n    Returns\n    -------\n    <float>\n       Converted temperature\n    \n    \"\"\"\n    return temp_kelvins - 273.15\n\n\nkelvins_to_celsius?\n\n\nSignature: kelvins_to_celsius(temp_kelvins)\nDocstring:\nConvers temperature in Kelvins to degrees celsius.\nParameters\n----------\ntemp_kelvins: <numerical>\n    Temperature in Kelvins\n    \nReturns\n-------\n<float>\n   Converted temperature\nFile:      /tmp/ipykernel_1023841/1189373821.py\nType:      function"
  },
  {
    "objectID": "lectures/week-02/2023-08-30-functions-scripts.html#using-modules-and-scripts",
    "href": "lectures/week-02/2023-08-30-functions-scripts.html#using-modules-and-scripts",
    "title": "Geog385F23",
    "section": "Using modules and scripts",
    "text": "Using modules and scripts\n\n%ls\n\n2023-08-23.ipynb                      2023-08-30-functions-scripts.ipynb\n2023-08-28-python-introduction.ipynb  temp_converter.py\n\n\n\n%pwd\n\n'/home/jupyter-serge/sessions'\n\n\n\nfrom temp_converter import celsius_to_fahr\n\n\ncelsius_to_fahr(100)\n\nFrom our cool function\n\n\n212.0\n\n\n\nimport math"
  },
  {
    "objectID": "lectures/week-02/2023-08-28-python-introduction.html",
    "href": "lectures/week-02/2023-08-28-python-introduction.html",
    "title": "Geog385F23",
    "section": "",
    "text": "high level language\nlow learning curve\nopen source\nwidespread use\nfun\n\n\n\n\n\n2 + 3\n\n5\n\n\n\n5 * 7\n\n35\n\n\n\n2**4\n\n16\n\n\n\n7 / 4\n\n1.75\n\n\n\nx = 8/3\n\n\nx\n\n2.6666666666666665\n\n\n\nimport math\n\n\nmath.pi\n\n3.141592653589793\n\n\n\nmath.sqrt(4)\n\n2.0\n\n\n\ntemp_celsius = 10.0\n\n\ntemp_celsius\n\n10.0\n\n\n\nprint(\"Temperature in Fahrenheit: \", 9/5 * temp_celsius + 32)\n\nTemperature in Fahrenheit:  50.0\n\n\n\nprint(\"Temperature in Fahrenheit: \", (9/5 * temp_celsius) + 32)\n\nTemperature in Fahrenheit:  50.0\n\n\n\ntemp_celsius\n\n10.0\n\n\n\ntemp_celsius = 40.0\n\n\nprint(\"Temperature in Fahrenheit: \", (9/5 * temp_celsius) + 32)\n\nTemperature in Fahrenheit:  104.0\n\n\n\nprint(\"Temperature in Fahrenheit: \", (9/5 * temp_celsius) + 32)\n\nTemperature in Fahrenheit:  104.0\n\n\n\n\n\n\nint\nfloat\nstr\nbool\n\n\nweatherForecast = \"Hot\"\n\n\ntype(weatherForecast)\n\nstr\n\n\n\nx = 10.1\ntype(x)\n\nfloat\n\n\n\ny = 7\ntype(y)\n\nint\n\n\n\ndir(weatherForecast)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\n\nweatherForecast.upper()\n\n'HOT'\n\n\n\nweatherForecast.startswith('f')\n\nFalse\n\n\n\ntitle = \"This is the title of my book\"\n\n\ntitle\n\n'This is the title of my book'\n\n\n\ntitle.upper()\n\n'THIS IS THE TITLE OF MY BOOK'\n\n\n\ntitle.title()\n\n'This Is The Title Of My Book'\n\n\n\ntitle.split()\n\n['This', 'is', 'the', 'title', 'of', 'my', 'book']\n\n\n\ndir(title)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\n\ntitle\n\n'This is the title of my book'\n\n\n\ntitle.center(40)\n\n'      This is the title of my book      '\n\n\n\nwords = title.split()\n\n\nwords\n\n['This', 'is', 'the', 'title', 'of', 'my', 'book']\n\n\n\ntype(words)\n\nlist\n\n\n\ndir(words)\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\n\nwords\n\n['This', 'is', 'the', 'title', 'of', 'my', 'book']\n\n\n\nlen(words)\n\n7\n\n\n\nlen(title)\n\n28\n\n\n\nwords[0]\n\n'This'\n\n\n\nwords[1]\n\n'is'\n\n\n\nwords[7]\n\nIndexError: list index out of range\n\n\n\nwords[6]\n\n'book'\n\n\n\ntitle\n\n'This is the title of my book'\n\n\n\ntitle[0]\n\n'T'\n\n\n\nlen(title)\n\n28\n\n\n\ntitle[-1]\n\n'k'\n\n\n\ntitle[-2]\n\n'o'\n\n\n\ntitle[0:-4]\n\n'This is the title of my '\n\n\n\ntitle[5:10]\n\n'is th'\n\n\n\ntitle[10]\n\n'e'\n\n\n\nwords\n\n['This', 'is', 'the', 'title', 'of', 'my', 'book']\n\n\n\nwords[1:-2]\n\n['is', 'the', 'title', 'of']\n\n\n\ntype(words)\n\nlist\n\n\n\ntype(words[0])\n\nstr\n\n\n\nfor word in words:\n    print(word)\n\nThis\nis\nthe\ntitle\nof\nmy\nbook\n\n\n\nfor i, word in enumerate(words):\n    print(i, word)\n\n0 This\n1 is\n2 the\n3 title\n4 of\n5 my\n6 book\n\n\n\nfor i, word in enumerate(words):\n    print(i, word, type(word))\n\n0 This <class 'str'>\n1 is <class 'str'>\n2 the <class 'str'>\n3 title <class 'str'>\n4 of <class 'str'>\n5 my <class 'str'>\n6 book <class 'str'>\n\n\n\nlen(words)\n\n7\n\n\n\nif len(words) > 7:\n    print('no its not longer than 7')\n\n\nif len(words) == 7:\n    print('it is of length 7')\n\nit is of length 7\n\n\n\nif len(words) > 7:\n    print('no its not longer than 7')\nelse:\n    print('It must be 7 or less in length')\n\nIt must be 7 or less in length\n\n\n\nif len(words) > 7:\n    print('no its not longer than 7')\nelif len(words)==7:\n    print('its 7')\nelse:\n    print('It is less than 7 in length')\n\nits 7"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html",
    "href": "lectures/week-03/2023-09-06.html",
    "title": "Geog385F23",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#unique-values",
    "href": "lectures/week-03/2023-09-06.html#unique-values",
    "title": "Geog385F23",
    "section": "Unique Values",
    "text": "Unique Values\n\ndata['TEMP'].unique()\n\narray([65.5, 65.8, 68.4, 57.5, 51.4, 52.2, 56.9, 54.2, 49.4, 49.5, 54. ,\n       55.4, 58.3, 59.7, 63.4, 57.8, 60.4, 57.3, 56.3, 59.3, 62.6, 61.7,\n       60.9, 61.1, 65.7, 69.6, 60.7, 65.4])\n\n\n\nprint(\n    \"There were\",\n    data[\"TEMP\"].nunique(),\n    \"days with unique mean temperatures in June 2016.\",\n)\n\nThere were 28 days with unique mean temperatures in June 2016.\n\n\n\ndata.TEMP.mean()\n\n59.730000000000004\n\n\n\ndata.mean()\n\nYEARMODA    20160615.50\nTEMP              59.73\nMAX               67.94\nMIN               51.75\ndtype: float64\n\n\n\ndata.describe()\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n    \n  \n  \n    \n      count\n      3.000000e+01\n      30.000000\n      30.000000\n      30.000000\n    \n    \n      mean\n      2.016062e+07\n      59.730000\n      67.940000\n      51.750000\n    \n    \n      std\n      8.803408e+00\n      5.475472\n      6.651761\n      5.634484\n    \n    \n      min\n      2.016060e+07\n      49.400000\n      54.100000\n      41.700000\n    \n    \n      25%\n      2.016061e+07\n      56.450000\n      63.150000\n      47.300000\n    \n    \n      50%\n      2.016062e+07\n      60.050000\n      69.000000\n      54.050000\n    \n    \n      75%\n      2.016062e+07\n      64.900000\n      72.375000\n      55.750000\n    \n    \n      max\n      2.016063e+07\n      69.600000\n      80.800000\n      60.300000"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#constructing-pandas-objects",
    "href": "lectures/week-03/2023-09-06.html#constructing-pandas-objects",
    "title": "Geog385F23",
    "section": "Constructing Pandas objects",
    "text": "Constructing Pandas objects\n\nnumber_series = pd.Series([4, 5, 6, 7.0])\nprint(number_series)\n\n0    4.0\n1    5.0\n2    6.0\n3    7.0\ndtype: float64\n\n\n\nnumber_series = pd.Series([4, 5, 6, 7.0], index=[\"a\", \"b\", \"c\", \"d\"])\nprint(number_series)\n\na    4.0\nb    5.0\nc    6.0\nd    7.0\ndtype: float64\n\n\n\nstations = [\"Hanko\", \"Heinola\", \"Kaisaniemi\", \"Malmi\"]\nlatitudes = [59.77, 61.2, 60.18, 60.25]\nlongitudes = [22.95, 26.05, 24.94, 25.05]\n\n\nnew_data = pd.DataFrame(data={\"station\": stations, \"lat\": latitudes, \"lon\": longitudes})\nnew_data\n\n\n\n\n\n  \n    \n      \n      station\n      lat\n      lon\n    \n  \n  \n    \n      0\n      Hanko\n      59.77\n      22.95\n    \n    \n      1\n      Heinola\n      61.20\n      26.05\n    \n    \n      2\n      Kaisaniemi\n      60.18\n      24.94\n    \n    \n      3\n      Malmi\n      60.25\n      25.05\n    \n  \n\n\n\n\n\ndictionaries = [\n    {\"station\": \"Hanko\", \"lat\": 59.77, \"lon\": 22.95},\n    {\"station\": \"Heinola\", \"lat\": 61.2, \"lon\": 26.05},\n    {\"station\": \"Kaisaniemi\", \"lat\": 60.18, \"lon\": 24.94},\n    {\"station\": \"Malmi\", \"lat\": 60.25, \"lon\": 25.05},\n]\n\n# Pass the list into the DataFrame constructor\nnew_data_2 = pd.DataFrame(dictionaries)\nnew_data_2\n\n\n\n\n\n  \n    \n      \n      station\n      lat\n      lon\n    \n  \n  \n    \n      0\n      Hanko\n      59.77\n      22.95\n    \n    \n      1\n      Heinola\n      61.20\n      26.05\n    \n    \n      2\n      Kaisaniemi\n      60.18\n      24.94\n    \n    \n      3\n      Malmi\n      60.25\n      25.05\n    \n  \n\n\n\n\n\ndf = pd.DataFrame()\nprint(df)\n\nEmpty DataFrame\nColumns: []\nIndex: []\n\n\n\ndf[\"lon\"] = longitudes\ndf[\"lat\"] = latitudes\ndf\n\n\n\n\n\n  \n    \n      \n      lon\n      lat\n    \n  \n  \n    \n      0\n      22.95\n      59.77\n    \n    \n      1\n      26.05\n      61.20\n    \n    \n      2\n      24.94\n      60.18\n    \n    \n      3\n      25.05\n      60.25"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#common-tabular-operations",
    "href": "lectures/week-03/2023-09-06.html#common-tabular-operations",
    "title": "Geog385F23",
    "section": "Common tabular operations",
    "text": "Common tabular operations\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n    \n  \n  \n    \n      0\n      20160601\n      65.5\n      73.6\n      54.7\n    \n    \n      1\n      20160602\n      65.8\n      80.8\n      55.0\n    \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n    \n    \n      3\n      20160604\n      57.5\n      70.9\n      47.3\n    \n    \n      4\n      20160605\n      51.4\n      58.3\n      43.2\n    \n  \n\n\n\n\n\ndata['DIFF'] = 0.0\ndata.head()\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n    \n  \n  \n    \n      0\n      20160601\n      65.5\n      73.6\n      54.7\n      0.0\n    \n    \n      1\n      20160602\n      65.8\n      80.8\n      55.0\n      0.0\n    \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n      0.0\n    \n    \n      3\n      20160604\n      57.5\n      70.9\n      47.3\n      0.0\n    \n    \n      4\n      20160605\n      51.4\n      58.3\n      43.2\n      0.0\n    \n  \n\n\n\n\n\ndata['DIFF'].dtypes\n\ndtype('float64')\n\n\n\ndata['DIFF'] = data['MAX'] - data['MIN']\ndata.head()\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n    \n  \n  \n    \n      0\n      20160601\n      65.5\n      73.6\n      54.7\n      18.9\n    \n    \n      1\n      20160602\n      65.8\n      80.8\n      55.0\n      25.8\n    \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n      22.3\n    \n    \n      3\n      20160604\n      57.5\n      70.9\n      47.3\n      23.6\n    \n    \n      4\n      20160605\n      51.4\n      58.3\n      43.2\n      15.1\n    \n  \n\n\n\n\n\ndata[\"TEMP_CELSIUS\"] = (data[\"TEMP\"] - 32) / (9 / 5)\ndata.head()\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n    \n  \n  \n    \n      0\n      20160601\n      65.5\n      73.6\n      54.7\n      18.9\n      18.611111\n    \n    \n      1\n      20160602\n      65.8\n      80.8\n      55.0\n      25.8\n      18.777778\n    \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n      22.3\n      20.222222\n    \n    \n      3\n      20160604\n      57.5\n      70.9\n      47.3\n      23.6\n      14.166667\n    \n    \n      4\n      20160605\n      51.4\n      58.3\n      43.2\n      15.1\n      10.777778"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#selecting-and-updating-data",
    "href": "lectures/week-03/2023-09-06.html#selecting-and-updating-data",
    "title": "Geog385F23",
    "section": "Selecting and updating data",
    "text": "Selecting and updating data\n\nselection = data[0:5]\nselection\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n    \n  \n  \n    \n      0\n      20160601\n      65.5\n      73.6\n      54.7\n      18.9\n      18.611111\n    \n    \n      1\n      20160602\n      65.8\n      80.8\n      55.0\n      25.8\n      18.777778\n    \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n      22.3\n      20.222222\n    \n    \n      3\n      20160604\n      57.5\n      70.9\n      47.3\n      23.6\n      14.166667\n    \n    \n      4\n      20160605\n      51.4\n      58.3\n      43.2\n      15.1\n      10.777778\n    \n  \n\n\n\n\n\n# Select temp column values on rows 0-5\nselection = data.loc[0:5, \"TEMP\"]\nselection\n\n0    65.5\n1    65.8\n2    68.4\n3    57.5\n4    51.4\n5    52.2\nName: TEMP, dtype: float64\n\n\n\nSelecting a single row\n\nrow = data.loc[4]\nrow\n\nYEARMODA        2.016060e+07\nTEMP            5.140000e+01\nMAX             5.830000e+01\nMIN             4.320000e+01\nDIFF            1.510000e+01\nTEMP_CELSIUS    1.077778e+01\nName: 4, dtype: float64\n\n\n\nrow[\"TEMP\"]\n\n51.4\n\n\n\n\nselection = data.loc[0:5, [\"TEMP\", \"TEMP_CELSIUS\"]]\nselection\n\n\n\n\n\n  \n    \n      \n      TEMP\n      TEMP_CELSIUS\n    \n  \n  \n    \n      0\n      65.5\n      18.611111\n    \n    \n      1\n      65.8\n      18.777778\n    \n    \n      2\n      68.4\n      20.222222\n    \n    \n      3\n      57.5\n      14.166667\n    \n    \n      4\n      51.4\n      10.777778\n    \n    \n      5\n      52.2\n      11.222222\n    \n  \n\n\n\n\n\nselection.at[0, \"TEMP\"]\n\n65.5\n\n\n\nselection.loc[0, \"TEMP\"]\n\n65.5\n\n\n\n\nSelection by position\n\n# Check the first rows\nprint(data.head())\nprint()\nprint(\"Value at position (0,0) is\", data.iloc[0, 0])\n\n   YEARMODA  TEMP   MAX   MIN  DIFF  TEMP_CELSIUS\n0  20160601  65.5  73.6  54.7  18.9     18.611111\n1  20160602  65.8  80.8  55.0  25.8     18.777778\n2  20160603  68.4  77.9  55.6  22.3     20.222222\n3  20160604  57.5  70.9  47.3  23.6     14.166667\n4  20160605  51.4  58.3  43.2  15.1     10.777778\n\nValue at position (0,0) is 20160601\n\n\n\ndata.iloc[-1, -1]\n\n18.722222222222225\n\n\n\ndata.tail()\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n    \n  \n  \n    \n      25\n      20160626\n      69.6\n      77.7\n      60.3\n      17.4\n      20.888889\n    \n    \n      26\n      20160627\n      60.7\n      70.0\n      57.6\n      12.4\n      15.944444\n    \n    \n      27\n      20160628\n      65.4\n      73.0\n      55.8\n      17.2\n      18.555556\n    \n    \n      28\n      20160629\n      65.8\n      73.2\n      59.7\n      13.5\n      18.777778\n    \n    \n      29\n      20160630\n      65.7\n      72.7\n      59.2\n      13.5\n      18.722222\n    \n  \n\n\n\n\n\ndata.iloc[-1, -2]\n\n13.5"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#selections-using-listed-criteria",
    "href": "lectures/week-03/2023-09-06.html#selections-using-listed-criteria",
    "title": "Geog385F23",
    "section": "Selections using listed criteria",
    "text": "Selections using listed criteria\n\n# List of values that will be used as basis for selecting the rows\nselection_criteria = [20160601, 20160608, 20160609]\n\n# Do the selection based on criteria applied to YEARMODA column\ndata.loc[data[\"YEARMODA\"].isin(selection_criteria)]\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n    \n  \n  \n    \n      0\n      20160601\n      65.5\n      73.6\n      54.7\n      18.9\n      18.611111\n    \n    \n      7\n      20160608\n      54.2\n      60.4\n      47.5\n      12.9\n      12.333333\n    \n    \n      8\n      20160609\n      49.4\n      54.1\n      45.7\n      8.4\n      9.666667"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#conditional-selection",
    "href": "lectures/week-03/2023-09-06.html#conditional-selection",
    "title": "Geog385F23",
    "section": "Conditional selection",
    "text": "Conditional selection\n\ndata.TEMP_CELSIUS > 15\n\n0      True\n1      True\n2      True\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13     True\n14     True\n15    False\n16     True\n17    False\n18    False\n19     True\n20     True\n21     True\n22     True\n23     True\n24     True\n25     True\n26     True\n27     True\n28     True\n29     True\nName: TEMP_CELSIUS, dtype: bool\n\n\n\nwarm_temps = data[data.TEMP_CELSIUS > 15]\nwarm_temps\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n    \n  \n  \n    \n      0\n      20160601\n      65.5\n      73.6\n      54.7\n      18.9\n      18.611111\n    \n    \n      1\n      20160602\n      65.8\n      80.8\n      55.0\n      25.8\n      18.777778\n    \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n      22.3\n      20.222222\n    \n    \n      13\n      20160614\n      59.7\n      67.8\n      47.8\n      20.0\n      15.388889\n    \n    \n      14\n      20160615\n      63.4\n      70.3\n      49.3\n      21.0\n      17.444444\n    \n    \n      16\n      20160617\n      60.4\n      70.7\n      55.9\n      14.8\n      15.777778\n    \n    \n      19\n      20160620\n      59.3\n      69.1\n      52.2\n      16.9\n      15.166667\n    \n    \n      20\n      20160621\n      62.6\n      71.4\n      50.4\n      21.0\n      17.000000\n    \n    \n      21\n      20160622\n      61.7\n      70.2\n      55.4\n      14.8\n      16.500000\n    \n    \n      22\n      20160623\n      60.9\n      67.1\n      54.9\n      12.2\n      16.055556\n    \n    \n      23\n      20160624\n      61.1\n      68.9\n      56.7\n      12.2\n      16.166667\n    \n    \n      24\n      20160625\n      65.7\n      75.4\n      57.9\n      17.5\n      18.722222\n    \n    \n      25\n      20160626\n      69.6\n      77.7\n      60.3\n      17.4\n      20.888889\n    \n    \n      26\n      20160627\n      60.7\n      70.0\n      57.6\n      12.4\n      15.944444\n    \n    \n      27\n      20160628\n      65.4\n      73.0\n      55.8\n      17.2\n      18.555556\n    \n    \n      28\n      20160629\n      65.8\n      73.2\n      59.7\n      13.5\n      18.777778\n    \n    \n      29\n      20160630\n      65.7\n      72.7\n      59.2\n      13.5\n      18.722222\n    \n  \n\n\n\n\n\nwarm_temps = data[(data[\"TEMP_CELSIUS\"] > 15) & (data[\"YEARMODA\"] >= 20160615)]\nwarm_temps\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n    \n  \n  \n    \n      14\n      20160615\n      63.4\n      70.3\n      49.3\n      21.0\n      17.444444\n    \n    \n      16\n      20160617\n      60.4\n      70.7\n      55.9\n      14.8\n      15.777778\n    \n    \n      19\n      20160620\n      59.3\n      69.1\n      52.2\n      16.9\n      15.166667\n    \n    \n      20\n      20160621\n      62.6\n      71.4\n      50.4\n      21.0\n      17.000000\n    \n    \n      21\n      20160622\n      61.7\n      70.2\n      55.4\n      14.8\n      16.500000\n    \n    \n      22\n      20160623\n      60.9\n      67.1\n      54.9\n      12.2\n      16.055556\n    \n    \n      23\n      20160624\n      61.1\n      68.9\n      56.7\n      12.2\n      16.166667\n    \n    \n      24\n      20160625\n      65.7\n      75.4\n      57.9\n      17.5\n      18.722222\n    \n    \n      25\n      20160626\n      69.6\n      77.7\n      60.3\n      17.4\n      20.888889\n    \n    \n      26\n      20160627\n      60.7\n      70.0\n      57.6\n      12.4\n      15.944444\n    \n    \n      27\n      20160628\n      65.4\n      73.0\n      55.8\n      17.2\n      18.555556\n    \n    \n      28\n      20160629\n      65.8\n      73.2\n      59.7\n      13.5\n      18.777778\n    \n    \n      29\n      20160630\n      65.7\n      72.7\n      59.2\n      13.5\n      18.722222\n    \n  \n\n\n\n\nSelectoins### Reset index\n\nwarm_temps = warm_temps.reset_index(drop=True)\nwarm_temps\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n    \n  \n  \n    \n      0\n      20160615\n      63.4\n      70.3\n      49.3\n      21.0\n      17.444444\n    \n    \n      1\n      20160617\n      60.4\n      70.7\n      55.9\n      14.8\n      15.777778\n    \n    \n      2\n      20160620\n      59.3\n      69.1\n      52.2\n      16.9\n      15.166667\n    \n    \n      3\n      20160621\n      62.6\n      71.4\n      50.4\n      21.0\n      17.000000\n    \n    \n      4\n      20160622\n      61.7\n      70.2\n      55.4\n      14.8\n      16.500000\n    \n    \n      5\n      20160623\n      60.9\n      67.1\n      54.9\n      12.2\n      16.055556\n    \n    \n      6\n      20160624\n      61.1\n      68.9\n      56.7\n      12.2\n      16.166667\n    \n    \n      7\n      20160625\n      65.7\n      75.4\n      57.9\n      17.5\n      18.722222\n    \n    \n      8\n      20160626\n      69.6\n      77.7\n      60.3\n      17.4\n      20.888889\n    \n    \n      9\n      20160627\n      60.7\n      70.0\n      57.6\n      12.4\n      15.944444\n    \n    \n      10\n      20160628\n      65.4\n      73.0\n      55.8\n      17.2\n      18.555556\n    \n    \n      11\n      20160629\n      65.8\n      73.2\n      59.7\n      13.5\n      18.777778\n    \n    \n      12\n      20160630\n      65.7\n      72.7\n      59.2\n      13.5\n      18.722222\n    \n  \n\n\n\n\n\nwarm_temps[\"MIN\"].hasnans\n\nFalse"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#sorting-data",
    "href": "lectures/week-03/2023-09-06.html#sorting-data",
    "title": "Geog385F23",
    "section": "Sorting Data",
    "text": "Sorting Data\n\ndata.sort_values(by=\"TEMP\").head()\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n    \n  \n  \n    \n      8\n      20160609\n      49.4\n      54.1\n      45.7\n      8.4\n      9.666667\n    \n    \n      9\n      20160610\n      49.5\n      55.9\n      43.0\n      12.9\n      9.722222\n    \n    \n      4\n      20160605\n      51.4\n      58.3\n      43.2\n      15.1\n      10.777778\n    \n    \n      5\n      20160606\n      52.2\n      59.7\n      42.8\n      16.9\n      11.222222\n    \n    \n      10\n      20160611\n      54.0\n      62.1\n      41.7\n      20.4\n      12.222222\n    \n  \n\n\n\n\n\ndata.sort_values(by=\"TEMP\", ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n    \n  \n  \n    \n      25\n      20160626\n      69.6\n      77.7\n      60.3\n      17.4\n      20.888889\n    \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n      22.3\n      20.222222\n    \n    \n      1\n      20160602\n      65.8\n      80.8\n      55.0\n      25.8\n      18.777778\n    \n    \n      28\n      20160629\n      65.8\n      73.2\n      59.7\n      13.5\n      18.777778\n    \n    \n      29\n      20160630\n      65.7\n      72.7\n      59.2\n      13.5\n      18.722222\n    \n  \n\n\n\n\n\n# Create a list of weekdays that matches with our data\n# The data covers 4 weeks + 2 days (i.e. altogether 30 days)\nweek_days = [\"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\", \"Mon\", \"Tue\"]\nday_list = week_days * 4 + week_days[:2]\n\n# Add the weekdays to our DataFrame\ndata[\"WEEKDAY\"] = day_list\ndata.head(10)\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n      WEEKDAY\n    \n  \n  \n    \n      0\n      20160601\n      65.5\n      73.6\n      54.7\n      18.9\n      18.611111\n      Wed\n    \n    \n      1\n      20160602\n      65.8\n      80.8\n      55.0\n      25.8\n      18.777778\n      Thu\n    \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n      22.3\n      20.222222\n      Fri\n    \n    \n      3\n      20160604\n      57.5\n      70.9\n      47.3\n      23.6\n      14.166667\n      Sat\n    \n    \n      4\n      20160605\n      51.4\n      58.3\n      43.2\n      15.1\n      10.777778\n      Sun\n    \n    \n      5\n      20160606\n      52.2\n      59.7\n      42.8\n      16.9\n      11.222222\n      Mon\n    \n    \n      6\n      20160607\n      56.9\n      65.1\n      45.9\n      19.2\n      13.833333\n      Tue\n    \n    \n      7\n      20160608\n      54.2\n      60.4\n      47.5\n      12.9\n      12.333333\n      Wed\n    \n    \n      8\n      20160609\n      49.4\n      54.1\n      45.7\n      8.4\n      9.666667\n      Thu\n    \n    \n      9\n      20160610\n      49.5\n      55.9\n      43.0\n      12.9\n      9.722222\n      Fri\n    \n  \n\n\n\n\n\ndata.sort_values(by=[\"WEEKDAY\", \"TEMP_CELSIUS\"], ascending=[True, False]).head(10)\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n      WEEKDAY\n    \n  \n  \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n      22.3\n      20.222222\n      Fri\n    \n    \n      23\n      20160624\n      61.1\n      68.9\n      56.7\n      12.2\n      16.166667\n      Fri\n    \n    \n      16\n      20160617\n      60.4\n      70.7\n      55.9\n      14.8\n      15.777778\n      Fri\n    \n    \n      9\n      20160610\n      49.5\n      55.9\n      43.0\n      12.9\n      9.722222\n      Fri\n    \n    \n      26\n      20160627\n      60.7\n      70.0\n      57.6\n      12.4\n      15.944444\n      Mon\n    \n    \n      19\n      20160620\n      59.3\n      69.1\n      52.2\n      16.9\n      15.166667\n      Mon\n    \n    \n      12\n      20160613\n      58.3\n      68.2\n      47.3\n      20.9\n      14.611111\n      Mon\n    \n    \n      5\n      20160606\n      52.2\n      59.7\n      42.8\n      16.9\n      11.222222\n      Mon\n    \n    \n      24\n      20160625\n      65.7\n      75.4\n      57.9\n      17.5\n      18.722222\n      Sat\n    \n    \n      3\n      20160604\n      57.5\n      70.9\n      47.3\n      23.6\n      14.166667\n      Sat\n    \n  \n\n\n\n\n\ndata.sort_values(by=[\"WEEKDAY\"], ascending=[True]).head(10)\n\n\n\n\n\n  \n    \n      \n      YEARMODA\n      TEMP\n      MAX\n      MIN\n      DIFF\n      TEMP_CELSIUS\n      WEEKDAY\n    \n  \n  \n    \n      2\n      20160603\n      68.4\n      77.9\n      55.6\n      22.3\n      20.222222\n      Fri\n    \n    \n      16\n      20160617\n      60.4\n      70.7\n      55.9\n      14.8\n      15.777778\n      Fri\n    \n    \n      23\n      20160624\n      61.1\n      68.9\n      56.7\n      12.2\n      16.166667\n      Fri\n    \n    \n      9\n      20160610\n      49.5\n      55.9\n      43.0\n      12.9\n      9.722222\n      Fri\n    \n    \n      12\n      20160613\n      58.3\n      68.2\n      47.3\n      20.9\n      14.611111\n      Mon\n    \n    \n      19\n      20160620\n      59.3\n      69.1\n      52.2\n      16.9\n      15.166667\n      Mon\n    \n    \n      5\n      20160606\n      52.2\n      59.7\n      42.8\n      16.9\n      11.222222\n      Mon\n    \n    \n      26\n      20160627\n      60.7\n      70.0\n      57.6\n      12.4\n      15.944444\n      Mon\n    \n    \n      17\n      20160618\n      57.3\n      62.8\n      54.0\n      8.8\n      14.055556\n      Sat\n    \n    \n      10\n      20160611\n      54.0\n      62.1\n      41.7\n      20.4\n      12.222222\n      Sat"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#visualization",
    "href": "lectures/week-03/2023-09-06.html#visualization",
    "title": "Geog385F23",
    "section": "Visualization",
    "text": "Visualization\n\nimport seaborn as sns\nsns.set()\n\n\nimport pandas\n\n\ndf = pandas.read_csv(\"data/shared/covid/covid_combined.csv\",\n                     index_col='date', parse_dates=True)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      state\n      fips\n      cases\n      deaths\n      dtc100\n      population\n      deaths100k\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-21\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-22\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-23\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-24\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n    \n      2020-01-25\n      Washington\n      53\n      1\n      0\n      0.0\n      7614893\n      0.0\n    \n  \n\n\n\n\n\ndf.tail()\n\n\n\n\n\n  \n    \n      \n      state\n      fips\n      cases\n      deaths\n      dtc100\n      population\n      deaths100k\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-07-29\n      Northern Mariana Islands\n      69\n      40\n      2\n      5.000000\n      55194\n      3.623582\n    \n    \n      2020-07-30\n      Northern Mariana Islands\n      69\n      42\n      2\n      4.761905\n      55194\n      3.623582\n    \n    \n      2020-07-31\n      Northern Mariana Islands\n      69\n      42\n      2\n      4.761905\n      55194\n      3.623582\n    \n    \n      2020-08-01\n      Northern Mariana Islands\n      69\n      44\n      2\n      4.545455\n      55194\n      3.623582\n    \n    \n      2020-08-02\n      Northern Mariana Islands\n      69\n      45\n      2\n      4.444444\n      55194\n      3.623582\n    \n  \n\n\n\n\n\ndf.shape\n\n(8287, 7)\n\n\n\ndf.index\n\nDatetimeIndex(['2020-01-21', '2020-01-22', '2020-01-23', '2020-01-24',\n               '2020-01-25', '2020-01-26', '2020-01-27', '2020-01-28',\n               '2020-01-29', '2020-01-30',\n               ...\n               '2020-07-24', '2020-07-25', '2020-07-26', '2020-07-27',\n               '2020-07-28', '2020-07-29', '2020-07-30', '2020-07-31',\n               '2020-08-01', '2020-08-02'],\n              dtype='datetime64[ns]', name='date', length=8287, freq=None)\n\n\n\nlast_df = df.loc['2020-08-02']\n\n\nlast_df.shape\n\n(54, 7)\n\n\n\nlast_df.reset_index(inplace=True)\nlast_df\n\n\n\n\n\n  \n    \n      \n      date\n      state\n      fips\n      cases\n      deaths\n      dtc100\n      population\n      deaths100k\n    \n  \n  \n    \n      0\n      2020-08-02\n      Washington\n      53\n      60161\n      1680\n      2.792507\n      7614893\n      22.062030\n    \n    \n      1\n      2020-08-02\n      Illinois\n      17\n      183662\n      7718\n      4.202285\n      12671821\n      60.906795\n    \n    \n      2\n      2020-08-02\n      California\n      6\n      515937\n      9399\n      1.821734\n      39512223\n      23.787576\n    \n    \n      3\n      2020-08-02\n      Arizona\n      4\n      178473\n      3769\n      2.111804\n      7278717\n      51.781104\n    \n    \n      4\n      2020-08-02\n      Massachusetts\n      25\n      118458\n      8638\n      7.292036\n      6949503\n      124.296658\n    \n    \n      5\n      2020-08-02\n      Wisconsin\n      55\n      58990\n      956\n      1.620614\n      5822434\n      16.419250\n    \n    \n      6\n      2020-08-02\n      Texas\n      48\n      452826\n      7515\n      1.659578\n      28995881\n      25.917474\n    \n    \n      7\n      2020-08-02\n      Nebraska\n      31\n      26702\n      338\n      1.265823\n      1934408\n      17.473046\n    \n    \n      8\n      2020-08-02\n      Utah\n      49\n      41175\n      313\n      0.760170\n      3205958\n      9.763072\n    \n    \n      9\n      2020-08-02\n      Oregon\n      41\n      19097\n      331\n      1.733257\n      4217737\n      7.847810\n    \n    \n      10\n      2020-08-02\n      Florida\n      12\n      487124\n      7083\n      1.454045\n      21477737\n      32.978335\n    \n    \n      11\n      2020-08-02\n      New York\n      36\n      421008\n      32401\n      7.696053\n      19453561\n      166.555624\n    \n    \n      12\n      2020-08-02\n      Rhode Island\n      44\n      19022\n      1007\n      5.293870\n      1059361\n      95.057303\n    \n    \n      13\n      2020-08-02\n      Georgia\n      13\n      177556\n      3758\n      2.116515\n      10617423\n      35.394653\n    \n    \n      14\n      2020-08-02\n      New Hampshire\n      33\n      6634\n      417\n      6.285800\n      1359711\n      30.668282\n    \n    \n      15\n      2020-08-02\n      North Carolina\n      37\n      125339\n      1996\n      1.592481\n      10488084\n      19.031121\n    \n    \n      16\n      2020-08-02\n      New Jersey\n      34\n      184225\n      15836\n      8.596010\n      8882190\n      178.289363\n    \n    \n      17\n      2020-08-02\n      Colorado\n      8\n      47799\n      1846\n      3.862005\n      5758736\n      32.055646\n    \n    \n      18\n      2020-08-02\n      Maryland\n      24\n      90835\n      3515\n      3.869654\n      6045680\n      58.140689\n    \n    \n      19\n      2020-08-02\n      Nevada\n      32\n      50270\n      833\n      1.657052\n      3080156\n      27.044085\n    \n    \n      20\n      2020-08-02\n      Tennessee\n      47\n      106804\n      1062\n      0.994345\n      6833174\n      15.541826\n    \n    \n      21\n      2020-08-02\n      Hawaii\n      15\n      2219\n      25\n      1.126634\n      1415872\n      1.765696\n    \n    \n      22\n      2020-08-02\n      Indiana\n      18\n      69531\n      2975\n      4.278667\n      6732219\n      44.190482\n    \n    \n      23\n      2020-08-02\n      Kentucky\n      21\n      31966\n      759\n      2.374398\n      4467673\n      16.988710\n    \n    \n      24\n      2020-08-02\n      Minnesota\n      27\n      55987\n      1654\n      2.954257\n      5639632\n      29.328155\n    \n    \n      25\n      2020-08-02\n      Oklahoma\n      40\n      38201\n      550\n      1.439753\n      3956971\n      13.899521\n    \n    \n      26\n      2020-08-02\n      Pennsylvania\n      42\n      118038\n      7274\n      6.162422\n      12801989\n      56.819296\n    \n    \n      27\n      2020-08-02\n      South Carolina\n      45\n      91788\n      1777\n      1.935983\n      5148714\n      34.513473\n    \n    \n      28\n      2020-08-02\n      District of Columbia\n      11\n      12274\n      586\n      4.774320\n      705749\n      83.032353\n    \n    \n      29\n      2020-08-02\n      Kansas\n      20\n      28341\n      361\n      1.273773\n      2913314\n      12.391387\n    \n    \n      30\n      2020-08-02\n      Missouri\n      29\n      52550\n      1311\n      2.494767\n      6137428\n      21.360739\n    \n    \n      31\n      2020-08-02\n      Vermont\n      50\n      1426\n      57\n      3.997195\n      623989\n      9.134776\n    \n    \n      32\n      2020-08-02\n      Virginia\n      51\n      91782\n      2218\n      2.416596\n      8535519\n      25.985532\n    \n    \n      33\n      2020-08-02\n      Connecticut\n      9\n      49810\n      4432\n      8.897812\n      3565287\n      124.309768\n    \n    \n      34\n      2020-08-02\n      Iowa\n      19\n      45723\n      878\n      1.920259\n      3155070\n      27.828226\n    \n    \n      35\n      2020-08-02\n      Louisiana\n      22\n      119861\n      4007\n      3.343039\n      4648794\n      86.194398\n    \n    \n      36\n      2020-08-02\n      Ohio\n      39\n      93031\n      3529\n      3.793359\n      11689100\n      30.190519\n    \n    \n      37\n      2020-08-02\n      Michigan\n      26\n      91857\n      6460\n      7.032670\n      9986857\n      64.685016\n    \n    \n      38\n      2020-08-02\n      South Dakota\n      46\n      8955\n      135\n      1.507538\n      884659\n      15.260117\n    \n    \n      39\n      2020-08-02\n      Arkansas\n      5\n      43810\n      464\n      1.059119\n      3017825\n      15.375312\n    \n    \n      40\n      2020-08-02\n      Delaware\n      10\n      14949\n      585\n      3.913305\n      973764\n      60.076158\n    \n    \n      41\n      2020-08-02\n      Mississippi\n      28\n      60553\n      1703\n      2.812412\n      2976149\n      57.221597\n    \n    \n      42\n      2020-08-02\n      New Mexico\n      35\n      21016\n      654\n      3.111915\n      2096829\n      31.189954\n    \n    \n      43\n      2020-08-02\n      North Dakota\n      38\n      6664\n      109\n      1.635654\n      762062\n      14.303298\n    \n    \n      44\n      2020-08-02\n      Wyoming\n      56\n      2808\n      26\n      0.925926\n      578759\n      4.492371\n    \n    \n      45\n      2020-08-02\n      Alaska\n      2\n      3982\n      22\n      0.552486\n      731545\n      3.007334\n    \n    \n      46\n      2020-08-02\n      Maine\n      23\n      3958\n      123\n      3.107630\n      1344212\n      9.150342\n    \n    \n      47\n      2020-08-02\n      Alabama\n      1\n      91444\n      1627\n      1.779231\n      4903185\n      33.182513\n    \n    \n      48\n      2020-08-02\n      Idaho\n      16\n      21465\n      197\n      0.917773\n      1787065\n      11.023662\n    \n    \n      49\n      2020-08-02\n      Montana\n      30\n      4193\n      61\n      1.454806\n      1068778\n      5.707453\n    \n    \n      50\n      2020-08-02\n      Puerto Rico\n      72\n      18411\n      230\n      1.249253\n      3193694\n      7.201692\n    \n    \n      51\n      2020-08-02\n      Guam\n      66\n      1328\n      6\n      0.451807\n      165718\n      3.620609\n    \n    \n      52\n      2020-08-02\n      West Virginia\n      54\n      6854\n      117\n      1.707032\n      1792147\n      6.528482\n    \n    \n      53\n      2020-08-02\n      Northern Mariana Islands\n      69\n      45\n      2\n      4.444444\n      55194\n      3.623582\n    \n  \n\n\n\n\n\n_ = sns.displot(last_df.deaths100k)\n\n\n\n\n\n_ = sns.displot(last_df.deaths100k, rug=True)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]