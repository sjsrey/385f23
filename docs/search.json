[
  {
    "objectID": "syllabus.html#class-meetings",
    "href": "syllabus.html#class-meetings",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Class Meetings",
    "text": "Class Meetings\n\n\n\nMeeting\nLocation\nTime\n\n\n\n\nLecture\nLSN 111\nMon & Wed 3:30 - 4:45pm"
  },
  {
    "objectID": "syllabus.html#teaching-team",
    "href": "syllabus.html#teaching-team",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nSergio Rey\nThu 3:30 - 4:30pm (by appointment)\nPSFA 361G\n\n\nJin Huang\nFri 10:30am\nPSFA 361F"
  },
  {
    "objectID": "syllabus.html#introduction",
    "href": "syllabus.html#introduction",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Introduction",
    "text": "Introduction\nWelcome to 385: Spatial Data Analysis!\nThe purpose of this course is to introduce you to methods of spatial data analysis. The focus is on both the conceptual and applied aspects of spatial statistical methods. We will place particular emphasis on the computational aspects of Exploratory Spatial Data Analysis (ESDA) methods for diﬀerent types of spatial data including point processes, lattice data, geostatistical data, network data, and spatial interaction. Throughout the course you will gain valuable hands-on experience with several specialized software packages for spatial data analysis. The overriding goal of the course is for you to acquire familiarity with the fundamental methodological and operational issues in the statistical analysis of geographic information and the ability to extend these methods in your own research.\nThe course takes an explicitly computational thinking approach to its pedagogy. Students are introduced to computational concepts and tools that are increasingly important to research that engages with geospatial data. By adopting these tools, students acquire a deeper engagement with, and mastery of, the substantive concepts. Put differently, students will learn to code. But this is a means to the end goal: students will code to learn spatial data analysis.\nIn the scope of a 15-week semester course we can only introduce a handful of the key concepts and methods relevant to the field of spatial data analysis. As such, the course is not intended as an exhaustive treatment. Instead, the goal is that students will acquire an understanding of the more common and useful methods and practices, and use the course as an entry point for further engagement with the field."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nGEOG 101 or GEOG 102\nSTAT 250 or comparable course in statistics.\n\nAll students are required to complete the prerequisite assessment quiz before 2023-08-23 3:00pm."
  },
  {
    "objectID": "syllabus.html#computational-learning",
    "href": "syllabus.html#computational-learning",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Computational Learning",
    "text": "Computational Learning\nWe will be using open source geospatial software throughout the course together with Jupyter Notebooks, and Python as our scripting language.\nAll software for the course will be made available through JupyterHub a web-based framework. Students wishing to install these materials on their own machines will be given instructions to do so, but this is not required."
  },
  {
    "objectID": "syllabus.html#readings",
    "href": "syllabus.html#readings",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Readings",
    "text": "Readings\nAll required readings are available through the links listed below. Assigned readings should be completed before the date listed in the schedule (see below). Readings are a critical part of the discussions we will hold in class, and therefore being prepared for class means having completed the readings and thought about the content. It will be difficult to do well in this course without having completed the readings.\n\n\n\nAbbrevation\nSource\n\n\n\n\nGDA\nTenkanen, H., V. Heikinheimo, D. Whipp (2023) Python for Geographic Data Analysis. CRC Press.\n\n\nGDS\nRey, S.J., D. Arribas-Bel, L.J. Wolf (2023) Geographic Data Science with Python. CRC Press."
  },
  {
    "objectID": "syllabus.html#schedule-planned",
    "href": "syllabus.html#schedule-planned",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)\n\n\n\nWeek\nDates\nTopic\nReading\nDue\n\n\n\n\n1\nAug-21\nCourse Introduction\n\n\n\n\n\nAug-23\nJupyter Hub\nGDA 1 GDS 2\n\n\n\n2\nAug-28\nPython: Programming Concepts\nGDA 2\nQuiz 1\n\n\n\nAug-30\nPython: Scripting\nGDA 2\n\n\n\n3\nSep-04\nLabor Day Holiday\n\n\n\n\n\nSep-06\nPython: Data Analysis/Visualization\nGDA 3,4\n\n\n\n4\nSep-11\nPython: Geographic Data\nGDA 5\nQuiz 2\n\n\n\nSep-13\nGeopandas\nGDA 6\nExercise 1\n\n\n5\nSep-18\nPySAL\nGDS 3\nQuiz 3\n\n\n\nSep-20\nGeoVisualization\nGDS 5\n\n\n\n6\nSep-25\nSpatial Weights\nGDS 4\nQuiz 4\n\n\n\nSep-27\nSpatial Dependence\nGDS 6\n\n\n\n7\nOct-02\nGlobal Autocorrelation\nGDS 6\nQuiz 5\n\n\n\nOct-04\nGlobal Autocorrelation Tests\nGDS 6\n\n\n\n8\nOct-09\nStudio\nGDS 7\nQuiz 6\n\n\n\nOct-11\nLocal Autocorrelation\nGDS 7\n\n\n\n9\nOct-16\nLocal Autocorrelation Tests\nGDS 8\nQuiz 7, Exercise 2\n\n\n\nOct-18\nPoint Pattern Data\nGDS 8\n\n\n\n10\nOct-23\nCentrography\nGDS 8\nQuiz 8\n\n\n\nOct-25\nPoint Processes\nGDS 8\n\n\n\n11\nOct-30\nQuadrat Statistics\nGDS 8\nQuiz 9\n\n\n\nNov-01\nNearest Neighbor Statistics\nGDS 8\n\n\n\n12\nNov-06\nDistance Based Statistics\nGDS 8\nQuiz 10\n\n\n\nNov-08\nGeostatistical Data\nDS 6\nExercise 3\n\n\n13\nNov-13\nSpatial Interpolation\nDS 6.6\nQuiz 11\n\n\n\nNov-15\nNARSC Workshop (Zoom)\n\n\n\n\n14\nNov-20\nNetwork Data\nDS 7\nQuiz 12\n\n\n\nNov-22\nNo Class SDSU Holiday\n\n\n\n\n15\nNov-27\nSpatial Interaction Data\nTO\nQuiz 13\n\n\n\nNov-29\nMeasuring Spatial Disparities\nGDS 9\n\n\n\n16\nDec-04\nSpatial Segregation\nCEN\nQuiz 14\n\n\n\nDec-06\nNext Steps in SDA\n\nExercise 4\n\n\n17\nDec-11\nFinal Review\n\nQuiz 15\n\n\n18\nDec-18\nFinal Exam (13:00-15:00)"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Grading",
    "text": "Grading\nGEOG385 uses specification grading in evaluating student work and in determining your final course grade. Your course grade will be based on the quality and quantity of the work that you submit that is evaluated to be of an acceptable level of quality. The acceptable level of quality demonstrates competency in the concepts and methods covered in the course.\nThere is a two-step process for determination of your final course grade at the end of the quarter:\n\nUsing your quizzes and exercises, your base grade is determined.\nUsing your final exam results, determine if your base grade includes a \"plus\", \"minus\", or level drop to form the course grade.\n\nFor Step 1, the base grade is determined using the following specification:\n\n\n\nLevel\nHurdles\n\n\n\n\nA\nPass at least 13 of 15 quizzes and earn \"Demonstrates Competency\" on 4 of 4 exercises,\n\n\nB\nPass at least 11 of 15 quizzes and earn \"Demonstrates Competency\" on 3 of 4 exercises\n\n\nC\nPass at least 9 of 15 quizzes and earn \"Demonstrates Competency\" on 2 of 4 exercises\n\n\nD\nPass at least 7 of 15 quizzes and earn \"Demonstrates Competency\" on 1 of 4 exercises\n\n\nF\nFail to clear D-level hurdles\n\n\n\nFor Step 2, your final course grade is determined as follows:\n\nIf you earn at least 85% on the final exam, you will obtain a “+” for your grade. So a B base grade would become a B+ course grade, and so on (Note: SDSU does not record A+ grades).\nIf you score between 70-85% on the final exam, your base grade becomes your course grade.\nIf you score between 50% and 69% on the final exam, you will obtain a “-” for your grade. So an A base grade becomes an A- course grade, a B base grade becomes a B- course grade, and so on.\nIf you score less than 50% on the final exam, your course grade will drop one level: An A base grade becomes a final B course grade."
  },
  {
    "objectID": "syllabus.html#quizzes",
    "href": "syllabus.html#quizzes",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Quizzes",
    "text": "Quizzes\nQuizzes are graded on a pass/fail basis. Starting in week two, there will be a quiz due before a session that pertains to the background reading that is required before our work in class."
  },
  {
    "objectID": "syllabus.html#exercises",
    "href": "syllabus.html#exercises",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Exercises",
    "text": "Exercises\nFour exercises will be introduced in class and are to be completed outside of class meetings.\nEach exercise is graded using a CRN rubric that classifies work with marks of C (\"Demonstrates Competence\"), R (\"Needs Revision\"), or N (\"Not assessable\"):\nOf each exercise the following questions will be asked: Does the work demonstrate that the student understands the concepts? Does the work demonstrate competence and meet the expectations outlined in the exercise?\nIf the answer is \"yes\" to both of the questions, a student passes the hurdle for that exercise.\nIf the initial submission does not clear the hurdle, then a second question is asked: Is there evidence of partial understanding of the concepts? If the answer to this question is \"Yes\" the student can exchange one token to attempt a revision of their work. If the answer is \"No\", the student does not clear the hurdle for this exercise and will not have the opportunity to revise their work."
  },
  {
    "objectID": "syllabus.html#final-exam",
    "href": "syllabus.html#final-exam",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Final Exam",
    "text": "Final Exam\nA closed book, closed note, timed final exam will be given on December 18 (13:00-15:00). The exam will be based on a blend of previous quiz questions and additional questions that pertain to material covered in class."
  },
  {
    "objectID": "syllabus.html#tokens",
    "href": "syllabus.html#tokens",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Tokens",
    "text": "Tokens\nEach student is provided with three tokens at the beginning of the semester.\nUsing Tokens\n\nOne token can be used for a one-day extension for an exercise.\nOne token can be used to revise an exercise that was submitted on-time but evaluated as \"Needing Revision\".\nTwo tokens can be used to request a make-up date for the final exam. (Requests required by 2023-11-18 17:00.)\n\nRemaining Tokens\nEach token that remains unused after 2023-12-18 will be counted as a passed quiz. Tokens cannot be exchanged with other students."
  },
  {
    "objectID": "syllabus.html#policies",
    "href": "syllabus.html#policies",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "Policies",
    "text": "Policies\n\nAccommodations\nIf you are a student with a disability and are in need of accommodations for this class, please contact Student Ability Success Center at (619) 594-6473 as soon as possible. Please know accommodations are not retroactive, and I cannot provide accommodations based upon disability until I have received an accommodation letter from Student Ability Success Center.\n\n\nPrivacy and Intellectual Property\nStudent Privacy and Intellectual Property: The Family Educational Rights and Privacy Act (FERPA) mandates the protection of student information, including contact information, grades, and graded assignments. I will use Canvas to communicate with you, and I will not post grades or leave graded assignments in public places. Students will be notified at the time of an assignment if copies of student work will be retained beyond the end of the semester or used as examples for future students or the wider public. Students maintain intellectual property rights to work products they create as part of this course unless they are formally notified otherwise.\n\n\nAcademic Integrity\nThe SDSU student academic integrity policy lists violations in detail. These violations fall into eight broad areas that include but are not limited to: cheating, fabrication, plagiarism, facilitating academic misconduct, unauthorized collaboration, interference or sabotage, non-compliance with research regulations and retaliation. For more information about the SDSU student academic integrity policy, please see the following: https://sacd.sdsu.edu/student-rights/academic-dishonesty.\n\n\nCode of Conduct\nAs course instructor, I am dedicated to providing a harassment-free learning experience for all students, regardless of gender, sexual orientation, disability, physical appearance, body size, race, religion, or choice of operating system. All course participants are expected to show respect and courtesy to other students throughout the semester. As a learning community we do not tolerate harassment of participants in any form.\n\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery are not appropriate in this course.\nBe kind to others. Do not insult or put down other students. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for this course.\nStudents violating these rules may be asked to leave the course, and their violations will be reported to the SDSU administration.\n\nThis code of conduct is an adaptation of the SciPy 2018 Code of Conduct."
  },
  {
    "objectID": "lectures/week-14/2023-11-20-interpolation_kriging.html",
    "href": "lectures/week-14/2023-11-20-interpolation_kriging.html",
    "title": "Spatial Interpolation: Kriging",
    "section": "",
    "text": "non-deterministic interpolation\nmeasures of uncertainty\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (16, 9)\nimport rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nimport pykrige\nprecip = gpd.read_file(\"precip_sd.geojson\")\nprecip.plot(column='inches', legend=True);\ncounty = gpd.read_file(\"sdcounty.geojson\")\nm = county.explore()\nprecip.explore(column='inches', m=m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "lectures/week-14/2023-11-20-interpolation_kriging.html#fit",
    "href": "lectures/week-14/2023-11-20-interpolation_kriging.html#fit",
    "title": "Spatial Interpolation: Kriging",
    "section": "Fit",
    "text": "Fit\n\nh3_cents = county_h3.centroid\nimport rioxarray\nimport rasterio\n\n\n# get observed values for all grid cells\n\n\nclipped = rasterio.open(\"clipped_example.tif\")\n\n\nclipped.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': None,\n 'width': 527,\n 'height': 336,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.002913805220117985, 0.0, -117.61397387622179,\n        0.0, -0.0029142217548938525, 33.50612487372884)}\n\n\n\nh3_cents_4326 = h3_cents.to_crs(clipped.meta['crs'])\ncp = h3_cents_4326\ncoord_list = [(x, y) for x, y in zip(cp.x, cp.y)]\n\n\nobservations = [x[0] for x in clipped.sample(coord_list)]\ncounty_h3['inches'] = observations\n\n\ncounty_h3.plot(column='inches', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# calculate fit for each approach (MAPE)\ndef mape(est, obs):\n    err = est-obs\n    aerr = numpy.abs(err)\n    den = obs + (obs == 0)\n    paerr = aerr / den\n    paerr *= 100\n    return paerr.mean()\n\n\nmape(county_h3.nn1_est, county_h3.inches)\n\n27.99203089869245\n\n\n\nmape(county_h3.nn5_est, county_h3.inches)\n\n40.242887541921576\n\n\n\nmape(county_h3.nn5id_est, county_h3.inches)\n\n33.161102090343014\n\n\n\nmape(county_h3.ok_est, county_h3.inches)\n\n23.964144515966048\n\n\n\n# plot fit\n\n\n# map errors for different models\n\n\nen1 = county_h3.nn1_est - county_h3.inches\n\ncounty_h3['nn1_error'] = en1\n\ncounty_h3.plot(column='nn1_error', legend=True, cmap='coolwarm')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nen5id = county_h3.nn5id_est - county_h3.inches\n\ncounty_h3['nn5id_error'] = en5id\n\ncounty_h3.plot(column='nn5id_error', legend=True, cmap='coolwarm')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(x=county_h3.inches, y=county_h3.nn5id_error);"
  },
  {
    "objectID": "lectures/week-07/2023-10-04.html",
    "href": "lectures/week-07/2023-10-04.html",
    "title": "Moran’s I",
    "section": "",
    "text": "import geopandas\ngdf = geopandas.read_parquet('repub_lean.parquet')\ngdf.plot()\n\n&lt;AxesSubplot:&gt;\ngdf.plot(column='rep_int', cmap='Reds', figsize=(16,9))\n\n&lt;AxesSubplot:&gt;\nimport matplotlib.pyplot as plt\nf, axs = plt.subplots(1, 2, figsize=(16, 9))\nax1, ax2 = axs\n\n\n\ngdf.plot(column='rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican\")\n\n\ngdf.plot(column='lag_rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\nax2.set_title(\"Spatial Lag Leaning Republican\")\n\nText(0.5, 1.0, 'Spatial Lag Leaning Republican')\nimport seaborn as sns\n_ = sns.regplot(x='rep_int', y='lag_rep_int', data=gdf)\nplt.axhline(y=gdf.lag_rep_int.mean(), color='g', linestyle='--')\nplt.axvline(x=gdf.rep_int.mean(), color='g', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f484009e280&gt;\ngdf.head()\n\n\n\n\n\n\n\n\nindex\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nRep\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\nrep_int\nqueen_neighbors\nlag_rep_int\n\n\n\n\n0\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((2516172.424 5253443.650, 25164...\n36%\n17%\n47%\n303\n36\n1\n35.00\n\n\n1\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((2351724.597 4850457.653, 23526...\n27%\n17%\n56%\n704\n27\n5\n30.80\n\n\n2\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((821167.988 5338182.388, 821794...\n34%\n19%\n47%\n982\n34\n3\n42.00\n\n\n3\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-371533.418 5042503.702, -386687.255...\n49%\n21%\n30%\n312\n49\n4\n52.25\n\n\n4\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-1384104.733 4308747.816, -1385163.9...\n37%\n18%\n46%\n314\n37\n5\n41.00\ny = gdf.rep_int\nylag = gdf.lag_rep_int\n\nyh = y &gt; y.mean()\nylh = ylag &gt; ylag.mean()\nq = 1 * yh * ylh + (2 * (1-yh) * ylh) + (3 * (1-yh) * (1-ylh))\nq[q==0] = 4\ngdf['q'] = q\nf, axs = plt.subplots(2,2, figsize=(16, 9))\nax1 = axs[0,0]\nax2 = axs[0,1]\nax3 = axs[1,0]\nax4 = axs[1,1]\n\n\ngdf.plot(column='rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican\")\n\n\ngdf.plot(column='lag_rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\nax2.set_title(\"Leaning Republican Spatial Lag\")\n\n\ngdf.plot(column='q',\n         categorical=True,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax3,\n        )\nax3.set_axis_off()\n_= ax3.set_title(\"Moran Scatterplot Quadrant\")\n\n\n_ = sns.regplot(x='rep_int', y='lag_rep_int', data=gdf, ax=ax4)\nplt.axhline(y=gdf.lag_rep_int.mean(), color='g', linestyle='--')\nplt.axvline(x=gdf.rep_int.mean(), color='g', linestyle='--')\n_= ax4.set_title(\"Moran Scatterplot\")\n\\[\nI = \\frac{n}{\\sum_i \\sum_j w_{i,j}} \\frac{\\sum_i \\sum_j w_{i,j} z_i z_j} {\\sum_i z_i^2}\n\\] where \\(n\\) is the number of observations, \\(z_i = y_i - \\bar{y}\\), and \\(w_{i,j}\\) is the cell corresponding to the \\(i\\)-th row and \\(j\\)-th column of the spatial weights matrix.\nfrom esda.moran import Moran\nimport libpysal\ny = gdf.rep_int\nw = libpysal.weights.Queen.from_dataframe(gdf)\nmi = Moran(y, w)\nmi.EI\n\n-0.020833333333333332\nmi.EI_sim\n\n-0.018233960161648737\nmi.p_sim\n\n0.001\nimport seaborn\nax = seaborn.histplot(mi.sim)\n_ = plt.axvline(mi.I, 0, 100, color='r')"
  },
  {
    "objectID": "lectures/week-07/2023-10-04.html#a-closer-look-at-permutation-based-inference",
    "href": "lectures/week-07/2023-10-04.html#a-closer-look-at-permutation-based-inference",
    "title": "Moran’s I",
    "section": "A closer look at permutation based inference",
    "text": "A closer look at permutation based inference\n\nimport numpy\n\n\ngdf.shape\n\n(49, 15)\n\n\n\nids = numpy.arange(49)\n\n\nids\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48])\n\n\n\nnumpy.random.seed(12345) # for reproducibility\nrids = numpy.random.permutation(ids)\n\n\nrids\n\narray([44, 21, 12, 40, 35,  8, 20, 33, 32,  2, 31, 38, 22, 30, 42, 26, 19,\n        5,  3, 15,  0,  4, 39, 25, 24, 23, 28, 43,  7,  6, 18, 17, 10, 13,\n       11,  9, 16, 27, 14, 45, 46, 48, 47, 41, 36,  1, 29, 37, 34])\n\n\n\ny = gdf.rep_int.values\n\n\ny\n\narray([36, 27, 34, 49, 37, 30, 28, 41, 42, 39, 30, 48, 39, 54, 33, 42, 31,\n       52, 40, 46, 30, 41, 32, 29, 11, 37, 41, 49, 33, 42, 41, 46, 44, 41,\n       39, 44, 41, 47, 35, 37, 50, 45, 32, 43, 53, 29, 43, 43, 57])\n\n\n\ny[rids]\n\narray([53, 41, 39, 50, 44, 42, 30, 41, 44, 34, 46, 35, 32, 41, 32, 41, 46,\n       30, 49, 42, 36, 37, 37, 37, 11, 29, 33, 43, 41, 28, 40, 52, 30, 54,\n       48, 39, 31, 49, 33, 29, 43, 57, 43, 45, 41, 27, 42, 47, 39])\n\n\n\nyr = y[rids]\n\n\nfrom libpysal.weights import lag_spatial\n\n\nw.transform='r'\n\n\nlag_yr = lag_spatial(w, yr)\n\n\ngdf['lag_yr'] = lag_yr\ngdf['yr' ]= yr\n\n\nyh = yr &gt; y.mean()\nylh = lag_yr &gt; lag_yr.mean()\nq = 1 * yh * ylh + (2 * (1-yh) * ylh) + (3 * (1-yh) * (1-ylh))\nq[q==0] = 4\ngdf['qr'] = q\n\n\ngdf.qr\n\n0     4\n1     4\n2     3\n3     1\n4     1\n5     4\n6     3\n7     4\n8     4\n9     2\n10    4\n11    3\n12    2\n13    1\n14    2\n15    1\n16    4\n17    3\n18    4\n19    1\n20    2\n21    2\n22    3\n23    2\n24    2\n25    3\n26    3\n27    1\n28    4\n29    3\n30    1\n31    1\n32    3\n33    4\n34    1\n35    2\n36    2\n37    1\n38    2\n39    2\n40    1\n41    4\n42    4\n43    4\n44    1\n45    3\n46    4\n47    4\n48    2\nName: qr, dtype: int64\n\n\n\n \nmir = Moran(yr, w)\n\n\nf, axs = plt.subplots(2,2, figsize=(16, 9))\n\nax1 = axs[0,0]\nax2 = axs[0,1]\nax3 = axs[1,0]\nax4 = axs[1,1]\n\n\ngdf.plot(column='yr',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican - Random\")\n\n\ngdf.plot(column='lag_yr',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\nax2.set_title(\"Leaning Republican Spatial Lag - Random\")\n\n\ngdf.plot(column='qr',\n         categorical=True,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax3,\n        )\nax3.set_axis_off()\n_= ax3.set_title(\"Moran Scatterplot Quadrant - Random\")\n\n\n_ = sns.regplot(x='yr', y='lag_yr', data=gdf, ax=ax4)\nplt.axhline(y=gdf.lag_rep_int.mean(), color='g', linestyle='--')\nplt.axvline(x=gdf.rep_int.mean(), color='g', linestyle='--')\n_= ax4.set_title(\"Moran Scatterplot\")\n\n\n\n\n\nmir.p_sim\n\n0.385\n\n\n\nmi.I, mir.I\n\n(0.4648896481706091, -0.049619432153642166)"
  },
  {
    "objectID": "lectures/week-07/2023-10-04.html#how-many-random-permutations-maps-can-we-conceivably-make",
    "href": "lectures/week-07/2023-10-04.html#how-many-random-permutations-maps-can-we-conceivably-make",
    "title": "Moran’s I",
    "section": "How many random permutations (maps) can we conceivably make?",
    "text": "How many random permutations (maps) can we conceivably make?\n\\[n!\\]\n\ngdf.shape\n\n(49, 18)\n\n\n\nimport math\n\n\nmath.factorial(49)\n\n608281864034267560872252163321295376887552831379210240000000000\n\n\n\nf'{float(math.factorial(49)):.2}'\n\n'6.1e+62'\n\n\n\n1000 / 100\n\n10.0\n\n\n\n6.1e+62 / 1e+60\n\n610.0\n\n\n\nf'There are {6.1e+62 / 1e+60} Novemdecillion possible random maps we could generate.'\n\n'There are 610.0 Novemdecillion possible random maps we could generate.'"
  },
  {
    "objectID": "lectures/week-07/2023-10-04.html#fastest-computer-today",
    "href": "lectures/week-07/2023-10-04.html#fastest-computer-today",
    "title": "Moran’s I",
    "section": "Fastest computer today",
    "text": "Fastest computer today\n2 quintrillion calculations per second\nA quintrillion is \\(10^{18}\\)\n\nseconds = 6.1e+62/1e+18\n\nprint(f'Using the fastest computer we have today, we would need {seconds} seconds to generate all such maps')\n\nUsing the fastest computer we have today, we would need 6.1e+44 seconds to generate all such maps\n\n\n\n13.7e+9 #  years since the big bang\n\n13700000000.0\n\n\n\nhistories = seconds / (13.7e+9 * 60 * 60 * 24 * 365)\n\n\nprint(f'We would need to string together {histories} periods of length since the Big Bang in order to do this!')\n\nWe would need to string together 1.411895847452292e+27 periods of length since the Big Bang in order to do this!"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#introduction",
    "href": "lectures/week-01/01-introduction.html#introduction",
    "title": "Course Introduction",
    "section": "Introduction",
    "text": "Introduction\n\nThis course introduces the fundamental concepts of spatial data analysis. Key fundamentals include spatial sampling, descriptive statistics for areal data, inferential statistics, use of maps in data analysis."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#approach",
    "href": "lectures/week-01/01-introduction.html#approach",
    "title": "Course Introduction",
    "section": "Approach",
    "text": "Approach\n\nThe course takes an explicitly computational thinking approach to its pedagogy. Students are introduced to computational concepts and tools that are increasingly important to research that engages with geospatial data. By adopting these tools, students acquire a deeper engagement with, and mastery of, the substantive concepts."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#scope",
    "href": "lectures/week-01/01-introduction.html#scope",
    "title": "Course Introduction",
    "section": "Scope",
    "text": "Scope\n\nIn the scope of a 15-week semester course we can only introduce a handful of the key concepts and methods relevant to the field of spatial data analysis. As such, the course is not intended as an exhaustive treatment. Instead, the goal is that students will acquire an understanding of the more common and useful methods and practices, and use the course as an entry point for further engagement with the field."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#prerequisites",
    "href": "lectures/week-01/01-introduction.html#prerequisites",
    "title": "Course Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#schedule-reading-and-content",
    "href": "lectures/week-01/01-introduction.html#schedule-reading-and-content",
    "title": "Course Introduction",
    "section": "Schedule, Reading, and Content",
    "text": "Schedule, Reading, and Content\nAll required readings are available through the links listed below. Assigned readings should be completed before the date listed in the schedule (see below). Readings are a critical part of the discussions we will hold in class, and therefore coming into class prepared means having completed the readings and thought about the content. It will be difficult to do well in this course without having completed the readings."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#readings",
    "href": "lectures/week-01/01-introduction.html#readings",
    "title": "Course Introduction",
    "section": "Readings",
    "text": "Readings\n\n\n\nAbbrevation\nSource\n\n\n\n\nGDA\nTenkanen, H., V. Heikinheimo, D. Whipp (2023) Python for Geographic Data Analysis. CRC Press.\n\n\nGDS\nRey, S.J., D. Arribas-Bel, L.J. Wolf (2023) Geographic Data Science with Python. CRC Press."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#schedule-planned",
    "href": "lectures/week-01/01-introduction.html#schedule-planned",
    "title": "Course Introduction",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#schedule-planned-1",
    "href": "lectures/week-01/01-introduction.html#schedule-planned-1",
    "title": "Course Introduction",
    "section": "Schedule (Planned)",
    "text": "Schedule (Planned)"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#grading",
    "href": "lectures/week-01/01-introduction.html#grading",
    "title": "Course Introduction",
    "section": "Grading",
    "text": "Grading\nGEOG385 uses specification grading in evaluating student work and in determining your final course grade. Your course grade will be based on the quality and quantity of the work that you submit that is evaluated to be of an acceptable level of quality. The acceptable level of quality demonstrates competency in the concepts and methods covered in the course."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#specification-grading",
    "href": "lectures/week-01/01-introduction.html#specification-grading",
    "title": "Course Introduction",
    "section": "Specification Grading",
    "text": "Specification Grading\nThere is a two-step process for determination of your final course grade at the end of the quarter:\n\nUsing your quizzes, and exercises, your base grade is determined.\nUsing your final exam results, determine if your base grade includes a \"plus\", \"minus\", or level drop to form the course grade."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#base-grade",
    "href": "lectures/week-01/01-introduction.html#base-grade",
    "title": "Course Introduction",
    "section": "Base Grade",
    "text": "Base Grade\n\n\n\nLevel\nHurdles\n\n\n\n\nA\nPass at least 13 of 15 quizzes and earn \"Demonstrates Competency\" on 4 of 4 exercises,\n\n\nB\nPass at least 11 of 15 quizzes and earn \"Demonstrates Competency\" on 3 of 4 exercises\n\n\nC\nPass at least 9 of 15 quizzes and earn \"Demonstrates Competency\" on 2 of 4 exercises\n\n\nD\nPass at least 7 of 15 quizzes and earn \"Demonstrates Competency\" on 1 of 4 exercises\n\n\nF\nFail to clear D-level hurdles"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#final-grade",
    "href": "lectures/week-01/01-introduction.html#final-grade",
    "title": "Course Introduction",
    "section": "Final Grade",
    "text": "Final Grade\n\nIf you earn at least 85% on the final exam, you will obtain a “+” for your grade. So a B base grade would become a B+ course grade, and so on (Note: SDSU does not record A+ grades).\nIf you score between 70-85% on the final exam, your base grade becomes your course grade.\nIf you score between 50% and 69% on the final exam, you will obtain a “-” for your grade. So an A base grade becomes an A- course grade, a B base grade becomes a B- course grade, and so on.\nIf you score less than 50% on the final exam, your course grade will drop one level: An A base grade becomes a final B course grade."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#quizzes",
    "href": "lectures/week-01/01-introduction.html#quizzes",
    "title": "Course Introduction",
    "section": "Quizzes",
    "text": "Quizzes\nQuizzes are graded on a pass/fail basis. Starting in week two, there will be a quiz due before a session that pertains to the background reading that is required before our work in class."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#exercises",
    "href": "lectures/week-01/01-introduction.html#exercises",
    "title": "Course Introduction",
    "section": "Exercises",
    "text": "Exercises\nFour exercises will be introduced in class and are to be completed outside of class meetings.\nEach exercise is graded using a CRN rubric that classifies work with marks of C (\"Demonstrates Competence\"), R (\"Needs Revision\"), or N (\"Not assessable\"):"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#exercises-1",
    "href": "lectures/week-01/01-introduction.html#exercises-1",
    "title": "Course Introduction",
    "section": "Exercises",
    "text": "Exercises\nOf each exercise the following questions will be asked: Does the work demonstrate that the student understands the concepts? Does the work demonstrate competence and meet the expectations outlined in the exercise?\nIf the answer is \"yes\" to both of the questions, a student passes the hurdle for that exercise."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#exercises-2",
    "href": "lectures/week-01/01-introduction.html#exercises-2",
    "title": "Course Introduction",
    "section": "Exercises",
    "text": "Exercises\nIf the initial submission does not clear the hurdle, then a second question is asked: Is there evidence of partial understanding of the concepts? If the answer to this question is \"Yes\" the student can exchange one token to attempt a revision of their work. If the answer is \"No\", the student does not clear the hurdle for this exercise and will not have the opportunity to revise their work."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#final-exam",
    "href": "lectures/week-01/01-introduction.html#final-exam",
    "title": "Course Introduction",
    "section": "Final Exam",
    "text": "Final Exam\nA closed book, closed note, timed final exam will be given on December 18 (13:00-15:00). The exam will be based on a blend of previous quiz questions and additional questions that pertain to material covered in class."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#tokens",
    "href": "lectures/week-01/01-introduction.html#tokens",
    "title": "Course Introduction",
    "section": "Tokens",
    "text": "Tokens\nEach student is provided with three tokens at the beginning of the semester."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#using-tokens",
    "href": "lectures/week-01/01-introduction.html#using-tokens",
    "title": "Course Introduction",
    "section": "Using Tokens",
    "text": "Using Tokens\n\nOne token can be used for a one-day extension for an exercise.\nOne token can be used to revise an exercise that was submitted on-time but evaluated as \"Needing Revision\".\nTwo tokens can be used to request a make-up date for the final exam."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#remaining-tokens",
    "href": "lectures/week-01/01-introduction.html#remaining-tokens",
    "title": "Course Introduction",
    "section": "Remaining Tokens",
    "text": "Remaining Tokens\nEach token that remains unused after 2023-12-18 will be counted as a passed quiz. Tokens cannot be exchanged with other students."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#administration",
    "href": "lectures/week-01/01-introduction.html#administration",
    "title": "Course Introduction",
    "section": "Administration",
    "text": "Administration"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#accommodations",
    "href": "lectures/week-01/01-introduction.html#accommodations",
    "title": "Course Introduction",
    "section": "Accommodations",
    "text": "Accommodations\nIf you are a student with a disability and are in need of accommodations for this class, please contact Student Ability Success Center at (619) 594-6473 as soon as possible. Please know accommodations are not retroactive, and I cannot provide accommodations based upon disability until I have received an accommodation letter from Student Ability Success Center."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#privacy-and-intellectual-property",
    "href": "lectures/week-01/01-introduction.html#privacy-and-intellectual-property",
    "title": "Course Introduction",
    "section": "Privacy and Intellectual Property",
    "text": "Privacy and Intellectual Property\nStudent Privacy and Intellectual Property: The Family Educational Rights and Privacy Act (FERPA) mandates the protection of student information, including contact information, grades, and graded assignments. I will use [Canvas / Blackboard] to communicate with you, and I will not post grades or leave graded assignments in public places. Students will be notified at the time of an assignment if copies of student work will be retained beyond the end of the semester or used as examples for future students or the wider public. Students maintain intellectual property rights to work products they create as part of this course unless they are formally notified otherwise."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#academic-integrity",
    "href": "lectures/week-01/01-introduction.html#academic-integrity",
    "title": "Course Introduction",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThe SDSU student academic integrity policy lists violations in detail. These violations fall into eight broad areas that include but are not limited to: cheating, fabrication, plagiarism, facilitating academic misconduct, unauthorized collaboration, interference or sabotage, non-compliance with research regulations and retaliation. For more information about the SDSU student academic integrity policy, please see the following: http://www.sa.sdsu.edu/srr/index.html."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#code-of-conduct",
    "href": "lectures/week-01/01-introduction.html#code-of-conduct",
    "title": "Course Introduction",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nAs course instructor, I am dedicated to providing a harassment-free learning experience for all students, regardless of gender, sexual orientation, disability, physical appearance, body size, race, religion, or choice of operating system. All course participants are expected to show respect and courtesy to other students throughout the semester. As a learning community we do not tolerate harassment of participants in any form."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#code-of-conduct-1",
    "href": "lectures/week-01/01-introduction.html#code-of-conduct-1",
    "title": "Course Introduction",
    "section": "Code of Conduct",
    "text": "Code of Conduct\n\nAll communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery are not appropriate in this course.\nBe kind to others. Do not insult or put down other students. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for this course.\nStudents violating these rules may be asked to leave the course, and their violations will be reported to the SDSU administration.\n\nThis code of conduct is an adaptation of the SciPy 2018 Code of Conduct."
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#computational-learning",
    "href": "lectures/week-01/01-introduction.html#computational-learning",
    "title": "Course Introduction",
    "section": "Computational Learning",
    "text": "Computational Learning\n\n\nShow me the code\nimport libpysal.examples\nimport geopandas \n\n# get path to built-in dataset for Mexico\npth = libpysal.examples.get_path(\"mexicojoin.shp\")\n# load the file with geopandas to create a GeoDataframe\ngdf = geopandas.read_file(pth)\n# call the plot method of the GeoDataFrame\ngdf.plot(edgecolor='white');"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#open-source",
    "href": "lectures/week-01/01-introduction.html#open-source",
    "title": "Course Introduction",
    "section": "Open Source",
    "text": "Open Source"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#server-or-laptop",
    "href": "lectures/week-01/01-introduction.html#server-or-laptop",
    "title": "Course Introduction",
    "section": "Server or Laptop",
    "text": "Server or Laptop\nYou can choose to either use an account on our course JupyterHub or install the packages on your own laptop.\nEither way, you will be using Jupyter Notebooks for all computation:"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#my-program",
    "href": "lectures/week-01/01-introduction.html#my-program",
    "title": "Course Introduction",
    "section": "My Program",
    "text": "My Program\n\n\n\nurl"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#why-am-i-here",
    "href": "lectures/week-01/01-introduction.html#why-am-i-here",
    "title": "Course Introduction",
    "section": "Why am I here",
    "text": "Why am I here"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#why-am-i-here-1",
    "href": "lectures/week-01/01-introduction.html#why-am-i-here-1",
    "title": "Course Introduction",
    "section": "Why am I here",
    "text": "Why am I here"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#trump-turned-this-place-into-a-ghost-town",
    "href": "lectures/week-01/01-introduction.html#trump-turned-this-place-into-a-ghost-town",
    "title": "Course Introduction",
    "section": "‘Trump turned this place into a ghost town’",
    "text": "‘Trump turned this place into a ghost town’"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-and-atlantic-city",
    "href": "lectures/week-01/01-introduction.html#stockton-and-atlantic-city",
    "title": "Course Introduction",
    "section": "Stockton and Atlantic City",
    "text": "Stockton and Atlantic City"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-and-atlantic-city-1",
    "href": "lectures/week-01/01-introduction.html#stockton-and-atlantic-city-1",
    "title": "Course Introduction",
    "section": "Stockton and Atlantic City",
    "text": "Stockton and Atlantic City\n\nSource"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton",
    "href": "lectures/week-01/01-introduction.html#stockton",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-1",
    "href": "lectures/week-01/01-introduction.html#stockton-1",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-2",
    "href": "lectures/week-01/01-introduction.html#stockton-2",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#stockton-3",
    "href": "lectures/week-01/01-introduction.html#stockton-3",
    "title": "Course Introduction",
    "section": "Stockton",
    "text": "Stockton"
  },
  {
    "objectID": "lectures/week-01/01-introduction.html#you",
    "href": "lectures/week-01/01-introduction.html#you",
    "title": "Course Introduction",
    "section": "You",
    "text": "You\nTake a few minutes and let us know a bit about yourself\n\nName\nProgram/Concentration\nWhy you are here"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html",
    "href": "lectures/week-10/2023-10-25-point-processes.html",
    "title": "Point Processes",
    "section": "",
    "text": "Thus far we have been looking at a collection of points as a point pattern.\nNow we want to take a different view of that pattern, one that sees the pattern as the outcome of a process.\nA point process is a statistical model that will generate point patterns with particular characteristics.\nFrom a scientific point of view we are interested in making inferences about the process that may have generated our point pattern."
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#introduction",
    "href": "lectures/week-10/2023-10-25-point-processes.html#introduction",
    "title": "Point Processes",
    "section": "",
    "text": "Thus far we have been looking at a collection of points as a point pattern.\nNow we want to take a different view of that pattern, one that sees the pattern as the outcome of a process.\nA point process is a statistical model that will generate point patterns with particular characteristics.\nFrom a scientific point of view we are interested in making inferences about the process that may have generated our point pattern."
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#first-order-properties",
    "href": "lectures/week-10/2023-10-25-point-processes.html#first-order-properties",
    "title": "Point Processes",
    "section": "First Order Properties",
    "text": "First Order Properties\n\nFirst Order Properties: Spatial Analysis\nMean value of the process in space\n\nVariation in mean value of the process in space\nGlobal, large scale spatial trend\n\nFirst Order Property of Point Patterns, Intensity: \\(\\lambda\\)\n\nIntensity: \\(\\lambda\\) = number of events expected per unit area\nEstimation of \\(\\lambda\\)\nSpatial variation of \\(\\lambda\\), \\(\\lambda(s)\\), \\(s\\) is a location\n\n\\[\\lambda(s) = \\lim_{ds\\rightarrow 0}\\left\\{ \\frac{E(Y(ds))}{ds} \\right\\}\\]"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#second-order-property",
    "href": "lectures/week-10/2023-10-25-point-processes.html#second-order-property",
    "title": "Point Processes",
    "section": "Second Order Property",
    "text": "Second Order Property\n\nSecond Order Properties: Spatial Analysis\nSpatial Correlation Structure\n\nDeviations in values from process mean\nLocal or small scale effects\n\nSecond Order Property of Point Patterns\n\nRelationship between number of events in pairs of areas\nSecond order intensity \\(\\gamma(s_i,s_j)\\)\n\n\\[\\gamma(s_i,s_j) = \\lim_{ds_i\\rightarrow 0,ds_j\\rightarrow 0}\\left\\{\n       \\frac{E(Y(ds_i)Y(ds_j))}{ds_ids_j} \\right\\}\\]"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#spatial-stationarity",
    "href": "lectures/week-10/2023-10-25-point-processes.html#spatial-stationarity",
    "title": "Point Processes",
    "section": "Spatial Stationarity",
    "text": "Spatial Stationarity\nFirst Order Stationarity \\[\\lambda(s) = \\lambda \\forall s \\in A\\] \\[E(Y(A)) = \\lambda \\times A\\]\nSecond Order Stationarity \\[\\gamma(s_i,s_j) = \\gamma(s_i - s_j) = \\gamma(h)\\]\n\n\\(h\\) is the vector difference between locations \\(s_i\\) and \\(s_j\\)\n\\(h\\) encompasses direction and distance (relative location)\nSecond order intensity only depends on \\(h\\) for second order stationarity"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#spatial-isotropy-and-stationarity",
    "href": "lectures/week-10/2023-10-25-point-processes.html#spatial-isotropy-and-stationarity",
    "title": "Point Processes",
    "section": "Spatial Isotropy and Stationarity",
    "text": "Spatial Isotropy and Stationarity\nIsotropic Process\n\nWhen a stationary process is invariant to rotation about the origin.\nRelationship between two events depend only on the distance separating their locations and not on their orientation to each other.\nDepends only on distance, not direction\n\nUsefulness\n\nTwo pairs of events from a stationary process separated by same distance and relative direction should have same “relatedness”\nTwo pairs of events from a stationary and isotropic process separated by the same distance (irrespective of direction) should have the same “relatedness”\nBoth allow for replication and the ability to carry out estimation of the underlying DGP."
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#invariance",
    "href": "lectures/week-10/2023-10-25-point-processes.html#invariance",
    "title": "Point Processes",
    "section": "Invariance",
    "text": "Invariance\n\n\n\n\n\n\n\nUnder Translation\n\n\n\n\n\n\n\nUnder Rotation\n\n\n\n\nFigure 1: Invariance"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#complete-spatial-randomness",
    "href": "lectures/week-10/2023-10-25-point-processes.html#complete-spatial-randomness",
    "title": "Point Processes",
    "section": "Complete Spatial Randomness",
    "text": "Complete Spatial Randomness\n\nCSR\n\nStandard of Reference\nUniform: each location has equal probability\nIndependent: location of points independent\nHomogeneous Planar Poisson Point Process\n\n\n\nPoisson Point Process\n\nIntensity\n\nnumber of points in region \\(A: N(A)\\)\nintensity: \\(\\lambda = N/|A|\\)\nimplies: \\(\\lambda |A|\\) points randomly scattered in a region with area \\(|A|\\)\ne.g., \\(10\\times 1\\) (points per \\(km^2\\))\n\n\n\nPoisson Distribution \\(N(A) \\sim Poi(\\lambda |A|)\\)"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#poisson-distribution",
    "href": "lectures/week-10/2023-10-25-point-processes.html#poisson-distribution",
    "title": "Point Processes",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\nSingle Parameter Distribution: \\(\\lambda |A|\\)\n\nGenerally, \\(\\lambda\\) is the number of events in some well defined interval\n\nTime: phone calls to operator in one hour\nTime: accidents at an intersection per week\nSpace: trees in a quadrat\n\nLet \\(x\\) be a Poisson random variable\n\n\\(E[x] = V[x]= \\lambda |A|\\)\n\n\n\n\nPoisson Distribution \\[P(x) =  \\frac{e^{-\\lambda |A|} (\\lambda |A|)^x}{x!}\\]"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#spatial-example",
    "href": "lectures/week-10/2023-10-25-point-processes.html#spatial-example",
    "title": "Point Processes",
    "section": "Spatial Example",
    "text": "Spatial Example\n\nCSR with \\(\\lambda = 5/km^2\\)\n\nRegion = Circle\n\narea = \\(|A| = \\pi r^2\\)\n\\(r=0.1\\ km\\) then area \\(\\approx 0.03 \\ km^2\\)\n\nProbability of Zero Points in Circle \\[\\begin{aligned}\n         P[N(A) = 0] &= &  e^{-\\lambda |A|} (\\lambda |A|)^x /x!\\\\\n                     &\\approx&e^{-5 \\times 0.03} (5 \\times 0.03)^0 /0!\\\\\n                     &\\approx&e^{-5 \\times 0.03} \\\\\n                     &\\approx&0.86\n       \\end{aligned}\\]"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#complete-spatial-randomness-csr",
    "href": "lectures/week-10/2023-10-25-point-processes.html#complete-spatial-randomness-csr",
    "title": "Point Processes",
    "section": "Complete Spatial Randomness (CSR)",
    "text": "Complete Spatial Randomness (CSR)\n\nHomogeneous spatial Poisson point process\n\nThe number of events occurring within a finite region \\(A\\) is a random variable following a Poisson distribution with mean \\(\\lambda|A|\\), with \\(|A|\\) denoting area of \\(A\\).\nGiven the total number of events \\(N\\) occurring within an area \\(A\\), the locations of the \\(N\\) events represent an independent random sample of \\(N\\) locations where each location is equally likely to be chosen as an event.\n\n\n\n\nCriterion 2 is the general concept of CSR (uniform (random)) distribution in \\(A\\).\nCriterion 1 pertains to the intensity \\(\\lambda\\)."
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#homogeneous-poisson-process",
    "href": "lectures/week-10/2023-10-25-point-processes.html#homogeneous-poisson-process",
    "title": "Point Processes",
    "section": "Homogeneous Poisson Process",
    "text": "Homogeneous Poisson Process\n\nImplications\n\nThe number of events in nonoverlapping regions in \\(A\\) are statistically independent.\nFor any region \\(R \\subset A\\): \\[\\lim_{|R| \\rightarrow 0} \\frac{Pr[exactly\\ one\\ event\\ in\\ R]}{|R|}\n      = \\lambda &gt; 0\\]\n\\[\\lim_{|R| \\rightarrow 0} \\frac{Pr[more\\ than\\ one\\ event\\ in\\\n       R]}{|R|} = 0\\]\n\n\n:::"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#homogeneous-poisson-process-1",
    "href": "lectures/week-10/2023-10-25-point-processes.html#homogeneous-poisson-process-1",
    "title": "Point Processes",
    "section": "Homogeneous Poisson process",
    "text": "Homogeneous Poisson process\n\nImplications\n\n\\(\\lambda\\) is the intensity of the spatial point pattern.\nFor a Poisson random variable, \\(Y\\): \\[E[Y] = \\lambda = V[Y]\\]\nProvides the motivation for some quadrat tests of CSR hypothesis.\n\nIf \\(Y_R\\) is the count in quadrat \\(R\\)\nIf \\(\\widehat{E[Y]}&lt; \\widehat{V[Y]}\\): overdispersion = spatial clustering\nIf \\(\\widehat{E[Y]}&gt; \\widehat{V[Y]}\\): underdispersion = spatial uniformity"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#poisson-distribution-lambda20",
    "href": "lectures/week-10/2023-10-25-point-processes.html#poisson-distribution-lambda20",
    "title": "Point Processes",
    "section": "Poisson Distribution \\(\\lambda=20\\)",
    "text": "Poisson Distribution \\(\\lambda=20\\)\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nnp.random.seed(12345)\nxy = np.random.rand(20,2)\ndf = pd.DataFrame(data=xy, columns=['x','y'])\nsns.scatterplot(x='x', y='y', data=df);\ndf.shape\n\n(20, 2)\n\n\n\n\n\nThe example we just did is known as \\(n-conditioning\\) where we will always get \\(n\\) points for the CSR process.\nA slightly different approach to generating a random point process is to use \\(\\lambda-conditioning\\)\n\nfrom scipy.stats import poisson\nlam=20\nn = poisson.rvs(lam, 1)\nxy = np.random.rand(n,2)\ndf = pd.DataFrame(data=xy, columns=['x','y'])\nsns.scatterplot(x='x', y='y', data=df);\ndf.shape\n\n(26, 2)\n\n\n\n\n\nThe difference is the number of points in the pattern will always be \\(n\\) with \\(n-conditioning\\) but may not be \\(n\\) with \\(\\lambda-conditioning\\). The latter allows the intensity to be drawn from a Poisson distribution, then that becomes the parameter for the draw of the point pattern."
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#limitations-of-csr",
    "href": "lectures/week-10/2023-10-25-point-processes.html#limitations-of-csr",
    "title": "Point Processes",
    "section": "Limitations of CSR",
    "text": "Limitations of CSR\n\nStationary Poisson Process\n\nhomogeneous\ntranslation invaratiant\n\n\n\nRare in practice - very few actual processes are CSR\n\n\nStrawman\n\npurely a benchmark\nnull hypothesis"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#inhomogeneous-poisson-process-ipp",
    "href": "lectures/week-10/2023-10-25-point-processes.html#inhomogeneous-poisson-process-ipp",
    "title": "Point Processes",
    "section": "Inhomogeneous Poisson Process (IPP)",
    "text": "Inhomogeneous Poisson Process (IPP)\n\nCriteria\n\nThe number of events occurring within a finite region \\(A\\) is a random variable following a Poisson Distribution with mean \\(\\int_{A}\\lambda(s) ds\\).\nGiven the total number of events \\(N\\) occurring within \\(A\\), the \\(N\\) events represent an independent sample of \\(N\\) locations, with the probability of sampling a particular point \\(s\\) proportional to \\(\\lambda(s)\\).\n\n\n\nSpatially Variable Intensity \\(\\lambda(s)\\)\n\nUseful for constant risk hypothesis\nUnderlying population at risk is spatially clustered\nWant to control for that since with individual constant risk apparent clusters would be generated.\nCompare pattern against constant risk, not CSR."
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#inhomogeneous-poisson-process",
    "href": "lectures/week-10/2023-10-25-point-processes.html#inhomogeneous-poisson-process",
    "title": "Point Processes",
    "section": "Inhomogeneous Poisson Process",
    "text": "Inhomogeneous Poisson Process\n\nImplications\n\nApparent clusters can occur solely due to heterogeneities in the intensity function \\(\\lambda(s)\\).\nIndividual event locations still remain independent of one another.\nProcess is not stationary due to intensity heterogeneity\n\n\n\nHPP vs. IPP HPP is a special case of IPP with a constant intensity"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#csr-vs.-constant-risk-hypotheses",
    "href": "lectures/week-10/2023-10-25-point-processes.html#csr-vs.-constant-risk-hypotheses",
    "title": "Point Processes",
    "section": "CSR vs. Constant Risk Hypotheses",
    "text": "CSR vs. Constant Risk Hypotheses\n\nCSR\n\nIntensity is spatially constant\nPopulation at risk assumed spatially uniform\nUseful null hypothesis if these conditions are met\n\n\n\nConstant Risk Hypothesis\n\nPopulation density variable\nIndividual risk constant\nExpected number of events should vary with population density\nClusters due to deviation from CSR\nClusters due to deviation from CSR and Constant Risk"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#contagion-process-of-size-20-with-10-parents-and-2-children",
    "href": "lectures/week-10/2023-10-25-point-processes.html#contagion-process-of-size-20-with-10-parents-and-2-children",
    "title": "Point Processes",
    "section": "Contagion process of size 20 with 10 parents and 2 children",
    "text": "Contagion process of size 20 with 10 parents and 2 children\n\nimport pointpats as pp\nnp.random.seed(12345)\nw = pp.Window([(0,0), (0,1), (1,1), (1,0), (0,0)])\ndraw = pp.PoissonClusterPointProcess(w, 20, 10, 0.05, 1, asPP=True, conditioning=False)\ndraw.realizations[0].plot(window=True, title='Contagion Point Process (10 parents)')"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#contagion-process-of-size-20-with-2-parents-and-10-children",
    "href": "lectures/week-10/2023-10-25-point-processes.html#contagion-process-of-size-20-with-2-parents-and-10-children",
    "title": "Point Processes",
    "section": "Contagion process of size 20 with 2 parents and 10 children",
    "text": "Contagion process of size 20 with 2 parents and 10 children\n\nimport pointpats as pp\nnp.random.seed(12345)\nw = pp.Window([(0,0), (0,1), (1,1), (1,0), (0,0)])\ndraw = pp.PoissonClusterPointProcess(w, 20, 2, 0.05, 1, asPP=True, conditioning=False)\ndraw.realizations[0].plot(window=True, title='Contagion Point Process (2 parents)')"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#inhomogenous-poisson-process",
    "href": "lectures/week-10/2023-10-25-point-processes.html#inhomogenous-poisson-process",
    "title": "Point Processes",
    "section": "Inhomogenous Poisson Process",
    "text": "Inhomogenous Poisson Process\n\nIntensity varies with a covariate\n\ntrend surface\n\\(\\lambda(s) = exp(\\alpha + \\beta s)\\)\n\n\n\nIntensity varies with distance to a focus\n\n\\(\\lambda(s) = \\lambda 0(s). f( || s-s_0||, \\theta)\\)"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#simulating-an-inhomogeneous-poisson-point-process",
    "href": "lectures/week-10/2023-10-25-point-processes.html#simulating-an-inhomogeneous-poisson-point-process",
    "title": "Point Processes",
    "section": "Simulating An Inhomogeneous Poisson Point Process",
    "text": "Simulating An Inhomogeneous Poisson Point Process\nIntensity function:\n\\(\\lambda(s) = 100 e^{-(x^2 + y^2) / \\sigma}\\)\n\\(\\sigma\\) is a scale parameter here, equal to 0.5\n\n\nCode\nimport numpy as np;  # NumPy package for arrays, random number generation, etc\nimport matplotlib.pyplot as plt  # For plotting\nfrom scipy.optimize import minimize  # For optimizing\nfrom scipy import integrate  # For integrating\n\nplt.close('all');  # close all figures\n\n# Simulation window parameters\nxMin = 0;\nxMax = 1;\nyMin = 0;\nyMax = 1;\nxDelta = xMax - xMin;\nyDelta = yMax - yMin;  # rectangle dimensions\nareaTotal = xDelta * yDelta;\n\nnumbSim = 10 ** 3;  # number of simulations\ns = 0.5;  # scale parameter\n# Point process parameters\ndef fun_lambda(x, y):\n    return 100 * np.exp(-(x ** 2 + y ** 2) / s ** 2);  # intensity function\n#fun_lambda = lambda x,y: 100 * np.exp(-(x ** 2 + y ** 2) / s ** 2);\n\n###START -- find maximum lambda -- START ###\n# For an intensity function lambda, given by function fun_lambda,\n# finds the maximum of lambda in a rectangular region given by\n# [xMin,xMax,yMin,yMax].\ndef fun_Neg(x):\n    return -fun_lambda(x[0], x[1]);  # negative of lambda\n#fun_Neg = lambda x: -fun_lambda(x[0], x[1]);  # negative of lambda\n\nxy0 = [(xMin + xMax) / 2, (yMin + yMax) / 2];  # initial value(ie centre)\n# Find largest lambda value\nresultsOpt = minimize(fun_Neg, xy0, bounds=((xMin, xMax), (yMin, yMax)));\nlambdaNegMin = resultsOpt.fun;  # retrieve minimum value found by minimize\nlambdaMax = -lambdaNegMin;\n\n\n###END -- find maximum lambda -- END ###\n\n# define thinning probability function\ndef fun_p(x, y):\n    return fun_lambda(x, y) / lambdaMax;\n#fun_p = lambda x, y: fun_lambda(x, y) / lambdaMax;\n\n# for collecting statistics -- set numbSim=1 for one simulation\nnumbPointsRetained = np.zeros(numbSim);  # vector to record number of points\nfor ii in range(numbSim):\n    # Simulate a Poisson point process\n    numbPoints = np.random.poisson(areaTotal * lambdaMax);  # Poisson number of points\n    xx = np.random.uniform(0, xDelta, ((numbPoints, 1))) + xMin;  # x coordinates of Poisson points\n    yy = np.random.uniform(0, yDelta, ((numbPoints, 1))) + yMin;  # y coordinates of Poisson points\n\n    # calculate spatially-dependent thinning probabilities\n    p = fun_p(xx, yy);\n\n    # Generate Bernoulli variables (ie coin flips) for thinning\n    booleRetained = np.random.uniform(0, 1, ((numbPoints, 1))) &lt; p;  # points to be retained\n\n    # x/y locations of retained points\n    xxRetained = xx[booleRetained];\n    yyRetained = yy[booleRetained];\n    numbPointsRetained[ii] = xxRetained.size;\n\n# Plotting\nplt.scatter(xxRetained, yyRetained, edgecolor='b', facecolor='none', alpha=0.5);\nplt.xlabel('x');\nplt.ylabel('y');\nplt.xlim([xMin, xMax]);\nplt.ylim([xMin, xMax]);\n\n\n\n\n\nsource\nThat pattern comes from a spatially-explicit thinning of a CSR pattern:\n\n\nCode\n# Plotting\nplt.scatter(xx, yy, edgecolor='b', facecolor='none', alpha=0.5);\nplt.xlabel('x');\nplt.ylabel('y');"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#regular-processes",
    "href": "lectures/week-10/2023-10-25-point-processes.html#regular-processes",
    "title": "Point Processes",
    "section": "Regular Processes",
    "text": "Regular Processes\n\nLess grouped than CSR\n\nfewer high densities\ndispersed\nrepulsion, competition\n\n\n\nUnderdispersion\n\nvariance &lt; mean\nless variation in densities than CSR"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#inhibition-process",
    "href": "lectures/week-10/2023-10-25-point-processes.html#inhibition-process",
    "title": "Point Processes",
    "section": "Inhibition Process",
    "text": "Inhibition Process\n\nMinimum Permissible Distance\n\nno two points closer than \\(\\delta\\)\npacking intensity \\(\\tau = \\lambda \\pi \\delta^2 / 4\\)\n\n\n\nMatern Process\n\nthinned Poisson process using \\(\\delta\\)\nsequential inhibition process, generate points conditional on previous points and distance (denser than the thinned approach)"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#matern-thinning",
    "href": "lectures/week-10/2023-10-25-point-processes.html#matern-thinning",
    "title": "Point Processes",
    "section": "Matern (Thinning)",
    "text": "Matern (Thinning)\n\nnp.random.seed(12345)\ndelta = 0.1\nn = 20\nxy = np.random.random((n,2))\nxy\nfrom scipy.spatial import distance_matrix\n\nd = distance_matrix(xy, xy) # 20 x 20 distance matrix\nd[0] # first row\n\narray([0.        , 0.75403388, 0.4570564 , 0.33860475, 0.38256491,\n       0.67009276, 0.94484483, 0.716711  , 0.56856568, 0.40881349,\n       0.49326812, 0.46210886, 0.64101492, 0.36620498, 0.20105384,\n       1.02432357, 0.29284635, 0.4855703 , 0.42540863, 0.41333518])\n\n\nDetermine which observations to thin\n\nijs = np.where(d&lt;delta)\ni,j = ijs\npairs = list(zip(i[i!=j], j[i!=j]))\nprint(\"The pairs within delta of one another:\")\nprint(pairs)\ndrop = []\n\nfor left, right in pairs:\n    if left in drop or right in drop:\n        continue\n    else:\n        drop.append(left)\n        \nprint(\"Observations to drop:\")\nprint(drop)\n\nThe pairs within delta of one another:\n[(3, 9), (3, 13), (9, 3), (9, 13), (9, 19), (13, 3), (13, 9), (19, 9)]\nObservations to drop:\n[3, 9]\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\ndf['thin'] = False\ndf.iloc[drop, df.columns.get_loc('thin')] = True\ndf.head()\n\n\n\n\n\n\n\n\nx\ny\nthin\n\n\n\n\n0\n0.929616\n0.316376\nFalse\n\n\n1\n0.183919\n0.204560\nFalse\n\n\n2\n0.567725\n0.595545\nFalse\n\n\n3\n0.964515\n0.653177\nTrue\n\n\n4\n0.748907\n0.653570\nFalse\n\n\n\n\n\n\n\n\nimport seaborn as sns\nsns.scatterplot(x='x', y='y', hue='thin', data=df);"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#matern-sequential",
    "href": "lectures/week-10/2023-10-25-point-processes.html#matern-sequential",
    "title": "Point Processes",
    "section": "Matern (Sequential)",
    "text": "Matern (Sequential)\n\ndelta = 0.1\nN = 20\nn = 1\nxy = np.zeros((N,2))\nxy[0,:] = np.random.rand(1,2)\nwhile n &lt; N:\n    candidate = np.random.rand(1,2)\n    d = distance_matrix(xy[:n,:], candidate)\n    if d.min() &gt; delta:\n        xy[n,:] = candidate\n        n += 1\n\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\nsns.scatterplot(x='x', y='y', data=df);"
  },
  {
    "objectID": "lectures/week-10/2023-10-25-point-processes.html#csr-n20",
    "href": "lectures/week-10/2023-10-25-point-processes.html#csr-n20",
    "title": "Point Processes",
    "section": "CSR n=20",
    "text": "CSR n=20\n\ndelta = 0.1\nxy = np.random.rand(20,2)\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\nsns.scatterplot(x='x', y='y', data=df);"
  },
  {
    "objectID": "lectures/week-12/distance_statistics-numpy-oriented.html",
    "href": "lectures/week-12/distance_statistics-numpy-oriented.html",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "",
    "text": "Authors: Serge Rey sjsrey@gmail.com and Wei Kang weikang9009@gmail.com"
  },
  {
    "objectID": "lectures/week-12/distance_statistics-numpy-oriented.html#introduction",
    "href": "lectures/week-12/distance_statistics-numpy-oriented.html#introduction",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "Introduction",
    "text": "Introduction\nDistance based methods for point patterns are of three types:\n\nMean Nearest Neighbor Distance Statistics\nNearest Neighbor Distance Functions\nInterevent Distance Functions\n\nIn addition, we are going to introduce a computational technique Simulation Envelopes to aid in making inferences about the data generating process. An example is used to demonstrate how to use and interprete simulation envelopes.\n\nfrom scipy import spatial\nimport libpysal as ps\nimport numpy as np\nfrom pointpats import ripley\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nImportError: cannot import name 'ripley' from 'pointpats' (/home/serge/miniforge3/envs/385f23/lib/python3.11/site-packages/pointpats/__init__.py)"
  },
  {
    "objectID": "lectures/week-12/distance_statistics-numpy-oriented.html#mean-nearest-neighbor-distance-statistics",
    "href": "lectures/week-12/distance_statistics-numpy-oriented.html#mean-nearest-neighbor-distance-statistics",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "Mean Nearest Neighbor Distance Statistics",
    "text": "Mean Nearest Neighbor Distance Statistics\nThe nearest neighbor(s) for a point \\(u\\) is the point(s) \\(N(u)\\) which meet the condition \\[d_{u,N(u)} \\leq d_{u,j} \\forall j \\in S - u\\]\nThe distance between the nearest neighbor(s) \\(N(u)\\) and the point \\(u\\) is nearest neighbor distance for \\(u\\). After searching for nearest neighbor(s) for all the points and calculating the corresponding distances, we are able to calculate mean nearest neighbor distance by averaging these distances.\nIt was demonstrated by Clark and Evans(1954) that mean nearest neighbor distance statistics distribution is a normal distribution under null hypothesis (underlying spatial process is CSR). We can utilize the test statistics to determine whether the point pattern is the outcome of CSR. If not, is it the outcome of cluster or regular spatial process?\n\npoints = np.array([[66.22, 32.54], [22.52, 22.39], [31.01, 81.21],\n                   [9.47, 31.02],  [30.78, 60.10], [75.21, 58.93],\n                   [79.26,  7.68], [8.23, 39.93],  [98.73, 77.17],\n                   [89.78, 42.53], [65.19, 92.08], [54.46, 8.48]])"
  },
  {
    "objectID": "lectures/week-12/distance_statistics-numpy-oriented.html#nearest-neighbor-distance-functions",
    "href": "lectures/week-12/distance_statistics-numpy-oriented.html#nearest-neighbor-distance-functions",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "Nearest Neighbor Distance Functions",
    "text": "Nearest Neighbor Distance Functions\nNearest neighbour distance distribution functions (including the nearest “event-to-event” and “point-event” distance distribution functions) of a point process are cumulative distribution functions of several kinds – \\(G, F, J\\). By comparing the distance function of the observed point pattern with that of the point pattern from a CSR process, we are able to infer whether the underlying spatial process of the observed point pattern is CSR or not for a given confidence level.\n\n\\(G\\) function - event-to-event\nThe \\(G\\) function is a kind of “cumulative” density describing the distribution of distances within a point pattern. For a given distance \\(d\\), \\(G(d)\\) is the proportion of nearest neighbor distances that are less than \\(d\\). To express this, we first need to define the nearest neighbor distance, which is the smallest distance from each observation \\(i\\) to some other observation \\(j\\), where \\(j \\neq i\\): \\[ min_{j\\neq i}\\{d_{ij}\\} = d^*_i \\]\nWith this, we can define the \\(G\\) function as a cumulative density function: \\[G(d) = \\frac{1}{N}\\sum_{i=1}^N \\mathcal{I}(d^*_i &lt; d)\\] where \\(\\mathcal{I}(.)\\) is an indicator function that is \\(1\\) when the argument is true and is zero otherwise. In simple terms, \\(G(d)\\) gives the percentage of of nearest neighbor distances (\\(d^*_i\\)) that are smaller than \\(d\\); when \\(d\\) is very small, \\(G(d)\\) is close to zero. When \\(d\\) is large, \\(G(d)\\) approaches one.\nAnalytical results about \\(G\\) are available assuming that the “null” process of locating points in the study area is completely spatially random. In a completely spatially random process, the \\(G(d)\\) value should be: \\[\nG(d) = 1-e^{-\\lambda \\pi d^2}\n\\] Practically, we assess statistical significance for the \\(G(d)\\) function using simulations, where a known spatially-random process is generated and then analyzed. This partially accounts for issues with irregularly-shaped study areas, where locations of points are constrained.\nIn practice, we use the ripley.g_test function to conduct a test on the \\(G(d)\\). It estimates a value of \\(G(d)\\) for a set of values (called the support). To compute the \\(G\\) function for ten values of \\(d\\) ranging from the smallest possible to the largest values in the data:\n\ng_test = ripley.g_test(points, support=10)\n\nAll statistical tests in the pointpats.distance_statistics return a collections.namedtuple object with the following properties: - support, which contains the distance values (\\(d\\)) used to compute the distance statistic. - statistic, which expresses the value of the requested function at each value of \\(d\\) in the support. - pvalue, which expresses the fraction of observed simulations (under a completely spatially random process) that are more extreme than the observed statistics. - simulations, which stores the simulated values of the statistic under a spatially random process. Generally, this is not saved (for efficiency reasons), but can be requested using keep_simulations.\n\ng_test.support\n\narray([ 0.        ,  3.84791574,  7.69583148, 11.54374723, 15.39166297,\n       19.23957871, 23.08749445, 26.93541019, 30.78332593, 34.63124168])\n\n\n\ng_test.statistic\n\narray([0.        , 0.        , 0.        , 0.16666667, 0.16666667,\n       0.25      , 0.58333333, 0.83333333, 0.91666667, 1.        ])\n\n\n\ng_test.pvalue\n\narray([0.00e+00, 0.00e+00, 0.00e+00, 2.89e-02, 1.10e-03, 1.00e-04,\n       4.30e-03, 6.10e-02, 7.33e-02, 0.00e+00])\n\n\n\ng_test.simulations\n\nTo make a plot of the statistic, the statistic is generally plotted on the vertical axis and the support on the horizontal axis. Here, we will show the median simulated value of \\(G(d)\\) as well.\n\ng_test = ripley.g_test(points, support=10, keep_simulations=True)\n\n\nplt.plot(g_test.support, np.median(g_test.simulations, axis=0), \n         color='k', label='simulated')\nplt.plot(g_test.support, g_test.statistic, \n         marker='x', color='orangered', label='observed')\nplt.legend()\nplt.xlabel('Distance')\nplt.ylabel('G Function')\nplt.title('G Function Plot')\nplt.show()\n\n\n\n\nAs you can see, the \\(G\\) function increases very slowly at small distances and the line is below the typical simulated value (shown in black). We can verify the visual intuition here by looking at the p-value for each point and plotting the simulated \\(G(d)\\) curves, too:\n\n# grab the middle 95% of simulations using numpy:\nmiddle_95pct = np.percentile(g_test.simulations, q=(2.5, 97.5), axis=0)\n# use the fill_between function to color between the 2.5% and 97.5% envelope\nplt.fill_between(g_test.support, *middle_95pct, \n                 color='lightgrey', label='simulated')\n\n# plot the line for the observed value of G(d)\nplt.plot(g_test.support, g_test.statistic, \n         color='orangered', label='observed')\n# and plot the support points depending on whether their p-value is smaller than .05\nplt.scatter(g_test.support, g_test.statistic, \n            cmap='viridis', c=g_test.pvalue &lt; .01)\nplt.legend()\nplt.xlabel('Distance')\nplt.ylabel('G Function')\nplt.title('G Function Plot')\nplt.show()\n\n\n\n\nFrom this, we can see that there is statistically significant “dispersion” at small values of \\(d\\), since there are too few nearest neighbor distances observed between \\(0 &lt; d &lt; 25\\). Once we get to very large distances, the simulation envelope covers the observed statistic. As such, we can say that the point pattern recorded in points is unusally dispersed.\nTo evaluate the \\(G(d)\\) function without considering any statistical significance or simulations, you can use the g_function in the ripley module, which simply returns the distances & values of \\(G(d)\\).\n\nripley.g_function(points)\n\n(array([ 0.        ,  1.82269693,  3.64539386,  5.46809079,  7.29078772,\n         9.11348465, 10.93618158, 12.75887851, 14.58157544, 16.40427237,\n        18.2269693 , 20.04966623, 21.87236316, 23.69506009, 25.51775702,\n        27.34045395, 29.16315088, 30.98584782, 32.80854475, 34.63124168]),\n array([0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.25      ,\n        0.25      , 0.25      , 0.41666667, 0.58333333, 0.75      ,\n        0.83333333, 0.83333333, 0.91666667, 0.91666667, 1.        ]))\n\n\n\n\n\\(F\\) function - “point-event”\nWhen the number of events in a point pattern is small, \\(G\\) function is rough. For the pattern contained in points, there are only 12 observations! This means that there are only 12 nearest neighbor distances, and thus only 12 possible values for the \\(G(d)\\) statistic, at any \\(d\\).\nOne way to get around this is to turn to an alternative, the \\(F(d)\\) function. This is analogous to the \\(G(d)\\) function, but measures the nearest neighbor distance from a set of known randomly-distributed points to a point in the observed pattern. Another way of thinking about \\(F(d)\\) is that it reflects a between-pattern measure of dispersion, where one pattern is completely spatially random and the other pattern is our observed pattern. In contrast, \\(G(d)\\) is a within-pattern measure of dispersion.\nFor a randomly simulated point pattern of size \\(N_s\\), this makes the \\(F(d)\\) function:\n\\[F(d) = \\frac{1}{N_s} \\sum_k^{N_s} \\mathcal{I}(d^*_k &lt; d)\\]\nThis can have \\(N_s\\) possible values for any \\(d\\), and thus can give a much more fine-grained view of the point pattern. In this sense, the \\(F(d)\\) function is often called the empty space function, as it measures the distance from random points in “empty space” to the “filled” points in our point pattern. The number of those random points governs how “fine-grained” our measure of the observed point pattern can be.\nJust like the ripley.g_test, this function is evaluated for every \\(d\\) in a support. Further, we can provide custom values for support, just in case we have known distance values of interest.\nBelow, we’ll use the same ten support values from \\(G(d)\\) function. And, let’s constrain the “simulated” point patterns to fall within the convex hull of our original point pattern:\n\nf_test = ripley.f_test(points, support = g_test.support, keep_simulations=True, hull='convex')\n\nSince the \\(F(d)\\) function is very smooth, we can see the \\(F(d)\\) statistic and its simulations clearly by plotting their values directly as lines. For the simulated values, we will make them very transparent. As before we will visualize statistical significance using the pvalue attribute:\n\nplt.plot(f_test.support, f_test.simulations.T, alpha=.01, color='k')\nplt.plot(f_test.support, f_test.statistic, color='red')\n\nplt.scatter(f_test.support, f_test.statistic, \n            cmap='viridis', c=f_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('F Function')\nplt.title('F Function Plot')\nplt.show()\n\nplt.show()\n\n\n\n\nFrom this we see that the values of the \\(F\\) function are too high for distances from about 15 to 25, and (in contrast) for values between \\(5 &lt; d &lt; 10\\), the \\(F(d)\\) function has too few short distances. When the observed \\(F(d)\\) values are too large, then the pattern is too dispersed, or regular. If the empirical \\(F(d)\\) tends to fall below the simulated values, then it reflects clustering. This is the opposite of the interpretation of the \\(G(d)\\) function above, so be careful!\n\n\n\\(J\\) function - a combination of “event-event” and “point-event”\nThe \\(J\\) function combines the \\(G\\) and \\(F\\) function, in an attempt to provide an immediate graphical indication of the clustering both internally and with respect to the empty space distribution. Practically, the \\(J(d)\\) function is computed as a kind of “relative clustering ratio”:\n\\[J(d) = \\frac{1-G(d)}{1-F(d)}\\]\nwhere the numerator captures the clustering due to within-pattern distances and the denominator captures that for the pattern-to-empty distances. This means that when \\(J(d)&lt;1\\), the underlying point process is a cluster point process, and when \\(J(d)=1\\), the underlying point process is a random point process; otherwise, it is a dispersed point process.\nThis function can suffer from numerical stability issues; as \\(G(d)\\) and \\(F(d)\\) both approach \\(1\\), the \\(J\\) ratio can become chaotic. Further, when \\(G\\) or \\(F\\) reaches one, the \\(J\\) function changes abruptly. As such, the \\(J\\) function is often truncated to the first \\(1\\) (either in \\(F(d)\\) or \\(G(d)\\)), and any \\(d\\) where both \\(F\\) and \\(G\\) are \\(1\\) is assigned a \\(J\\) value of \\(1\\).\n\njp1 = ripley.j_test(points, support=20)\n\n/home/lw17329/Dropbox/dev/pointpats/pointpats/ripley.py:894: UserWarning: requested 20 bins to evaluate the J function, but it reaches infinity at d=25.5178, meaning only 14 bins will be used to characterize the J function.\n  tree, distances=distances, **core_kwargs\n\n\nAs you can see from the warning above, the \\(J\\) function did encounter numerical stability issues at about \\(d=25\\). To address this, pointpats truncated the \\(J\\) function to only have 14 values in its support, rather than the \\(20\\) requested.\n\nplt.plot(jp1.support, jp1.statistic, color='orangered')\nplt.axhline(1, linestyle=':', color='k')\nplt.xlabel('Distance')\nplt.ylabel('J Function')\n\nText(0, 0.5, 'J Function')\n\n\n\n\n\nFrom the above figure, we see that the \\(J\\) function is above the \\(J(d)=1\\) horizontal line, especially as \\(d\\) gets large. This suggests that the process is over-dispersed."
  },
  {
    "objectID": "lectures/week-12/distance_statistics-numpy-oriented.html#interevent-distance-functions",
    "href": "lectures/week-12/distance_statistics-numpy-oriented.html#interevent-distance-functions",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "Interevent Distance Functions",
    "text": "Interevent Distance Functions\nWhile both the \\(F(d)\\) and \\(G(d)\\) functions are useful, they only consider the distance between each point \\(i\\) and its nearest point. Earlier we spelled this distance \\(d_i^*\\), and the distance between \\(i\\) and \\(j\\) was \\(d_{ij}\\). So, note that \\(d_{i}^*\\) is the only term that matters for \\(F\\) and \\(G\\), if \\(d_{ij}\\) changes (but \\(j\\) isn’t closest to \\(i\\)), then the \\(F\\) and \\(G\\) functions generally remain the same.\nSo, further statistical summary functions have been developed to consider the whole distance distribution, not only the nearest neighbor distances. These functions (still considered part of the “Ripley” alphabet, are the \\(K\\), and \\(L\\) functions.\n\n\\(K\\) function\nThe \\(K(d)\\) function is a scaled version of the cumulative density function for all distances within a point pattern. As such, it’s a “relative” of the \\(G\\) function that considers all distances, not just the nearest neighbor distances. Practically, the \\(K(d)\\) function can be thought of as the percentage of all distances that are less than \\(d\\). Therefore, for a threshold distance \\(d\\), the \\(K\\) function is defined as:\n\\[K(d) = \\frac{1}{N\\hat\\lambda} \\underset{i=1}{\\overset{N}{\\sum}}\\underset{j=1}{\\overset{N}{\\sum}} \\mathcal{I}\\left(d_ij &lt; d\\right)\\]\nIn this equation, \\(\\hat\\lambda\\) is the intensity of the point process. This represents how many points (on average) you would expect in a unit area. You can think of this as an analogue to the density of the points in the pattern: large values of \\(\\hat\\lambda\\) mean many points per area, and small values of \\(\\hat\\lambda\\) mean there are fewer points per area. Generally, this parameter is unknown, and is modelled using the average number of points in the study area. This assumes that the intensity of the point pattern is constant or homogeneous over the study area.\nIn the same manner as before, we can construct a set of \\(K(d)\\) function evaluations for random point patterns, and compare them to the observed \\(K(d)\\) function we saw in our original data.\n\nk_test = ripley.k_test(points, keep_simulations=True)\n\n\nplt.plot(k_test.support, k_test.simulations.T, color='k', alpha=.01)\nplt.plot(k_test.support, k_test.statistic, color='orangered')\n\nplt.scatter(k_test.support, k_test.statistic, \n            cmap='viridis', c=k_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('K Function')\nplt.title('K Function Plot')\nplt.show()\n\n\n\n\nAgain, we can see that the envelopes are generally above the observed function, meaining that our point pattern is dispersed. We can draw this conclusion because the distances are too small, suggesting the pattern is less clustered than otherwise woudl be expected. When points are too regular, their distances tend to be smaller than if they were distributed randomly.\n\n\n\\(L\\) function - “interevent”\nThe \\(L\\) function is a scaled version of \\(K\\) function, defined in order to assist with interpretation. The expected value of the \\(K(d)\\) function increases with \\(d\\); this makes sense, since the number of pairs of points closer than \\(d\\) will increase as \\(d\\) increases. So, we can define a normalization of \\(K\\) that removes this increase as \\(d\\) increases.\n\\[L(d) = \\sqrt{\\frac{K(d)}{\\pi}}-d\\]\nFor a pattern that is spatially random, \\(L(d)\\) is \\(0\\) at all \\(d\\) values. So, we can use this standardization to make it easier to visualize the results of the \\(K\\) function:\n\nl_test = ripley.l_test(points, keep_simulations=True)\n\n\nplt.plot(l_test.support, l_test.simulations.T, color='k', alpha=.01)\nplt.plot(l_test.support, l_test.statistic, color='orangered')\n\nplt.scatter(l_test.support, l_test.statistic, \n            cmap='viridis', c=l_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('K Function')\nplt.title('K Function Plot')\nplt.show()"
  },
  {
    "objectID": "lectures/week-12/distance_statistics-numpy-oriented.html#csr-example",
    "href": "lectures/week-12/distance_statistics-numpy-oriented.html#csr-example",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "CSR Example",
    "text": "CSR Example\nIn this example, we are going to generate a point pattern as the “observed” point pattern. This ensures that the data generating process is completely spatially random. Then, we will simulate CSR in the same domain for 100 times and construct evaluate the ripley functions for these simulations.\n\nimport geopandas\ndf = geopandas.read_file(ps.examples.get_path(\"vautm17n.shp\"))\nstate = df.geometry.cascaded_union\n\nGenerate the point pattern pp (size 100) from CSR as the “observed” point pattern.\n\npattern = ripley.simulate(state, size=100)\n\nbefore we go any further, let’s visualize these simulated values:\n\ndf.plot()\nplt.scatter(*pattern.T, color='orangered', marker='.')\nplt.show()\n\n&lt;matplotlib.collections.PathCollection at 0x7fa1a2827890&gt;\n\n\n\n\n\nAnd, let’s check if there are 100 points:\n\npattern.shape\n\n(100, 2)\n\n\nYep! So, next to simulate a set of realizations in the same manner, we can use the size argument again, just like the numpy.random simulators. This means that, to simulate \\(K\\) realizations of a pattern of size \\(N\\), then we use simulate(hull, size=(N,K). For just one realization, we can use size=N.\n\nrandom_realizations = ripley.simulate(state, size=(100,100))\n\nTo show the random pattern is truly random, we can visualize all of the points:\n\ndf.plot(facecolor='none', edgecolor='k')\nplt.scatter(*random_realizations.T, marker='.', s=2)\nplt.scatter(*pattern.T, color='orangered', marker='.')\nplt.show()\n\n\n\n\nLet’s now compute the G function for the observed pattern as well as all the realizations we just made.\n\nobserved_g = ripley.g_function(pattern)\ncomparison_g = [ripley.g_function(realization, support=observed_g[0]) \n                for realization in random_realizations]\n\n\nplt.plot(*observed_g, color='orangered')\n[plt.plot(*comparison, color='k', alpha=.01) \n for comparison in comparison_g]\nplt.show()\n\n\n\n\nAll other functions work identically!"
  },
  {
    "objectID": "lectures/week-12/2023-11-08-geostat-data.html",
    "href": "lectures/week-12/2023-11-08-geostat-data.html",
    "title": "Sampling the raster for “observations”",
    "section": "",
    "text": "import rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nras = 'data/stanford-td754wr4701-geotiff.tiff' # already interpolated but we will treat it as \"data\"\nshp = 'data/tl_2022_06073_faces.shp'\ngdf = gpd.read_file(shp)\ngdf.shape\ngdf.head()\ncounty = gdf.dissolve(by='COUNTYFP20')\ncounty.plot()\nrast = rasterio.open(ras)\ncounty = county.to_crs(rast.crs)\ncounty.plot()\nrast.crs\ncoords = gdf.geometry\nsrc = rast\ndf = county\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nclipped_array, clipped_transform = mask(dataset=src, shapes=coords, crop=True)\n\ndf = df.to_crs(src.crs)\nout_meta = src.meta.copy()\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": clipped_array.shape[1],\n                 \"width\": clipped_array.shape[2],\n                 \"transform\": clipped_transform})\nout_tif= \"clipped_example.tif\"\nwith rasterio.open(out_tif, \"w\", **out_meta) as dest:\n    dest.write(clipped_array)\n    \nclipped = rasterio.open(out_tif)\nfig, ax = plt.subplots(figsize=(8, 6))\np1 = df.plot(color=None,facecolor='none',edgecolor='red',linewidth = 2,ax=ax)\nshow(clipped, ax=ax)\nax.axis('off');\nclipped\nimport rioxarray\nd = rioxarray.open_rasterio(\"clipped_example.tif\")\nd\nd.plot()\nd.values.max()\nd.plot()\ntype(d)\nd.dims\nd.values.mean()\nimport numpy\nnumpy.median(d.values)\nd.values.shape\nd.plot()\nd.plot.hist()\ntype(d)\nimport numpy\nnumpy.random.seed(12345)\nsample_points = county.sample_points(50)\nm = county.explore()\nsample_points.explore(m=m, color='red')"
  },
  {
    "objectID": "lectures/week-12/2023-11-08-geostat-data.html#interpolation-methods",
    "href": "lectures/week-12/2023-11-08-geostat-data.html#interpolation-methods",
    "title": "Sampling the raster for “observations”",
    "section": "Interpolation Methods",
    "text": "Interpolation Methods\n\ntracts = gdf.dissolve(by='TRACTCE20')\n\ntracts.shape\n\n\nSurface to Area Interpolation\n\nSpatial Join on Centroid\n\ncents = tracts.centroid\n\n\ncents.plot()\n\n\ntype(cents)\n\n\ncoord_list = [(x, y) for x, y in zip(cents.x, cents.y)]\ntracts['centest'] = [x[0] for x in clipped.sample(coord_list)]\ntracts.head()\n\n\ntracts['centroid'] = tracts.centroid\ntracts.set_geometry('centroid', inplace=True)\n\n\ntracts.plot(column='centest', legend=True);\n\n\ntracts.set_geometry('geometry', inplace=True)\ntracts.plot(column='centest', legend=True);\n\n\n\nZonal Methods of Surface to Area Interpolation\n\nimport rasterstats\n\n\ngdf.head()\n\n\ntracts.plot()\n\n\nfrom rasterstats import zonal_stats\ntstats = zonal_stats(tracts, \"clipped_example.tif\",\n            stats=\"count min mean max median\")\n\n#elevations2 = zonal_stats(\n#    sd_tracts.to_crs(dem.rio.crs),  # Geotable with zones\n#    \"../data/nasadem/nasadem_sd.tif\",  # Path to surface file\n#)\n#elevations2 = pandas.DataFrame(elevations2)\n\n\ntstats[:5]\n\n\ntstats = pd.DataFrame(tstats)\n\n\ntstats.head()\n\n\ntstats.shape\n\n\ntracts.shape\n\n\ntracts['mean'] = tstats['mean'].values\ntracts.plot(column='mean', legend=True);\n\n\ntracts['median'] = tstats['median'].values\ntracts.plot(column='median', legend=True);\n\n\ntracts['range'] = tstats['max'].values - tstats['min'].values\ntracts.plot(column='range', legend=True);\n\n\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(data=tracts, x='centest', y='mean')\nplt.plot([10, 40], [10, 40]);\n\n\nsns.scatterplot(data=tracts, x='median', y='mean')\nplt.plot([10, 40], [10, 40]);"
  },
  {
    "objectID": "lectures/week-12/2023-11-06-distance-based.html",
    "href": "lectures/week-12/2023-11-06-distance-based.html",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "",
    "text": "from scipy import spatial\nimport libpysal as ps\nimport numpy as np\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\npoints = np.array([[66.22, 32.54], [22.52, 22.39], [31.01, 81.21],\n                   [9.47, 31.02],  [30.78, 60.10], [75.21, 58.93],\n                   [79.26,  7.68], [8.23, 39.93],  [98.73, 77.17],\n                   [89.78, 42.53], [65.19, 92.08], [54.46, 8.48]])\nimport pointpats\npointpats.__version__\n\n'2.3.0'\nimport sklearn\nsklearn.__version__\n\n'1.3.2'"
  },
  {
    "objectID": "lectures/week-12/2023-11-06-distance-based.html#nearest-neighbor-distance-functions",
    "href": "lectures/week-12/2023-11-06-distance-based.html#nearest-neighbor-distance-functions",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "Nearest Neighbor Distance Functions",
    "text": "Nearest Neighbor Distance Functions\nNearest neighbour distance distribution functions (including the nearest “event-to-event” and “point-event” distance distribution functions) of a point process are cumulative distribution functions of several kinds – \\(G, F, J\\). By comparing the distance function of the observed point pattern with that of the point pattern from a CSR process, we are able to infer whether the underlying spatial process of the observed point pattern is CSR or not for a given confidence level.\n\n\\(G\\) function - event-to-event\nThe \\(G\\) function is a kind of “cumulative” density describing the distribution of distances within a point pattern. For a given distance \\(d\\), \\(G(d)\\) is the proportion of nearest neighbor distances that are less than \\(d\\). To express this, we first need to define the nearest neighbor distance, which is the smallest distance from each observation \\(i\\) to some other observation \\(j\\), where \\(j \\neq i\\): \\[ min_{j\\neq i}\\{d_{ij}\\} = d^*_i \\]\nWith this, we can define the \\(G\\) function as a cumulative density function: \\[G(d) = \\frac{1}{N}\\sum_{i=1}^N \\mathcal{I}(d^*_i &lt; d)\\] where \\(\\mathcal{I}(.)\\) is an indicator function that is \\(1\\) when the argument is true and is zero otherwise. In simple terms, \\(G(d)\\) gives the percentage of of nearest neighbor distances (\\(d^*_i\\)) that are smaller than \\(d\\); when \\(d\\) is very small, \\(G(d)\\) is close to zero. When \\(d\\) is large, \\(G(d)\\) approaches one.\nAnalytical results about \\(G\\) are available assuming that the “null” process of locating points in the study area is completely spatially random. In a completely spatially random process, the \\(G(d)\\) value should be: \\[\nG(d) = 1-e^{-\\lambda \\pi d^2}\n\\] Practically, we assess statistical significance for the \\(G(d)\\) function using simulations, where a known spatially-random process is generated and then analyzed. This partially accounts for issues with irregularly-shaped study areas, where locations of points are constrained.\nIn practice, we use the ripley.g_test function to conduct a test on the \\(G(d)\\). It estimates a value of \\(G(d)\\) for a set of values (called the support). To compute the \\(G\\) function for ten values of \\(d\\) ranging from the smallest possible to the largest values in the data:\n\nfrom pointpats import g_test\n\n\ng_res = g_test(points, support=10)\n\n\ng_res.support\n\narray([ 0.        ,  3.84791574,  7.69583148, 11.54374723, 15.39166297,\n       19.23957871, 23.08749445, 26.93541019, 30.78332593, 34.63124168])\n\n\n\ng_res.statistic\n\narray([0.        , 0.        , 0.        , 0.16666667, 0.16666667,\n       0.25      , 0.58333333, 0.83333333, 0.91666667, 1.        ])\n\n\n\ng_res.pvalue\n\narray([0.00e+00, 0.00e+00, 0.00e+00, 2.92e-02, 1.10e-03, 1.00e-04,\n       4.50e-03, 6.28e-02, 7.30e-02, 0.00e+00])\n\n\n\nKeeping simulations\nTo make a plot of the statistic, the statistic is generally plotted on the vertical axis and the support on the horizontal axis. Here, we will show the median simulated value of \\(G(d)\\) as well.\n\ng_res = g_test(points, support=10, keep_simulations=True)\n\n\nplt.plot(g_res.support, np.median(g_res.simulations, axis=0), \n         color='k', label='simulated')\nplt.plot(g_res.support, g_res.statistic, \n         marker='x', color='orangered', label='observed')\nplt.legend()\nplt.xlabel('Distance')\nplt.ylabel('G Function')\nplt.title('G Function Plot')\nplt.show()\n\n\n\n\n\n# grab the middle 95% of simulations using numpy:\nmiddle_95pct = np.percentile(g_res.simulations, q=(2.5, 97.5), axis=0)\n# use the fill_between function to color between the 2.5% and 97.5% envelope\nplt.fill_between(g_res.support, *middle_95pct, \n                 color='lightgrey', label='simulated')\n\n# plot the line for the observed value of G(d)\nplt.plot(g_res.support, g_res.statistic, \n         color='orangered', label='observed')\n# and plot the support points depending on whether their p-value is smaller than .05\nplt.scatter(g_res.support, g_res.statistic, \n            cmap='viridis', c=g_res.pvalue &lt; .01)\nplt.legend()\nplt.xlabel('Distance')\nplt.ylabel('G Function')\nplt.title('G Function Plot')\nplt.show()\n\n\n\n\n\n\n\n\\(F\\) function - “point-event”\nWhen the number of events in a point pattern is small, \\(G\\) function is rough. For the pattern contained in points, there are only 12 observations! This means that there are only 12 nearest neighbor distances, and thus only 12 possible values for the \\(G(d)\\) statistic, at any \\(d\\).\nOne way to get around this is to turn to an alternative, the \\(F(d)\\) function. This is analogous to the \\(G(d)\\) function, but measures the nearest neighbor distance from a set of known randomly-distributed points to a point in the observed pattern. Another way of thinking about \\(F(d)\\) is that it reflects a between-pattern measure of dispersion, where one pattern is completely spatially random and the other pattern is our observed pattern. In contrast, \\(G(d)\\) is a within-pattern measure of dispersion.\nFor a randomly simulated point pattern of size \\(N_s\\), this makes the \\(F(d)\\) function:\n\\[F(d) = \\frac{1}{N_s} \\sum_k^{N_s} \\mathcal{I}(d^*_k &lt; d)\\]\nThis can have \\(N_s\\) possible values for any \\(d\\), and thus can give a much more fine-grained view of the point pattern. In this sense, the \\(F(d)\\) function is often called the empty space function, as it measures the distance from random points in “empty space” to the “filled” points in our point pattern. The number of those random points governs how “fine-grained” our measure of the observed point pattern can be.\nJust like the ripley.g_test, this function is evaluated for every \\(d\\) in a support. Further, we can provide custom values for support, just in case we have known distance values of interest.\nBelow, we’ll use the same ten support values from \\(G(d)\\) function. And, let’s constrain the “simulated” point patterns to fall within the convex hull of our original point pattern:\n\nfrom pointpats import f_test as f\n\n\nf_test = f(points, support = g_res.support, keep_simulations=True, hull='convex', n_simulations=999)\n\n\nplt.plot(f_test.support, f_test.simulations.T, alpha=.01, color='k')\nplt.plot(f_test.support, f_test.statistic, color='red')\n\nplt.scatter(f_test.support, f_test.statistic, \n            cmap='viridis', c=f_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('F Function')\nplt.title('F Function Plot')\nplt.show()\n\nplt.show()\n\n\n\n\nFrom this we see that the values of the \\(F\\) function are too high for distances from about 15 to 25, and (in contrast) for values between \\(5 &lt; d &lt; 10\\), the \\(F(d)\\) function has too few short distances. When the observed \\(F(d)\\) values are too large, then the pattern is too dispersed, or regular. If the empirical \\(F(d)\\) tends to fall below the simulated values, then it reflects clustering. This is the opposite of the interpretation of the \\(G(d)\\) function above, so be careful!\n\n\n\\(J\\) function - a combination of “event-event” and “point-event”\nThe \\(J\\) function combines the \\(G\\) and \\(F\\) function, in an attempt to provide an immediate graphical indication of the clustering both internally and with respect to the empty space distribution. Practically, the \\(J(d)\\) function is computed as a kind of “relative clustering ratio”:\n\\[J(d) = \\frac{1-G(d)}{1-F(d)}\\]\nwhere the numerator captures the clustering due to within-pattern distances and the denominator captures that for the pattern-to-empty distances. This means that when \\(J(d)&lt;1\\), the underlying point process is a cluster point process, and when \\(J(d)=1\\), the underlying point process is a random point process; otherwise, it is a dispersed point process.\nThis function can suffer from numerical stability issues; as \\(G(d)\\) and \\(F(d)\\) both approach \\(1\\), the \\(J\\) ratio can become chaotic. Further, when \\(G\\) or \\(F\\) reaches one, the \\(J\\) function changes abruptly. As such, the \\(J\\) function is often truncated to the first \\(1\\) (either in \\(F(d)\\) or \\(G(d)\\)), and any \\(d\\) where both \\(F\\) and \\(G\\) are \\(1\\) is assigned a \\(J\\) value of \\(1\\).\n\nfrom pointpats import j_test as j\n\n\njp1 = j(points, support=20)\n\n/tmp/ipykernel_26245/134666654.py:1: UserWarning: requested 20 bins to evaluate the J function, but it reaches infinity at d=34.6312, meaning only 20 bins will be used to characterize the J function.\n  jp1 = j(points, support=20)\n\n\nAs you can see from the warning above, the \\(J\\) function did encounter numerical stability issues at about \\(d=25\\). To address this, pointpats truncated the \\(J\\) function to only have 14 values in its support, rather than the \\(20\\) requested.\n\nplt.plot(jp1.support, jp1.statistic, color='orangered')\nplt.axhline(1, linestyle=':', color='k')\nplt.xlabel('Distance')\nplt.ylabel('J Function')\n\nText(0, 0.5, 'J Function')\n\n\n\n\n\nFrom the above figure, we see that the \\(J\\) function is above the \\(J(d)=1\\) horizontal line, especially as \\(d\\) gets large. This suggests that the process is over-dispersed."
  },
  {
    "objectID": "lectures/week-12/2023-11-06-distance-based.html#interevent-distance-functions",
    "href": "lectures/week-12/2023-11-06-distance-based.html#interevent-distance-functions",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "Interevent Distance Functions",
    "text": "Interevent Distance Functions\nWhile both the \\(F(d)\\) and \\(G(d)\\) functions are useful, they only consider the distance between each point \\(i\\) and its nearest point. Earlier we spelled this distance \\(d_i^*\\), and the distance between \\(i\\) and \\(j\\) was \\(d_{ij}\\). So, note that \\(d_{i}^*\\) is the only term that matters for \\(F\\) and \\(G\\), if \\(d_{ij}\\) changes (but \\(j\\) isn’t closest to \\(i\\)), then the \\(F\\) and \\(G\\) functions generally remain the same.\nSo, further statistical summary functions have been developed to consider the whole distance distribution, not only the nearest neighbor distances. These functions (still considered part of the “Ripley” alphabet, are the \\(K\\), and \\(L\\) functions.\n\n\\(K\\) function\nThe \\(K(d)\\) function is a scaled version of the cumulative density function for all distances within a point pattern. As such, it’s a “relative” of the \\(G\\) function that considers all distances, not just the nearest neighbor distances. Practically, the \\(K(d)\\) function can be thought of as the percentage of all distances that are less than \\(d\\). Therefore, for a threshold distance \\(d\\), the \\(K\\) function is defined as:\n\\[K(d) = \\frac{1}{N\\hat\\lambda} \\underset{i=1}{\\overset{N}{\\sum}}\\underset{j=1}{\\overset{N}{\\sum}} \\mathcal{I}\\left(d_ij &lt; d\\right)\\]\nIn this equation, \\(\\hat\\lambda\\) is the intensity of the point process. This represents how many points (on average) you would expect in a unit area. You can think of this as an analogue to the density of the points in the pattern: large values of \\(\\hat\\lambda\\) mean many points per area, and small values of \\(\\hat\\lambda\\) mean there are fewer points per area. Generally, this parameter is unknown, and is modelled using the average number of points in the study area. This assumes that the intensity of the point pattern is constant or homogeneous over the study area.\nIn the same manner as before, we can construct a set of \\(K(d)\\) function evaluations for random point patterns, and compare them to the observed \\(K(d)\\) function we saw in our original data.\n\nfrom pointpats import k_test as k\n\n\nk_test = k(points, keep_simulations=True, n_simulations=99)\n\n\nplt.plot(k_test.support, k_test.simulations.T, color='k', alpha=.01)\nplt.plot(k_test.support, k_test.statistic, color='orangered')\n\nplt.scatter(k_test.support, k_test.statistic, \n            cmap='viridis', c=k_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('K Function')\nplt.title('K Function Plot')\nplt.show()\n\n\n\n\nAgain, we can see that the envelopes are generally above the observed function, meaining that our point pattern is dispersed. We can draw this conclusion because the distances are too small, suggesting the pattern is less clustered than otherwise woudl be expected. When points are too regular, their distances tend to be smaller than if they were distributed randomly."
  },
  {
    "objectID": "lectures/week-12/2023-11-06-distance-based.html#csr-example",
    "href": "lectures/week-12/2023-11-06-distance-based.html#csr-example",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "CSR Example",
    "text": "CSR Example\nIn this example, we are going to generate a point pattern as the “observed” point pattern. This ensures that the data generating process is completely spatially random. Then, we will simulate CSR in the same domain for 100 times and construct evaluate the ripley functions for these simulations.\n\nimport geopandas\nimport libpysal as ps\ndf = geopandas.read_file(ps.examples.get_path(\"vautm17n.shp\"))\nstate = df.geometry.cascaded_union\n\n/tmp/ipykernel_26245/3264935310.py:4: FutureWarning: The 'cascaded_union' attribute is deprecated, use 'unary_union' instead\n  state = df.geometry.cascaded_union\n\n\n\nfrom pointpats.random import poisson\n\n\ncsr = poisson(state, size=100)\n\n\ncsr[0:5]\n\narray([[ 392528.04579551, 4077062.60765435],\n       [ 711782.74903107, 4245360.48305302],\n       [ 593647.22771944, 4102442.85622552],\n       [ 616166.00712074, 4230904.07751182],\n       [ 731984.57021936, 4201204.81536705]])\n\n\n\ndf.plot()\nplt.scatter(*csr.T, color='orangered', marker=\".\")\nplt.show()\n\n\n\n\n\nrealizations = poisson(state, size=(100,100))\n\n\ndf.plot(facecolor='none', edgecolor='k')\nplt.scatter(*realizations.T, marker='.', s=2)\nplt.scatter(*csr.T, color='orangered', marker='.')\nplt.show()\n\n\n\n\nLet’s now compute the G function for the observed pattern as well as all the realizations we just made.\n\nfrom pointpats import g\n\n\nobserved_g = g(csr)\n\ncomparison_g = [ g(realization, support=observed_g[0])\n                for realization in realizations]\n\nplt.plot(*observed_g, color='orangered')\n[plt.plot(*comparison, color='k', alpha=.01) \n for comparison in comparison_g]\nplt.show()"
  },
  {
    "objectID": "lectures/week-12/2023-11-06-distance-based.html#clustered-process",
    "href": "lectures/week-12/2023-11-06-distance-based.html#clustered-process",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "Clustered Process",
    "text": "Clustered Process\n\nfrom pointpats.random import cluster_poisson\nimport numpy\n\n\nnumpy.random.seed(1234567)\nclust = cluster_poisson(state, size=100, n_seeds=3,\n                       cluster_radius=100000)\n\n\ndf.plot()\nplt.scatter(*clust.T, color='orangered', marker=\".\")\nplt.show()\n\n\n\n\n\nobserved_g_csr = g(csr)\n\n\nobserved_g_clust = g(clust)\n\ncomparison_g = [ g(realization, support=observed_g[0])\n                for realization in realizations]\n\nplt.plot(*observed_g_csr, color='orangered', label='CSR')\nplt.plot(*observed_g_clust, color='blue', label='Clustered')\n[plt.plot(*comparison, color='k', alpha=.01) \n for comparison in comparison_g]\nplt.ylabel(\"G(d)\")\nplt.xlabel(\"d\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#outline",
    "href": "lectures/week-06/2023-09-27.html#outline",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Outline",
    "text": "Outline\n\nConcepts and Issues\nNull and Alternative Hypotheses\nSpatial Autocorrelation Tests"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-dependence",
    "href": "lectures/week-06/2023-09-27.html#spatial-dependence",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\n\nThere is no question with respect to emergent geospatial science. The important harbingers were Geary’s article on spatial autocorrelation, Dacey’s paper about two- and K-color maps, and that of Bachi on geographic series.\n– Berry, Griifth, Tiefelsdorf (2008)"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-dependence-1",
    "href": "lectures/week-06/2023-09-27.html#spatial-dependence-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nWorking Concept\n\nwhat happens at one place depends on events in nearby places\nall things are related but nearby things are more related than distant things (Tobler)\ncentral focus in lattice data analysis"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#goodchild-1991",
    "href": "lectures/week-06/2023-09-27.html#goodchild-1991",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Goodchild 1991",
    "text": "Goodchild 1991\n\na world without positive spatial dependence would be an impossible world\nimpossible to describe\nimpossible to live in\nhell is a place with no spatial dependence"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-dependence-2",
    "href": "lectures/week-06/2023-09-27.html#spatial-dependence-2",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nCategorizing\n\nType: Substantive versus nuisance\nDirection: Positive versus negative\n\nIssues\n\nTime versus space\nInference"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#substantive-spatial-dependence",
    "href": "lectures/week-06/2023-09-27.html#substantive-spatial-dependence",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Substantive Spatial Dependence",
    "text": "Substantive Spatial Dependence\nProcess Based\n\nPart of the process under study\nLeaving it out\n\nIncomplete understanding\nBiased inferences"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#nuisance-spatial-dependence",
    "href": "lectures/week-06/2023-09-27.html#nuisance-spatial-dependence",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Nuisance Spatial Dependence",
    "text": "Nuisance Spatial Dependence\nNot Process Based\n\nArtifact of data collection\nProcess boundaries not matching data boundaries\nScattering across pixels\nGIS induced"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#boundary",
    "href": "lectures/week-06/2023-09-27.html#boundary",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Boundary",
    "text": "Boundary"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#boundary-mismatch",
    "href": "lectures/week-06/2023-09-27.html#boundary-mismatch",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Boundary Mismatch",
    "text": "Boundary Mismatch\n\n\nEven if \\(A\\) and \\(B\\) are independent\n\\(A'\\) and \\(B'\\) will be dependent"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#nusiance-vs.-substantive-dependence",
    "href": "lectures/week-06/2023-09-27.html#nusiance-vs.-substantive-dependence",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Nusiance vs. Substantive Dependence",
    "text": "Nusiance vs. Substantive Dependence\nIssues\n\nNot always easy to differentiate from substantive\nDifferent implications for each type\nSpecification strategies (Econometrics)\nBoth can be operating jointly"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#space-versus-time",
    "href": "lectures/week-06/2023-09-27.html#space-versus-time",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Space versus Time",
    "text": "Space versus Time\nTemporal Dependence\n\nPast influences the future\nRecursive\nOne dimension"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#space-versus-time-1",
    "href": "lectures/week-06/2023-09-27.html#space-versus-time-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Space versus Time",
    "text": "Space versus Time\nSpatial Dependence\n\nMulti-directional\nSimultaneous"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#terminology",
    "href": "lectures/week-06/2023-09-27.html#terminology",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Terminology",
    "text": "Terminology\nRelated Concepts\n\nSpatial Dependence\nSpatial Autocorrelation\nSpatial Association"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-dependence-3",
    "href": "lectures/week-06/2023-09-27.html#spatial-dependence-3",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nDistributional Characteristic\n\nMultivariate density function\ndifficult/impossible to verify empirically\n\nDependent Distribution\n\ndoes not factor in marginal densities"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-autocorrelation",
    "href": "lectures/week-06/2023-09-27.html#spatial-autocorrelation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\n\nAuto = same variable\nCorrelation = scaled covariance\nSpatial - geographic pattern to the correlation"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-autocorrelation-1",
    "href": "lectures/week-06/2023-09-27.html#spatial-autocorrelation-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\nMeasurement of Moment of Distribution\n\noff-diagonal elements of variance-covariance matrix\nautocovariance\n\\(C[y_i,y_j] \\ne 0 \\ \\forall i\\ne j\\)\ncan be estimated\n\nSpatial Autocorrelation Coefficient\n\nsignificance test on coefficient = 0"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-autocorrelation-2",
    "href": "lectures/week-06/2023-09-27.html#spatial-autocorrelation-2",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation",
    "text": "Spatial Autocorrelation\nJoint multivariate distribution function \\[f(y) = \\frac{ \\exp\\left[\n-\\frac{1}{2}\n(y-\\mu)'\n\\Sigma^{-1}\n(y-\\mu)\n\\right]}\n{\\sqrt{(2\\pi)^n|\\Sigma|}}\\]"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#variance-covariance-matrix",
    "href": "lectures/week-06/2023-09-27.html#variance-covariance-matrix",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Variance-Covariance Matrix",
    "text": "Variance-Covariance Matrix\n\\[\\Sigma=\n\\left[\n\\begin{array}{rrrr}\n\\sigma_{1,1}&\\sigma_{1,2}&\\ldots&\\sigma_{1,n}\\\\\n\\sigma_{2,1}&\\sigma_{2,2}&\\ldots&\\sigma_{2,n}\\\\\n\\vdots&\\vdots&\\ddots&\\vdots\\\\\n\\sigma_{n,1}&\\sigma_{n,2}&\\ldots&\\sigma_{n,n}\n\\end{array}\n\\right]\\]\n\ncovariance: \\(\\sigma_{i,j} = E[(y_i - \\mu_i)(y_j-\\mu_j) ]\\)\nsymmetry: \\(\\sigma_{i,j} =\\sigma_{i,j}\\)\nvariance: \\(\\sigma_{i,i} = E[(y_i - \\mu_i)(y_i-\\mu_i) ]\\)"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#correlation",
    "href": "lectures/week-06/2023-09-27.html#correlation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Correlation",
    "text": "Correlation\n\\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{i}^2}\\sqrt{\\sigma_{j}^2}}\\] \\[-1.0 \\le \\rho_{ij} \\le 1.0\\]"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#data-types-and-autocorrelation",
    "href": "lectures/week-06/2023-09-27.html#data-types-and-autocorrelation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Data Types and Autocorrelation",
    "text": "Data Types and Autocorrelation\nPoint Data\n\nfocus on geometric pattern\nrandom vs. nonrandom\nclustered vs. uniform\n\nGeostatistics\n\n2-D modeling of spatial covariance (pairs of observations in function of distance)\nkriging, spatial prediction"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#data-types-and-autocorrelation-1",
    "href": "lectures/week-06/2023-09-27.html#data-types-and-autocorrelation-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Data Types and Autocorrelation",
    "text": "Data Types and Autocorrelation\nLattice Data\n\nareal units: states, counties, census tracts, watersheds\npoints: centroids of areal units\nfocus on the spatial nonrandomness of attribute values"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-association",
    "href": "lectures/week-06/2023-09-27.html#spatial-association",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Association",
    "text": "Spatial Association\nNot a Rigorously Defined Term\n\nUsually the same as spatial autocorrelation\noften used in non-technical discussion\navoid unless meaning is clear"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-dependence-4",
    "href": "lectures/week-06/2023-09-27.html#spatial-dependence-4",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nGood News (for geographers)\n\nSpace matters\nSuggestive of underlying process\n\nBad news\n\ninvalidates random sampling assumption\nnecessitates new methods = spatial statistics and spatial econometrics"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-dependence-implications",
    "href": "lectures/week-06/2023-09-27.html#spatial-dependence-implications",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence: Implications",
    "text": "Spatial Dependence: Implications\nThe specific process we are simulating is as follows:\\[\\begin{aligned}\n\\label{eq:simdgp}y&=&X\\beta + \\epsilon \\\\ \\nonumber\\epsilon &=& \\lambda W \\epsilon + \\nu  \\end{aligned}\\] where \\(\\nu^{\\sim}N(0,\\sigma^{2}I)\\), \\(\\lambda\\) is a spatial autocorrelation parameter (scalar) and \\(W\\) is a spatial weights matrix. If \\(\\lambda=0\\) then the \\(i.i.d.\\) assumption holds, otherwise there is spatial dependence.\n\\(\\beta=40, \\ \\sigma^2=16, \\ x=[1,1,\\ldots]\\)\n\\(\\lambda=[0.0, 0.25, 0.50, 0.75], \\ n=25\\)"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-dependence-implications-1",
    "href": "lectures/week-06/2023-09-27.html#spatial-dependence-implications-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Dependence: Implications",
    "text": "Spatial Dependence: Implications\nFor each D.G.P. we are going to generate 500 samples of size \\(n=25\\) for our map. You can think of this as generating 500 maps using the same D.G.P.. For each sample we will then do the following:\n\nEstimate \\(\\mu\\) with \\(\\bar{y}\\)\nTest the hypothesis that \\(\\mu=40\\)"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#implications",
    "href": "lectures/week-06/2023-09-27.html#implications",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Implications",
    "text": "Implications\n\n\nMonte Carlo Results\n\n\n\\(\\lambda\\)\n0.00\n0.25\n0.50\n0.75\n\n\n\n\n\\(\\hat{\\mu}\\)\n39.947\n39.931\n39.901\n39.814\n\n\n\\(\\sigma_{\\bar{x}}\\)\n0.816\n1.090\n1.641\n3.304\n\n\n\\(p\\)\n0.056\n0.148\n0.278\n0.492"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-randomness",
    "href": "lectures/week-06/2023-09-27.html#spatial-randomness",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Randomness",
    "text": "Spatial Randomness\nNull Hypothesis\n\nobserved spatial pattern of values is equally likely as any other spatial pattern\nvalues at one location do no depend on values at other (neighboring) locations\nunder spatial randomness, the location of values may be altered without affecting the information content of the data"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-autocorrelation-on-a-grid",
    "href": "lectures/week-06/2023-09-27.html#spatial-autocorrelation-on-a-grid",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation on a Grid",
    "text": "Spatial Autocorrelation on a Grid\n\nNegative, Random, Positive"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#positive-spatial-autocorrelation",
    "href": "lectures/week-06/2023-09-27.html#positive-spatial-autocorrelation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Positive Spatial Autocorrelation",
    "text": "Positive Spatial Autocorrelation\nClustering\n\nlike values tend to be in similar locations\n\nNeighbor similarity\n\nmore alike than they would be under spatial randomness\n\nCompatible with Diffusion\n\nbut not necessarily caused by diffusion"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#positive-spatial-autocorrelation-1",
    "href": "lectures/week-06/2023-09-27.html#positive-spatial-autocorrelation-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Positive Spatial Autocorrelation",
    "text": "Positive Spatial Autocorrelation"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#negative-spatial-autocorrelation",
    "href": "lectures/week-06/2023-09-27.html#negative-spatial-autocorrelation",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Negative Spatial Autocorrelation",
    "text": "Negative Spatial Autocorrelation\nCheckerboard pattern\n\nanti-clustering\n\nNeighbor dissimilarity\n\nmore dissimilar than they would be under spatial randomness\n\nCompatible with Competition\n\nbut not necessarily caused by competition"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#negative-spatial-autocorrelation-1",
    "href": "lectures/week-06/2023-09-27.html#negative-spatial-autocorrelation-1",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Negative Spatial Autocorrelation",
    "text": "Negative Spatial Autocorrelation"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#autocorrelation-and-diffusion",
    "href": "lectures/week-06/2023-09-27.html#autocorrelation-and-diffusion",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Autocorrelation and Diffusion",
    "text": "Autocorrelation and Diffusion\nOne does not necessarily imply the other\n\ndiffusion tends to yield positive spatial autocorrelation but the reverse is not necessary\npositive spatial correlation may be due to structural factors, without contagion or diffusion"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#true-vs.-apparent-contagion",
    "href": "lectures/week-06/2023-09-27.html#true-vs.-apparent-contagion",
    "title": "Spatial Autocorrelation Concepts",
    "section": "True vs. Apparent Contagion",
    "text": "True vs. Apparent Contagion\nWhat is the Cause behind the clustering?\n\nTrue contagion\n\nresult of a contagious process, social interaction, dynamic process\n\nApparent contagion\n\nspatial heterogeneity\nstratification\n\nCannot be distinguished in a pure cross section\nEquifinality or Identification Problem"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#clustering",
    "href": "lectures/week-06/2023-09-27.html#clustering",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Clustering",
    "text": "Clustering\nGlobal characeristic\n\nproperty of overall pattern = all observations\nare like values more grouped in space than random\ntest by means of a global spatial autocorrelation statistic\nno location of the clusters determined"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#clusters",
    "href": "lectures/week-06/2023-09-27.html#clusters",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Clusters",
    "text": "Clusters\nLocal characeristic\n\nwhere are the like values more grouped in space than random?\nproperty of local pattern = location-specific\ntest by means of a local spatial autocorrelation statistic\nlocal clusters may be compatible with global spatial randomness"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#spatial-autocorrelation-statistic",
    "href": "lectures/week-06/2023-09-27.html#spatial-autocorrelation-statistic",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Spatial Autocorrelation Statistic",
    "text": "Spatial Autocorrelation Statistic\nStructure\n\nFormal Test of Match between Value Similarity and Locational Similarity\nStatistic Summarizes Both Aspects\nSignificance\n\nhow likely is it (p-value) that the computed statistic would take this (extreme) value in a spatially random pattern"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#attribute-similarity",
    "href": "lectures/week-06/2023-09-27.html#attribute-similarity",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Attribute Similarity",
    "text": "Attribute Similarity\n\nSummary of the similarity or dissimilarity of a variable at different locations\n\nvariable \\(y\\) at locations \\(i,j\\) with \\(i\\ne j\\)\n\nMeasures of similarity\n\ncross product: \\(y_i y_j\\)\n\nMeasures of dissimilarity\n\nsquared differences: \\((y_i - y_j)^2\\)\nabsolute differences: \\(|y_i - y_j|\\)"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#locational-similarity",
    "href": "lectures/week-06/2023-09-27.html#locational-similarity",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Locational Similarity",
    "text": "Locational Similarity\n\nFormalizing the notion of Neighbor\n\nwhen two spatial units a-priori are likely to interact\n\nSpatial Weights\n\nnot necessarility geographical\nmany approaches"
  },
  {
    "objectID": "lectures/week-06/2023-09-27.html#summary",
    "href": "lectures/week-06/2023-09-27.html#summary",
    "title": "Spatial Autocorrelation Concepts",
    "section": "Summary",
    "text": "Summary\nSpatial Dependence\n\nCore of Lattice Analysis\nSpatial Autocorrelation More Complex than Temporal Autocorrelation\nCombine Value and Locational Similarities"
  },
  {
    "objectID": "lectures/week-08/2023-10-11.html",
    "href": "lectures/week-08/2023-10-11.html",
    "title": "Local Spatial Autocorrelation",
    "section": "",
    "text": "import geopandas\n\n/tmp/ipykernel_1500521/1529612126.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas\ngdf = geopandas.read_parquet('repub_lean.parquet')\ngdf.plot()\n\n&lt;AxesSubplot:&gt;\ngdf.plot(column='rep_int', cmap='Reds', figsize=(16,9))\n\n&lt;AxesSubplot:&gt;\nimport matplotlib.pyplot as plt\nf, axs = plt.subplots(1, 2, figsize=(16, 9))\nax1, ax2 = axs\n\n\n\ngdf.plot(column='rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican\")\n\n\ngdf.plot(column='lag_rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\nax2.set_title(\"Spatial Lag Leaning Republican\")\n\nText(0.5, 1.0, 'Spatial Lag Leaning Republican')\nimport seaborn as sns\n_ = sns.regplot(x='rep_int', y='lag_rep_int', data=gdf)\nplt.axhline(y=gdf.lag_rep_int.mean(), color='g', linestyle='--')\nplt.axvline(x=gdf.rep_int.mean(), color='g', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f99e21f1970&gt;\ngdf.head()\n\n\n\n\n\n\n\n\nindex\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nRep\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\nrep_int\nqueen_neighbors\nlag_rep_int\n\n\n\n\n0\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((2516172.424 5253443.650, 25164...\n36%\n17%\n47%\n303\n36\n1\n35.00\n\n\n1\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((2351724.597 4850457.653, 23526...\n27%\n17%\n56%\n704\n27\n5\n30.80\n\n\n2\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((821167.988 5338182.388, 821794...\n34%\n19%\n47%\n982\n34\n3\n42.00\n\n\n3\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-371533.418 5042503.702, -386687.255...\n49%\n21%\n30%\n312\n49\n4\n52.25\n\n\n4\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-1384104.733 4308747.816, -1385163.9...\n37%\n18%\n46%\n314\n37\n5\n41.00\ny = gdf.rep_int\nylag = gdf.lag_rep_int\n\nyh = y &gt; y.mean()\nylh = ylag &gt; ylag.mean()\nq = 1 * yh * ylh + (2 * (1-yh) * ylh) + (3 * (1-yh) * (1-ylh))\nq[q==0] = 4\ngdf['q'] = q\nf, axs = plt.subplots(2,2, figsize=(16, 9))\nax1 = axs[0,0]\nax2 = axs[0,1]\nax3 = axs[1,0]\nax4 = axs[1,1]\n\n\ngdf.plot(column='rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican\")\n\n\ngdf.plot(column='lag_rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\nax2.set_title(\"Leaning Republican Spatial Lag\")\n\n\ngdf.plot(column='q',\n         categorical=True,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax3,\n        )\nax3.set_axis_off()\n_= ax3.set_title(\"Moran Scatterplot Quadrant\")\n\n\n_ = sns.regplot(x='rep_int', y='lag_rep_int', data=gdf, ax=ax4)\nplt.axhline(y=gdf.lag_rep_int.mean(), color='g', linestyle='--')\nplt.axvline(x=gdf.rep_int.mean(), color='g', linestyle='--')\n_= ax4.set_title(\"Moran Scatterplot\")"
  },
  {
    "objectID": "lectures/week-08/2023-10-11.html#morans-i",
    "href": "lectures/week-08/2023-10-11.html#morans-i",
    "title": "Local Spatial Autocorrelation",
    "section": "Moran’s I",
    "text": "Moran’s I\n\\[\nI = \\frac{n}{\\sum_i \\sum_j w_{i,j}} \\frac{\\sum_i \\sum_j w_{i,j} z_i z_j} {\\sum_i z_i^2}\n\\] where \\(n\\) is the number of observations, \\(z_i = y_i - \\bar{y}\\), and \\(w_{i,j}\\) is the cell corresponding to the \\(i\\)-th row and \\(j\\)-th column of the spatial weights matrix.\n\nfrom esda.moran import Moran\nimport libpysal\n\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:39: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_dist(x, y):\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:165: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def get_faces(triangle):\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:199: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:261: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_mask_faces(mask, faces):\n\n\n\ny = gdf.rep_int\nw = libpysal.weights.Queen.from_dataframe(gdf)\n\n\n \nmi = Moran(y, w)\n\n\nmi.EI\n\n-0.020833333333333332\n\n\n\nmi.EI_sim\n\n-0.017173754335507878\n\n\n\nmi.p_sim\n\n0.001\n\n\n\nimport seaborn\nax = seaborn.histplot(mi.sim)\n_ = plt.axvline(mi.I, 0, 100, color='r')"
  },
  {
    "objectID": "lectures/week-08/2023-10-11.html#local-moran",
    "href": "lectures/week-08/2023-10-11.html#local-moran",
    "title": "Local Spatial Autocorrelation",
    "section": "Local Moran",
    "text": "Local Moran\n\nfrom esda.moran import Moran_Local\n\n\\[\nI_i = \\frac{\\sum_j w_{i,j}z_i z_j}{\\sum_i z_i^2}= \\frac{z_i}{\\sum_i z_i^2} \\sum_j w_{i,j} z_j\n\\] where \\(n\\) is the number of observations, \\(z_i = y_i - \\bar{y}\\), and \\(w_{i,j}\\) is the cell corresponding to the \\(i\\)-th row and \\(j\\)-th column of the spatial weights matrix.\n\nlmi = Moran_Local(y, w)\n\n\nimport seaborn\n\n\nax = seaborn.histplot(lmi.Is)\n# Add one small bar (rug) for each observation\n# along horizontal axis\nseaborn.rugplot(lmi.Is, ax=ax);\n\n\n\n\n\nfrom splot import esda as esdaplot\nimport pandas\n\n\n# Set up figure and axes\nf, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n# Make the axes accessible with single indexing\naxs = axs.flatten()\ndb = gdf\nlisa = lmi\n# Subplot 1 #\n# Choropleth of local statistics\n# Grab first axis in the figure\nax = axs[0]\n# Assign new column with local statistics on-the-fly\ndb.assign(\n    Is=lmi.Is\n    # Plot choropleth of local statistics\n).plot(\n    column=\"Is\",\n    cmap=\"plasma\",\n    scheme=\"quantiles\",\n    k=5,\n    edgecolor=\"white\",\n    linewidth=0.1,\n    alpha=0.75,\n    legend=True,\n    ax=ax,\n)\n\n# Subplot 2 #\n# Quadrant categories\n# Grab second axis of local statistics\nax = axs[1]\n# Plot Quadrant colors (note to ensure all polygons are assigned a\n# quadrant, we \"trick\" the function by setting significance level to\n# 1 so all observations are treated as \"significant\" and thus assigned\n# a quadrant color\nesdaplot.lisa_cluster(lisa, db, p=1, ax=ax)\n\n# Subplot 3 #\n# Significance map\n# Grab third axis of local statistics\nax = axs[2]\n#\n# Find out significant observations\nlabels = pandas.Series(\n    1 * (lisa.p_sim &lt; 0.05),  # Assign 1 if significant, 0 otherwise\n    index=db.index  # Use the index in the original data\n    # Recode 1 to \"Significant and 0 to \"Non-significant\"\n).map({1: \"Significant\", 0: \"Non-Significant\"})\n# Assign labels to `db` on the fly\ndb.assign(\n    cl=labels\n    # Plot choropleth of (non-)significant areas\n).plot(\n    column=\"cl\",\n    categorical=True,\n    k=2,\n    cmap=\"Paired\",\n    linewidth=0.1,\n    edgecolor=\"white\",\n    legend=True,\n    ax=ax,\n)\n\n\n# Subplot 4 #\n# Cluster map\n# Grab second axis of local statistics\nax = axs[3]\n# Plot Quadrant colors In this case, we use a 5% significance\n# level to select polygons as part of statistically significant\n# clusters\nesdaplot.lisa_cluster(lisa, db, p=0.05, ax=ax)\n\n# Figure styling #\n# Set title to each subplot\nfor i, ax in enumerate(axs.flatten()):\n    ax.set_axis_off()\n    ax.set_title(\n        [\n            \"Local Statistics\",\n            \"Scatterplot Quadrant\",\n            \"Statistical Significance\",\n            \"Moran Cluster Map\",\n        ][i],\n        y=0,\n    )\n# Tight layout to minimize in-between white space\nf.tight_layout()\n\n# Display the figure\nplt.show()\n\n\n\n\n\ndir(lmi)\n\n['EI',\n 'EI_sim',\n 'EIc',\n 'Is',\n 'VI',\n 'VI_sim',\n 'VIc',\n '_Moran_Local__calc',\n '_Moran_Local__moments',\n '_Moran_Local__quads',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_statistic',\n 'by_col',\n 'den',\n 'geoda_quads',\n 'n',\n 'n_1',\n 'p_sim',\n 'p_z_sim',\n 'permutations',\n 'q',\n 'quads',\n 'rlisas',\n 'seI_sim',\n 'sim',\n 'w',\n 'y',\n 'z',\n 'z_sim']\n\n\n\nlmi.sim.shape\n\n(999, 49)\n\n\n\nidxs = lmi.p_sim &lt; 0.05\n\n\nidxs\n\narray([False,  True, False,  True, False, False,  True, False, False,\n       False, False,  True, False, False, False, False,  True, False,\n       False, False, False,  True,  True, False, False, False, False,\n       False, False, False, False, False, False, False,  True,  True,\n       False,  True,  True, False,  True, False, False, False,  True,\n        True, False, False,  True])\n\n\n\nidx = 1 # the second observation has a significant LISA\n\n\nax = seaborn.histplot(lmi.sim[idx])\n_ = plt.axvline(lmi.Is[idx], 0, 100, color='r')\n\n\n\n\n\nidx = 0 # the first observation does not have a significant lisa\n\n\nax = seaborn.histplot(lmi.sim[idx])\n_ = plt.axvline(lmi.Is[idx], 0, 100, color='r')"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#data-definitions",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#data-definitions",
    "title": "Spatial Data",
    "section": "Data Definitions",
    "text": "Data Definitions\n\nfacts and statistics collected together for reference or analysis\n\n\nthe quantities, characters, or symbols on which operations are performed by a computer, being stored and transmitted in the form of electrical signals and recorded on magnetic, optical, or mechanical recording media.\n\n\nthings known or assumed as facts, making the basis of reasoning or calculate\n\nSource: Oxford languages"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#datas-place",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#datas-place",
    "title": "Spatial Data",
    "section": "Data’s Place",
    "text": "Data’s Place\n\n\n\n\n\nDIKW Pyramid\n\n\n\n\ndata: discrete facts, unorganized and lacking context or information\ninformation: data imbued with meaning - what is in the data\nknowledge: perception of the world seen through information synthesis\nwisdom: “knowing the right things to do”"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#data-sets",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#data-sets",
    "title": "Spatial Data",
    "section": "Data Sets",
    "text": "Data Sets\nA data set is a collection of observations recorded for individual units on a set of variables.\nVariables are sometimes referred to as attributes or features (in machine learning parlance)."
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#measurement-scales",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#measurement-scales",
    "title": "Spatial Data",
    "section": "Measurement Scales",
    "text": "Measurement Scales\n\n\n\nScale\nOperations\nExample\n\n\n\n\nnominal\nmode, frequencies\nZip Code\n\n\nordinal\nA &gt; B\nRanks, Primary, Intermediate\n\n\ninterval\n+ -\nTime\n\n\nratio\n+ - * /\nWeight, Kelvin"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-data-is-special",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-data-is-special",
    "title": "Spatial Data",
    "section": "Spatial Data is Special",
    "text": "Spatial Data is Special\n\nSpatial data comes in many varieties and it is not easy to arrive at a system of classification that is simultaneously exclusive, exhaustive, imaginative, and satisfying.\n\n– G. Upton & B. Fingleton"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#what-is-special-about-spatial-data-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#what-is-special-about-spatial-data-1",
    "title": "Spatial Data",
    "section": "What is special about spatial data?",
    "text": "What is special about spatial data?\nLocation, Location, Location\nwhere matters\nDependence is the rule, not the exception\n\nspatial interaction, contagion, spill-overs\nspatial externalities\n\nSpatial Scale\n\nInference can change with scale"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#nature-of-spatial-data",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#nature-of-spatial-data",
    "title": "Spatial Data",
    "section": "Nature of Spatial Data",
    "text": "Nature of Spatial Data\nGeoreferences\nattribute data together with location\nGeocoding\n\nassociate observations with location\npoint: latitude-longtitude (GPS)\nareal unit: spatial reference"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geocoding-on-line",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geocoding-on-line",
    "title": "Spatial Data",
    "section": "Geocoding on-line",
    "text": "Geocoding on-line\n\nGeocode Input"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geocoding-on-line-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geocoding-on-line-1",
    "title": "Spatial Data",
    "section": "Geocoding on-line",
    "text": "Geocoding on-line\n\nGeocode Output"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#on-the-map",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#on-the-map",
    "title": "Spatial Data",
    "section": "On the Map?",
    "text": "On the Map?\n\nMap of Geocode Output"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#on-the-map-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#on-the-map-1",
    "title": "Spatial Data",
    "section": "On the Map?",
    "text": "On the Map?\n\nErrors in Geocode Output"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#location",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#location",
    "title": "Spatial Data",
    "section": "Location",
    "text": "Location\n\nGiven: in most spatial data analysis, no choice in location\nNo sampling in the usual sense\nData = attributes augmented with locational information"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-effects",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-effects",
    "title": "Spatial Data",
    "section": "Spatial Effects",
    "text": "Spatial Effects\nThe Trilogy\n\nSpatial Dependence\nSpatial Heterogeneity\nSpatial Scale"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-dependence",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-dependence",
    "title": "Spatial Data",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nTobler’s First Law of Geography\n\n“everything depends on everything else, but closer things more so”\n\n\nStructure of spatial dependence\nDistance Decay\nCloseness = Similarity"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-heterogenety",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-heterogenety",
    "title": "Spatial Data",
    "section": "Spatial Heterogenety",
    "text": "Spatial Heterogenety\nSpatial Instability\n\nProcess varies in some way over spatial units\nMultiple forms\n\nDiscrete = regimes\nContinuous = expansion method, GWR\n\nTrade-off\n\nSpatial homogeneity = stationary process\nUniqueness = extreme heterogeneity"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-scale-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-scale-1",
    "title": "Spatial Data",
    "section": "Spatial Scale",
    "text": "Spatial Scale\nMismatch\n\nSpatial scale of the process\nSpatial scale of our measurement\n\nIssues\n\npoints too far apart = miss small distance variation\narea aggregates cannot provide information on individual behavior\nEcological Fallacy"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#modifiable-areal-unit-problem-maup",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#modifiable-areal-unit-problem-maup",
    "title": "Spatial Data",
    "section": "Modifiable Areal Unit Problem (MAUP)",
    "text": "Modifiable Areal Unit Problem (MAUP)\nAggregation Problem\n\nspecial case of ecological fallacy\na million correlation coefficients\n\nZonation Problem\n\nsize\narangement\nHow many ways could you partition the coterminous US land area into 48 polygons?"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#maup-zonation-problem",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#maup-zonation-problem",
    "title": "Spatial Data",
    "section": "MAUP Zonation Problem",
    "text": "MAUP Zonation Problem\n\nhttp://en.wikipedia.org/wiki/Modifiable_areal_unit_problem"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#maup-aggregation-problem",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#maup-aggregation-problem",
    "title": "Spatial Data",
    "section": "MAUP Aggregation Problem",
    "text": "MAUP Aggregation Problem\n\n\n\n\n\nhttp://en.wikipedia.org/wiki/Modifiable_areal_unit_problem\n\n\n\n\nTrue rate = 1/3 = 33%\nA’s rate = (0 +1/2) /2 = 25%\nA’s weighted rate = 1/3 * 0 + 2/3 * 50 = 33%\nB’s rate = (0 + 100) /2 = 50%\nB’s weighted rate = 2/3 * 0 + 1/3 * 100 = 33%"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#spatial-process",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#spatial-process",
    "title": "Spatial Data",
    "section": "Spatial Process",
    "text": "Spatial Process\nSpatial Random Field\na mathemtical construct to capture randomness of values distributed over space\n\\[\\{Z(s):s \\in D \\} \\]\n\n\\(s \\in R^d:\\) location (e.g., lat-lon)\n\\(D \\in R^d:\\) index set = possible locations\n\\(Z(s):\\) random variable at location \\(s\\)"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#types-of-spatial-data-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#types-of-spatial-data-1",
    "title": "Spatial Data",
    "section": "Types of Spatial Data",
    "text": "Types of Spatial Data\n\nEvents\n\naddresses of crimes\n\nDiscrete Spatial Objects\n\ncounty crime rates\n\nContinuous surfaces\n\nair quality\nrainfall"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis",
    "title": "Spatial Data",
    "section": "Point Pattern Analysis",
    "text": "Point Pattern Analysis\nData\n\nmapped pattern = all the values\nnot a sample in the usual sense\n\nSpatial Process\n\nobservations as a realization of a random point process\npoints occur in space according to a mathematical model"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#point-patterns",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#point-patterns",
    "title": "Spatial Data",
    "section": "Point Patterns",
    "text": "Point Patterns\nUnmarked Point Pattern\n\nonly location is recorded\nno other attribute information\n\nMarked Point Pattern\n\nLocation is recorded\nStochastic attributes are also recorded\ne.g., sales price at address, DBH of a tree"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis-quadrat-methods",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis-quadrat-methods",
    "title": "Spatial Data",
    "section": "Point Pattern Analysis: Quadrat Methods",
    "text": "Point Pattern Analysis: Quadrat Methods\n\nQuadrat Analysis"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis-distance-based-methods",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#point-pattern-analysis-distance-based-methods",
    "title": "Spatial Data",
    "section": "Point Pattern Analysis: Distance Based Methods",
    "text": "Point Pattern Analysis: Distance Based Methods\n\nDistance Distributions"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#areal-unit-data-lattice",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#areal-unit-data-lattice",
    "title": "Spatial Data",
    "section": "Areal Unit Data (Lattice)",
    "text": "Areal Unit Data (Lattice)\n\nSpatial Domain: \\(D\\)\n\nDiscrete and fixed\nLocations nonrandom\nLocations countable\n\n\n\nExamples of lattice data\n\nAttributes collected by ZIP code\ncensus tract"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#lattice-data-indexing",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#lattice-data-indexing",
    "title": "Spatial Data",
    "section": "Lattice Data: Indexing",
    "text": "Lattice Data: Indexing\n\nSite\n\nEach location is now an area or site\nOne observation on \\(Z\\) for each site\nNeed a spatial index: \\(Z(s_i)\\)\n\n\n\n\\(Z(s_i)\\)\n\n\\(s_i\\) is a representative location within the site\ne.g., centroid, largest city\nAllows for measuring distances between sites"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#lattice-data-county-per-capita-incomes",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#lattice-data-county-per-capita-incomes",
    "title": "Spatial Data",
    "section": "Lattice Data: County Per Capita Incomes",
    "text": "Lattice Data: County Per Capita Incomes\n\n1969"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-analysis",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-analysis",
    "title": "Spatial Data",
    "section": "Geostatistical Analysis",
    "text": "Geostatistical Analysis\n\nSpatial Domain: \\(D\\)\n\nA continuous and fixed set.\nMeaning \\(Z(s)\\) can be observed everywhere within \\(D\\).\nBetween any two sample locations \\(s_i\\) and \\(s_j\\) you can theoretically place an infinite number of other samples.\nBy fixed: the points in \\(D\\) are non-stochastic"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data",
    "title": "Spatial Data",
    "section": "Geostatistical Data",
    "text": "Geostatistical Data\n\nContinuous Variation\n\nBecause of the continuity of \\(D\\)\nGeostatistical data is referred to as “spatial data with continuous variation.”\nContinuity is associated with \\(D\\).\nAttribute \\(Z\\) may, or may not, be continuous."
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-monitoring-sites",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-monitoring-sites",
    "title": "Spatial Data",
    "section": "Geostatistical Data: Monitoring Sites",
    "text": "Geostatistical Data: Monitoring Sites\n\nSites"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction",
    "title": "Spatial Data",
    "section": "Geostatistical Data: Surface Reconstruction",
    "text": "Geostatistical Data: Surface Reconstruction\n\nTessellation"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction-1",
    "title": "Spatial Data",
    "section": "Geostatistical Data: Surface Reconstruction",
    "text": "Geostatistical Data: Surface Reconstruction\n\nInterpolation"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction-2",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#geostatistical-data-surface-reconstruction-2",
    "title": "Spatial Data",
    "section": "Geostatistical Data: Surface Reconstruction",
    "text": "Geostatistical Data: Surface Reconstruction\n\nKriging"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#network-data",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#network-data",
    "title": "Spatial Data",
    "section": "Network Data",
    "text": "Network Data\n\n\nA network is a system of linear features connected at intersections and interchanges.\nThese intersections and interchanges are called nodes.\nThe linear feature connecting any given pair of nodes is called an arc.\nFormally, a network is defined as a directed graph \\(G = (N,  A)\\) consisting of an indexed set of nodes \\(N\\) with \\(n = |N|\\) and a spanning set of directed arcs \\(A\\) with \\(m = |A|\\), where \\(n\\) is the number of nodes and \\(m\\) is the number of arcs.\nEach arc on a network is represented as an ordered pair of nodes, in the form from node \\(i\\) to node \\(j\\), denoted by \\((i, j)\\)."
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#network-data-1",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#network-data-1",
    "title": "Spatial Data",
    "section": "Network Data",
    "text": "Network Data\n\nStreet Network"
  },
  {
    "objectID": "lectures/week-04/2023-09-11-spatial-data.html#flow-data",
    "href": "lectures/week-04/2023-09-11-spatial-data.html#flow-data",
    "title": "Spatial Data",
    "section": "Flow Data",
    "text": "Flow Data\n\n\n\n\nFlows"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#structure",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#structure",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Structure",
    "text": "Structure\n\na\nb\nc"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-disparities",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-disparities",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Spatial Disparities",
    "text": "Spatial Disparities"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#research-hypotheses",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#research-hypotheses",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Research Hypotheses",
    "text": "Research Hypotheses"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#data",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#data",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geoprocessing-in-support-of-spatial-disparities",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geoprocessing-in-support-of-spatial-disparities",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Geoprocessing in support of Spatial Disparities",
    "text": "Geoprocessing in support of Spatial Disparities"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#statistical-evaluation-of-hypotheses",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#statistical-evaluation-of-hypotheses",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Statistical Evaluation of Hypotheses",
    "text": "Statistical Evaluation of Hypotheses"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#interpretation",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#interpretation",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Interpretation",
    "text": "Interpretation"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#data-definitions",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#data-definitions",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Data Definitions",
    "text": "Data Definitions\n\nfacts and statistics collected together for reference or analysis\n\n\nthe quantities, characters, or symbols on which operations are performed by a computer, being stored and transmitted in the form of electrical signals and recorded on magnetic, optical, or mechanical recording media.\n\n\nthings known or assumed as facts, making the basis of reasoning or calculate\n\nSource: Oxford languages"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#datas-place",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#datas-place",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Data’s Place",
    "text": "Data’s Place\n\n\n\n\n\nDIKW Pyramid\n\n\n\n\ndata: discrete facts, unorganized and lacking context or information\ninformation: data imbued with meaning - what is in the data\nknowledge: perception of the world seen through information synthesis\nwisdom: “knowing the right things to do”"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#data-sets",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#data-sets",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Data Sets",
    "text": "Data Sets\nA data set is a collection of observations recorded for individual units on a set of variables.\nVariables are sometimes referred to as attributes or features (in machine learning parlance)."
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#measurement-scales",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#measurement-scales",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Measurement Scales",
    "text": "Measurement Scales\n\n\n\nScale\nOperations\nExample\n\n\n\n\nnominal\nmode, frequencies\nZip Code\n\n\nordinal\nA &gt; B\nRanks, Primary, Intermediate\n\n\ninterval\n+ -\nTime\n\n\nratio\n+ - * /\nWeight, Kelvin"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-data-is-special",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-data-is-special",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Spatial Data is Special",
    "text": "Spatial Data is Special\n\nSpatial data comes in many varieties and it is not easy to arrive at a system of classification that is simultaneously exclusive, exhaustive, imaginative, and satisfying.\n\n– G. Upton & B. Fingleton"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#what-is-special-about-spatial-data-1",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#what-is-special-about-spatial-data-1",
    "title": "Spatial Disparities Studio: Overview",
    "section": "What is special about spatial data?",
    "text": "What is special about spatial data?\nLocation, Location, Location\nwhere matters\nDependence is the rule, not the exception\n\nspatial interaction, contagion, spill-overs\nspatial externalities\n\nSpatial Scale\n\nInference can change with scale"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#nature-of-spatial-data",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#nature-of-spatial-data",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Nature of Spatial Data",
    "text": "Nature of Spatial Data\nGeoreferences\nattribute data together with location\nGeocoding\n\nassociate observations with location\npoint: latitude-longtitude (GPS)\nareal unit: spatial reference"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geocoding-on-line",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geocoding-on-line",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Geocoding on-line",
    "text": "Geocoding on-line\n\nGeocode Input"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geocoding-on-line-1",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geocoding-on-line-1",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Geocoding on-line",
    "text": "Geocoding on-line\n\nGeocode Output"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#on-the-map",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#on-the-map",
    "title": "Spatial Disparities Studio: Overview",
    "section": "On the Map?",
    "text": "On the Map?\n\nMap of Geocode Output"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#on-the-map-1",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#on-the-map-1",
    "title": "Spatial Disparities Studio: Overview",
    "section": "On the Map?",
    "text": "On the Map?\n\nErrors in Geocode Output"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#location",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#location",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Location",
    "text": "Location\n\nGiven: in most spatial data analysis, no choice in location\nNo sampling in the usual sense\nData = attributes augmented with locational information"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-effects",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-effects",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Spatial Effects",
    "text": "Spatial Effects\nThe Trilogy\n\nSpatial Dependence\nSpatial Heterogeneity\nSpatial Scale"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-dependence",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-dependence",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Spatial Dependence",
    "text": "Spatial Dependence\nTobler’s First Law of Geography\n\n“everything depends on everything else, but closer things more so”\n\n\nStructure of spatial dependence\nDistance Decay\nCloseness = Similarity"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-heterogenety",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-heterogenety",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Spatial Heterogenety",
    "text": "Spatial Heterogenety\nSpatial Instability\n\nProcess varies in some way over spatial units\nMultiple forms\n\nDiscrete = regimes\nContinuous = expansion method, GWR\n\nTrade-off\n\nSpatial homogeneity = stationary process\nUniqueness = extreme heterogeneity"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-scale-1",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-scale-1",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Spatial Scale",
    "text": "Spatial Scale\nMismatch\n\nSpatial scale of the process\nSpatial scale of our measurement\n\nIssues\n\npoints too far apart = miss small distance variation\narea aggregates cannot provide information on individual behavior\nEcological Fallacy"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#modifiable-areal-unit-problem-maup",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#modifiable-areal-unit-problem-maup",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Modifiable Areal Unit Problem (MAUP)",
    "text": "Modifiable Areal Unit Problem (MAUP)\nAggregation Problem\n\nspecial case of ecological fallacy\na million correlation coefficients\n\nZonation Problem\n\nsize\narangement\nHow many ways could you partition the coterminous US land area into 48 polygons?"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#maup-zonation-problem",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#maup-zonation-problem",
    "title": "Spatial Disparities Studio: Overview",
    "section": "MAUP Zonation Problem",
    "text": "MAUP Zonation Problem\n\nhttp://en.wikipedia.org/wiki/Modifiable_areal_unit_problem"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#maup-aggregation-problem",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#maup-aggregation-problem",
    "title": "Spatial Disparities Studio: Overview",
    "section": "MAUP Aggregation Problem",
    "text": "MAUP Aggregation Problem\n\n\n\n\n\nhttp://en.wikipedia.org/wiki/Modifiable_areal_unit_problem\n\n\n\n\nTrue rate = 1/3 = 33%\nA’s rate = (0 +1/2) /2 = 25%\nA’s weighted rate = 1/3 * 0 + 2/3 * 50 = 33%\nB’s rate = (0 + 100) /2 = 50%\nB’s weighted rate = 2/3 * 0 + 1/3 * 100 = 33%"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-process",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#spatial-process",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Spatial Process",
    "text": "Spatial Process\nSpatial Random Field\na mathemtical construct to capture randomness of values distributed over space\n\\[\\{Z(s):s \\in D \\} \\]\n\n\\(s \\in R^d:\\) location (e.g., lat-lon)\n\\(D \\in R^d:\\) index set = possible locations\n\\(Z(s):\\) random variable at location \\(s\\)"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#types-of-spatial-data-1",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#types-of-spatial-data-1",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Types of Spatial Data",
    "text": "Types of Spatial Data\n\nEvents\n\naddresses of crimes\n\nDiscrete Spatial Objects\n\ncounty crime rates\n\nContinuous surfaces\n\nair quality\nrainfall"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#point-pattern-analysis",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#point-pattern-analysis",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Point Pattern Analysis",
    "text": "Point Pattern Analysis\nData\n\nmapped pattern = all the values\nnot a sample in the usual sense\n\nSpatial Process\n\nobservations as a realization of a random point process\npoints occur in space according to a mathematical model"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#point-patterns",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#point-patterns",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Point Patterns",
    "text": "Point Patterns\nUnmarked Point Pattern\n\nonly location is recorded\nno other attribute information\n\nMarked Point Pattern\n\nLocation is recorded\nStochastic attributes are also recorded\ne.g., sales price at address, DBH of a tree"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#point-pattern-analysis-quadrat-methods",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#point-pattern-analysis-quadrat-methods",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Point Pattern Analysis: Quadrat Methods",
    "text": "Point Pattern Analysis: Quadrat Methods\n\nQuadrat Analysis"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#point-pattern-analysis-distance-based-methods",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#point-pattern-analysis-distance-based-methods",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Point Pattern Analysis: Distance Based Methods",
    "text": "Point Pattern Analysis: Distance Based Methods\n\nDistance Distributions"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#areal-unit-data-lattice",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#areal-unit-data-lattice",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Areal Unit Data (Lattice)",
    "text": "Areal Unit Data (Lattice)\n\nSpatial Domain: \\(D\\)\n\nDiscrete and fixed\nLocations nonrandom\nLocations countable\n\n\n\nExamples of lattice data\n\nAttributes collected by ZIP code\ncensus tract"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#lattice-data-indexing",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#lattice-data-indexing",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Lattice Data: Indexing",
    "text": "Lattice Data: Indexing\n\nSite\n\nEach location is now an area or site\nOne observation on \\(Z\\) for each site\nNeed a spatial index: \\(Z(s_i)\\)\n\n\n\n\\(Z(s_i)\\)\n\n\\(s_i\\) is a representative location within the site\ne.g., centroid, largest city\nAllows for measuring distances between sites"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#lattice-data-county-per-capita-incomes",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#lattice-data-county-per-capita-incomes",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Lattice Data: County Per Capita Incomes",
    "text": "Lattice Data: County Per Capita Incomes\n\n1969"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-analysis",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-analysis",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Geostatistical Analysis",
    "text": "Geostatistical Analysis\n\nSpatial Domain: \\(D\\)\n\nA continuous and fixed set.\nMeaning \\(Z(s)\\) can be observed everywhere within \\(D\\).\nBetween any two sample locations \\(s_i\\) and \\(s_j\\) you can theoretically place an infinite number of other samples.\nBy fixed: the points in \\(D\\) are non-stochastic"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Geostatistical Data",
    "text": "Geostatistical Data\n\nContinuous Variation\n\nBecause of the continuity of \\(D\\)\nGeostatistical data is referred to as “spatial data with continuous variation.”\nContinuity is associated with \\(D\\).\nAttribute \\(Z\\) may, or may not, be continuous."
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data-monitoring-sites",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data-monitoring-sites",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Geostatistical Data: Monitoring Sites",
    "text": "Geostatistical Data: Monitoring Sites\n\nSites"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data-surface-reconstruction",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data-surface-reconstruction",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Geostatistical Data: Surface Reconstruction",
    "text": "Geostatistical Data: Surface Reconstruction\n\nTessellation"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data-surface-reconstruction-1",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data-surface-reconstruction-1",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Geostatistical Data: Surface Reconstruction",
    "text": "Geostatistical Data: Surface Reconstruction\n\nInterpolation"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data-surface-reconstruction-2",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#geostatistical-data-surface-reconstruction-2",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Geostatistical Data: Surface Reconstruction",
    "text": "Geostatistical Data: Surface Reconstruction\n\nKriging"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#network-data",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#network-data",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Network Data",
    "text": "Network Data\n\n\nA network is a system of linear features connected at intersections and interchanges.\nThese intersections and interchanges are called nodes.\nThe linear feature connecting any given pair of nodes is called an arc.\nFormally, a network is defined as a directed graph \\(G = (N,  A)\\) consisting of an indexed set of nodes \\(N\\) with \\(n = |N|\\) and a spanning set of directed arcs \\(A\\) with \\(m = |A|\\), where \\(n\\) is the number of nodes and \\(m\\) is the number of arcs.\nEach arc on a network is represented as an ordered pair of nodes, in the form from node \\(i\\) to node \\(j\\), denoted by \\((i, j)\\)."
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#network-data-1",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#network-data-1",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Network Data",
    "text": "Network Data\n\nStreet Network"
  },
  {
    "objectID": "lectures/week-15/2023-11-27-studio-disparities-overview.html#flow-data",
    "href": "lectures/week-15/2023-11-27-studio-disparities-overview.html#flow-data",
    "title": "Spatial Disparities Studio: Overview",
    "section": "Flow Data",
    "text": "Flow Data\n\n\n\n\nFlows"
  },
  {
    "objectID": "lectures/week-05/2023-09-20.html",
    "href": "lectures/week-05/2023-09-20.html",
    "title": "GeoVisualization",
    "section": "",
    "text": "import pandas\ndf = pandas.read_csv(\"./data/shared/covid/covid_combined.csv\", index_col=\"date\",\n                    parse_dates=True)\ndf.head()\n\n\n\n\n\n\n\n\nstate\nfips\ncases\ndeaths\ndtc100\npopulation\ndeaths100k\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2020-01-21\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-22\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-23\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-24\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-25\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\nlast_df = df.loc['2020-08-02']\nlast_df.shape\n\n(54, 7)\nimport geopandas\ngdf = geopandas.read_file(\"./data/shared/covid/gz_2010_us_040_00_500k.json\")\ngdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\ngdf.columns.values\n\narray(['GEO_ID', 'STATE', 'NAME', 'LSAD', 'CENSUSAREA', 'geometry'],\n      dtype=object)\ngdf.plot()\n\n&lt;AxesSubplot:&gt;\ndf.head()\n\n\n\n\n\n\n\n\nstate\nfips\ncases\ndeaths\ndtc100\npopulation\ndeaths100k\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2020-01-21\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-22\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-23\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-24\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-25\nWashington\n53\n1\n0\n0.0\n7614893\n0.0"
  },
  {
    "objectID": "lectures/week-05/2023-09-20.html#table-join",
    "href": "lectures/week-05/2023-09-20.html#table-join",
    "title": "GeoVisualization",
    "section": "Table Join",
    "text": "Table Join\n\ngdf.rename(columns={'NAME': 'state'}, inplace=True)\ngdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n\n\n\n\n\n\n\n\ndf['date'] = df.index.values\njoin_gdf = gdf.merge(df, on='state')\njoin_gdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nfips\ncases\ndeaths\ndtc100\npopulation\ndeaths100k\ndate\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n1\n0\n0.0\n1344212\n0.0\n2020-03-12\n\n\n1\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n2\n0\n0.0\n1344212\n0.0\n2020-03-13\n\n\n2\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n3\n0\n0.0\n1344212\n0.0\n2020-03-14\n\n\n3\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n12\n0\n0.0\n1344212\n0.0\n2020-03-15\n\n\n4\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n17\n0\n0.0\n1344212\n0.0\n2020-03-16\n\n\n\n\n\n\n\n\nlast_gdf = join_gdf[join_gdf.date=='2020-08-02']\nlast_gdf.shape\n\n(52, 13)\n\n\n\ndrop = ['Puerto Rico', 'Alaska', 'Hawaii']\nlast_gdf = last_gdf[~last_gdf['state'].isin(drop)]\n\n\nlast_gdf.shape\n\n(49, 13)\n\n\n\nlast_gdf.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nlast_gdf.plot(column='dtc100')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nlast_gdf.plot(column='dtc100', legend=True, figsize=(16,9))\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nlast_gdf.plot(column='dtc100', legend=True, figsize=(16,9),\n               scheme='quantiles', k=10)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nlast_gdf.plot(column='dtc100', legend=True, figsize=(16,9),\n               scheme='quantiles', k=10,\n               legend_kwds={'loc': 'lower right'})\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='quantiles',k=10, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()"
  },
  {
    "objectID": "lectures/week-05/2023-09-20.html#classifiers",
    "href": "lectures/week-05/2023-09-20.html#classifiers",
    "title": "GeoVisualization",
    "section": "Classifiers",
    "text": "Classifiers\n\nEqual Interval\n\nax = last_gdf.plot(column='dtc100', scheme='EqualInterval',k=5, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\ny = last_gdf.dtc100\nimport mapclassify\n\n\nea5 = mapclassify.EqualInterval(y, k=5)\n\n\nea5\n\nEqualInterval\n\n  Interval     Count\n--------------------\n[0.76, 2.39] |    25\n(2.39, 4.02] |    13\n(4.02, 5.64] |     4\n(5.64, 7.27] |     3\n(7.27, 8.90] |     4\n\n\n\n4.02-2.39\n\n1.6299999999999994\n\n\n\n5.64-4.02\n\n1.62\n\n\n\n\nEqual Count (Quantiles)\n\neq5 = mapclassify.Quantiles(y, k=5)\n\n\neq5\n\nQuantiles\n\n  Interval     Count\n--------------------\n[0.76, 1.49] |    10\n(1.49, 1.84] |    10\n(1.84, 2.93] |     9\n(2.93, 4.23] |    10\n(4.23, 8.90] |    10\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='Quantiles',k=5, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\n\nMaximum Breaks\n\nmb5 = mapclassify.MaximumBreaks(y, k=5)\nmb5\n\nMaximumBreaks\n\n  Interval     Count\n--------------------\n[0.76, 5.03] |    41\n(5.03, 5.73] |     1\n(5.73, 6.66] |     2\n(6.66, 8.15] |     3\n(8.15, 8.90] |     2\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='MaximumBreaks',k=5, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\n\nBox Plot\n\nmapclassify.BoxPlot(y)\n\nBoxPlot\n\n   Interval      Count\n----------------------\n( -inf, -1.82] |     0\n(-1.82,  1.62] |    13\n( 1.62,  2.37] |    12\n( 2.37,  3.91] |    12\n( 3.91,  7.35] |     9\n( 7.35,  8.90] |     3\n\n\n\ny.median()\n\n2.3743977976600137\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='BoxPlot', legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\nmapclassify.BoxPlot?\n\n\nInit signature: mapclassify.BoxPlot(y, hinge=1.5)\nDocstring:     \nBoxPlot Map Classification.\nParameters\n----------\ny : numpy.array\n    Attribute to classify\nhinge : float (default 1.5)\n    Multiplier for *IQR*.\nAttributes\n----------\nyb : numpy.array\n    :math:`(n,1)`, bin ids for observations.\nbins : array\n    :math:`(n,1)`, the upper bounds of each class  (monotonic).\nk : int\n    The number of classes.\ncounts : numpy.array\n    :math:`(k,1)`, the number of observations falling in each class.\nlow_outlier_ids : numpy.array\n    Indices of observations that are low outliers.\nhigh_outlier_ids : numpy.array\n    Indices of observations that are high outliers.\nNotes\n-----\nThe bins are set as follows::\n    bins[0] = q[0]-hinge*IQR\n    bins[1] = q[0]\n    bins[2] = q[1]\n    bins[3] = q[2]\n    bins[4] = q[2]+hinge*IQR\n    bins[5] = inf  (see Notes)\nwhere :math:`q` is an array of the first three quartiles of :math:`y` and\n:math:`IQR=q[2]-q[0]`.\nIf :math:`q[2]+hinge*IQR &gt; max(y)` there will only be 5 classes and no high\noutliers, otherwise, there will be 6 classes and at least one high\noutlier.\nExamples\n--------\n&gt;&gt;&gt; import mapclassify\n&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; cal = mapclassify.load_example()\n&gt;&gt;&gt; bp = mapclassify.BoxPlot(cal)\n&gt;&gt;&gt; bp.bins\narray([-5.287625e+01,  2.567500e+00,  9.365000e+00,  3.953000e+01,\n        9.497375e+01,  4.111450e+03])\n&gt;&gt;&gt; list(bp.counts)\n[0, 15, 14, 14, 6, 9]\n&gt;&gt;&gt; list(bp.high_outlier_ids)\n[0, 6, 18, 29, 33, 36, 37, 40, 42]\n&gt;&gt;&gt; cal[bp.high_outlier_ids].values\narray([ 329.92,  181.27,  370.5 ,  722.85,  192.05,  110.74, 4111.45,\n        317.11,  264.93])\n&gt;&gt;&gt; bx = mapclassify.BoxPlot(numpy.arange(100))\n&gt;&gt;&gt; bx.bins\narray([-49.5 ,  24.75,  49.5 ,  74.25, 148.5 ])\nInit docstring:\nParameters\n----------\ny : numpy.array\n    :math:`(n,1)`, attribute to classify\nhinge : float (default 1.5)\n    Multiple of inter-quartile range.\nFile:           /opt/tljh/user/lib/python3.9/site-packages/mapclassify/classifiers.py\nType:           type\nSubclasses:     \n\n\n\n\n\nSTDMEAN\n\nmapclassify.StdMean(y)\n\nStdMean\n\n   Interval      Count\n----------------------\n( -inf, -1.15] |     0\n(-1.15,  0.97] |     3\n( 0.97,  5.22] |    38\n( 5.22,  7.34] |     5\n( 7.34,  8.90] |     3\n\n\n\ny.mean()- y.std()\n\n0.9741967014843262\n\n\n\ny.mean()+y.std()\n\n5.217609367021282\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='StdMean', legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\n\nFisher-Jenks\n\nmapclassify.FisherJenks(y, k=5)\n\nFisherJenks\n\n  Interval     Count\n--------------------\n[0.76, 2.12] |    24\n(2.12, 3.34] |     9\n(3.34, 5.29] |     9\n(5.29, 7.29] |     4\n(7.29, 8.90] |     3\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='FisherJenks', k=5, legend=True,\n             figsize=(16,9), cmap='Reds',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\nax = last_gdf.plot(column='dtc100', scheme='FisherJenks', k=5, legend=True,\n             figsize=(16,9), cmap='Blues',edgecolor='k',\n             legend_kwds={'loc': 'lower right'})\nax.set_title('COVID-19 Deaths per 100 Cases, 2020-08-02')\nax.set_axis_off()\n\n\n\n\n\nfj5 = mapclassify.FisherJenks(y,5)\nq5 = mapclassify.Quantiles(y, 5)\nea5 = mapclassify.EqualInterval(y, 5)\nfj5.adcm, q5.adcm, ea5.adcm\n\n(15.68079626959761, 21.603825374669483, 18.970554166382374)"
  },
  {
    "objectID": "lectures/week-05/2023-09-20.html#color-categorical",
    "href": "lectures/week-05/2023-09-20.html#color-categorical",
    "title": "GeoVisualization",
    "section": "Color: Categorical",
    "text": "Color: Categorical\n\nc_gdf = last_gdf.to_crs(last_gdf.estimate_utm_crs())\n\n\ncentroids =c_gdf.centroid\n\n\neast = c_gdf.centroid.x &gt; c_gdf.centroid.x.median()\nnorth = c_gdf.centroid.y &gt; c_gdf.centroid.y.median()\n\n\nc_gdf['east'] = c_gdf.centroid.x &lt; centroids.x.median()\nc_gdf['west'] = c_gdf.centroid.x &gt;= centroids.x.median()\nc_gdf['south'] = c_gdf.centroid.y &lt; centroids.y.median()\nc_gdf['north'] = c_gdf.centroid.y &gt;= centroids.y.median()\nc_gdf['NE'] = c_gdf.north * c_gdf.east\nc_gdf['SE'] = c_gdf.south * c_gdf.east\nc_gdf['NW'] = c_gdf.north * c_gdf.west\nc_gdf['SW'] = c_gdf.south * c_gdf.west\n    \n\n\n\nc_gdf['region'] = (1 * c_gdf.NE) + (2 * c_gdf.NW) + (3* c_gdf.SW) + (4*c_gdf.SE)\n\n\nc_gdf.groupby(by='region').count()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nfips\ncases\ndeaths\ndtc100\n...\ndate\neast\nwest\nsouth\nnorth\nNE\nSE\nNW\nSW\nusgreedy\n\n\nregion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n...\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n\n\n2\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n...\n11\n11\n11\n11\n11\n11\n11\n11\n11\n11\n\n\n3\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n...\n14\n14\n14\n14\n14\n14\n14\n14\n14\n14\n\n\n4\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n...\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n\n\n\n\n4 rows × 22 columns\n\n\n\n\nc_gdf.plot(column='region', categorical=True)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nc_gdf.plot(column='region')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n\nusgreedy = mapclassify.greedy(c_gdf)\n\n\nc_gdf['usgreedy'] = usgreedy\nc_gdf.plot(column='usgreedy', categorical=True)\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "lectures/week-05/2023-09-20.html#spatial-dynamics",
    "href": "lectures/week-05/2023-09-20.html#spatial-dynamics",
    "title": "GeoVisualization",
    "section": "Spatial Dynamics",
    "text": "Spatial Dynamics\n\n\njoin_gdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nfips\ncases\ndeaths\ndtc100\npopulation\ndeaths100k\ndate\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n1\n0\n0.0\n1344212\n0.0\n2020-03-12\n\n\n1\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n2\n0\n0.0\n1344212\n0.0\n2020-03-13\n\n\n2\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n3\n0\n0.0\n1344212\n0.0\n2020-03-14\n\n\n3\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n12\n0\n0.0\n1344212\n0.0\n2020-03-15\n\n\n4\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n17\n0\n0.0\n1344212\n0.0\n2020-03-16\n\n\n\n\n\n\n\n\ndates = [f\"2020-{month:02}-01\"  for month  in range(5,9)]\n\ndates\n\n['2020-05-01', '2020-06-01', '2020-07-01', '2020-08-01']\n\n\n\nlast4 = join_gdf[join_gdf.date.isin(dates)]\nlast4 = last4[~last4['state'].isin(drop)]\nlast4.shape\n\n(196, 13)\n\n\n\nfor day in dates:\n    last4[last4.date==day].plot(column='dtc100', scheme='Quantiles', k=5, \n                                legend=True, legend_kwds={'loc': 'lower right'})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport mapclassify\n\nq5 = mapclassify.Quantiles(last4.dtc100, k=5)\n\nq5\n\nQuantiles\n\n  Interval     Count\n--------------------\n[0.76, 1.87] |    40\n(1.87, 3.12] |    39\n(3.12, 4.20] |    39\n(4.20, 5.44] |    39\n(5.44, 9.45] |    39\n\n\n\nq5.bins\n\narray([1.86813187, 3.12420625, 4.20334682, 5.44027239, 9.45494994])\n\n\n\nlast4['pooled5'] = q5.yb\n\n\nfor day in dates:\n    last4[last4.date==day].plot(column='pooled5', scheme='equalinterval',\n                                legend=True, legend_kwds={'loc': 'lower right'})"
  },
  {
    "objectID": "lectures/week-05/2023-09-20.html#interactive-maps",
    "href": "lectures/week-05/2023-09-20.html#interactive-maps",
    "title": "GeoVisualization",
    "section": "Interactive Maps",
    "text": "Interactive Maps\n\nlast_gdf.columns\n\nIndex(['GEO_ID', 'STATE', 'state', 'LSAD', 'CENSUSAREA', 'geometry', 'fips',\n       'cases', 'deaths', 'dtc100', 'population', 'deaths100k', 'date'],\n      dtype='object')\n\n\n\nlast_gdf = last_gdf[['GEO_ID', 'STATE', 'state', 'LSAD', 'CENSUSAREA', 'geometry', 'fips',\n       'cases', 'deaths', 'dtc100', 'population', 'deaths100k']] # omit the date column\n\n\nlast_gdf.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nlast_gdf.explore(column='dtc100')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nlast_gdf.explore(column='dtc100', scheme='quantiles', k=5)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nlast_gdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nfips\ncases\ndeaths\ndtc100\npopulation\ndeaths100k\n\n\n\n\n143\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n23\n3958\n123\n3.107630\n1344212\n9.150342\n\n\n327\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n25\n118458\n8638\n7.292036\n6949503\n124.296658\n\n\n473\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n26\n91857\n6460\n7.032670\n9986857\n64.685016\n\n\n616\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n30\n4193\n61\n1.454806\n1068778\n5.707453\n\n\n767\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n32\n50270\n833\n1.657052\n3080156\n27.044085\n\n\n\n\n\n\n\n\nlast_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'])\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nimport folium\n\n\ncentroids = last_gdf.centroid\n\n/tmp/ipykernel_3348826/2951951418.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  centroids = last_gdf.centroid\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'])\ncentroids.explore(m=m)\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon')\ncentroids.explore(m=m, name='centroid')\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon', tiles='Stamen Toner')\ncentroids.explore(m=m, name='centroid')\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon', tiles='Stamen Terrain')\ncentroids.explore(m=m, name='centroid')\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon', tiles='CartoDB positron')\ncentroids.explore(m=m, name='centroid')\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nm.crs\n\n'EPSG3857'\n\n\n\nm = last_gdf.explore(column='dtc100', scheme='quantiles', k=5,\n                tooltip=['dtc100', 'state', 'fips'],\n                    name='polygon', tiles='CartoDB positron')\ncentroids.explore(m=m, name='centroid')\nfolium.Marker([32.7774, -117.0714],\n              popup='SDSU',\n              icon=folium.Icon(color='red', icon='ok-sign'),).add_to(m)\nfolium.LayerControl().add_to(m)\nm\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "lectures/week-11/2023-11-01-nearest-neighbor.html",
    "href": "lectures/week-11/2023-11-01-nearest-neighbor.html",
    "title": "Nearest Neighbor Methods",
    "section": "",
    "text": "Now that we have been introduced to the different statistical models than are used to represent point processes, we turn to the methods that are used to link observed point patterns back to the process that generated the pattern.\nMore specifically, the challenge that we face is as follows. Given an observed point pattern, we wish to make inferences about the process that generated the observed pattern.\nThe general approach that is used is to construct measures that characterise the observed point pattern, and then compare these against the proporties of the theoretical process models we explored previously.\nFor example, if we assume that the underlying process is CSR, we know what kinds of properties the empirical patterns from such a process should exhibit. The critical thing to keep in mind is that we never actually see the underlying process - we only see outcomes of the process (i.e., the pattern).\nThis raises a number of challenges that we will need to address later on, but for now we are going to build up an inituition of the general strategy for analyzing point patterns."
  },
  {
    "objectID": "lectures/week-11/2023-11-01-nearest-neighbor.html#introduction",
    "href": "lectures/week-11/2023-11-01-nearest-neighbor.html#introduction",
    "title": "Nearest Neighbor Methods",
    "section": "",
    "text": "Now that we have been introduced to the different statistical models than are used to represent point processes, we turn to the methods that are used to link observed point patterns back to the process that generated the pattern.\nMore specifically, the challenge that we face is as follows. Given an observed point pattern, we wish to make inferences about the process that generated the observed pattern.\nThe general approach that is used is to construct measures that characterise the observed point pattern, and then compare these against the proporties of the theoretical process models we explored previously.\nFor example, if we assume that the underlying process is CSR, we know what kinds of properties the empirical patterns from such a process should exhibit. The critical thing to keep in mind is that we never actually see the underlying process - we only see outcomes of the process (i.e., the pattern).\nThis raises a number of challenges that we will need to address later on, but for now we are going to build up an inituition of the general strategy for analyzing point patterns."
  },
  {
    "objectID": "lectures/week-11/2023-11-01-nearest-neighbor.html#example-patterns",
    "href": "lectures/week-11/2023-11-01-nearest-neighbor.html#example-patterns",
    "title": "Nearest Neighbor Methods",
    "section": "Example Patterns",
    "text": "Example Patterns\nTo begin we are going to create two different point patterns, one from a CSR process and one from a clustered process. We will use these two patterns to introduce the different statistical methods used to analyze the patterns. Here we are in the rare circumstance in which we actually know what process generated the pattern.\n\nCSR n=60\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(12345)\nn = 60\nxy = np.random.rand(60,2)\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\nsns.scatterplot(x='x', y='y', data=df);\n\n\n\n\n\nimport pointpats as pp\n\n\ncsr = pp.PointPattern(xy)\n\n\ncsr.summary()\n\nPoint Pattern\n60 points\nBounding rectangle [(0.00838829794155349,0.024676210429265266), (0.9940145858999619,0.9613067360728214)]\nArea of window: 0.9231676681785911\nIntensity estimate for window: 64.99361066054225\n          x         y\n0  0.929616  0.316376\n1  0.183919  0.204560\n2  0.567725  0.595545\n3  0.964515  0.653177\n4  0.748907  0.653570\n\n\n\nw = pp.Window([(0,0), (0,1), (1,1), (1,0), (0,0)])\ndraw = pp.PoissonClusterPointProcess(w, n, 2, 0.05, 1, asPP=True, conditioning=False)\ndraw.realizations[0].plot(window=True, title='Contagion Point Process (2 parents)')\n\n\n\n\n\nclustered = draw.realizations[0]\n\n\nclustered.summary()\n\nPoint Pattern\n60 points\nBounding rectangle [(0.47331760265312733,0.023178703349462502), (0.9696584457277277,0.6150208352748628)]\nArea of window: 1.0\nIntensity estimate for window: 60.0\n          x         y\n0  0.513060  0.541971\n1  0.473318  0.578385\n2  0.508373  0.536200\n3  0.881716  0.060328\n4  0.894221  0.059273"
  },
  {
    "objectID": "lectures/week-11/2023-11-01-nearest-neighbor.html#nearest-neighbor-distances",
    "href": "lectures/week-11/2023-11-01-nearest-neighbor.html#nearest-neighbor-distances",
    "title": "Nearest Neighbor Methods",
    "section": "Nearest Neighbor Distances",
    "text": "Nearest Neighbor Distances\n\nplt.scatter(csr.points.x, csr.points.y);\n\n\n\n\n\nimport networkx as nx\n\n\nG = nx.DiGraph()\nfor idx, point in enumerate(csr.points.values):\n    G.add_node(idx, pos=point)\n    \n\n\npos = nx.get_node_attributes(G, 'pos')\nnx.draw(G, pos, node_size=2)\n\n\n\n\n\nnidx, nnd = csr.knn(1) # here we have the indices of the nearest neighbors (nidx) and the distances (nnd)\n\n\nfor idx, neighbor in enumerate(nidx):\n    edge = (idx, neighbor[0])\n    G.add_edges_from([edge])\n    \n\n\npos = nx.get_node_attributes(G, 'pos')\nnx.draw(G, pos, node_size=2)\n\n\n\n\nHere we draw an arrow towards the nearest neighbor for a given observation.\nIn some cases, an observation is also the nearest neighbor to its nearest neighbor, or so-called “mutual nearest neighbors”. These pairs would appear at the the end of a segment with arrows on both ends.\nIf we look in the extreme northwest three points, we see one pair of mutual nearest neighbors, while the third point is not a mutual nearest neighbor.\nIt is also possible for a point to be a nearest neighbor to more than a single point, as is also seen in this case.\nThe nearest neighbor distances are the lengths of these segments.\n\nMean Nearest Neighbor Distance\nOur first distance based statistic was suggested by Clark and Evans (1954) as the average nearest neighbor distances:\n\\[\\bar{d}_{min} = \\frac{1}{n} \\sum_{i} d_{i, min} \\]\nwhere \\(d_{i, min}\\) is the nearest neighbor distance for observation \\(i\\), and \\(n\\) is the number of observations.\nUnder a CSR process, the expected value of this statistic is:\n\\[E[\\bar{d}_{min}] = \\frac{1}{2 \\sqrt{\\lambda}}\\]\nThe logic of the statistic is to compare the observed mean nearest neighbor distance to this expectation forming their ratio:\n\\[R = \\frac{\\bar{d}_{min}}{\\frac{1}{2 \\sqrt{\\lambda}}} = 2 \\bar{d}_{min} \\sqrt{\\lambda}\\]\nValues of \\(R&lt;1\\) are indicative of a tendancy towards clustering since the observed nearest neighbor distances are smaller than expected under CSR.\nValues of \\(R&gt;1\\) are indicative of a uniform or dispersed pattern.\n\nnnd.mean() # the mean nearest neighbor distance\n\n0.07360281110243255\n\n\n\ncsr.lambda_window # the intensity using the window for the point pattern\n\n64.99361066054225\n\n\n\ndmin = nnd.mean()\nlam = csr.lambda_window\nR = 2 * dmin * lam**(1/2)\nR\n\n1.1867513365512292\n\n\nLet’s compare this to the same statistic based on the mean nearest neighbor distance for the clustered pattern:\n\nnidx, nnd = clustered.knn(1) # here we have the indices of the nearest neighbors (nidx) and the distances (nnd)\n\n\ndmin = nnd.mean()\nlam = clustered.lambda_window\nR = 2 * dmin * lam**(1/2)\nR\n\n0.13087134840769868\n\n\nSo we see that the \\(R\\) value for the clustered pattern is much below 1, while the R value for the CSR pattern is slightly over 1.\nWhat we would like to know is if these values are significantly different from what we would expect if the underlying process that generated the patterns was CSR?\nOne approach is to use theoretical results on the distribution for the \\(R\\) statistic from Petrere (1985). The expected value of \\(R\\) is \\(E[R]=1\\). The variance of the \\(R\\) statistic is: \\[ \\sigma^2_R = \\frac{0.2732}{n}\\]\n\nimport scipy.stats\ndef R_test(pattern):\n    nidx, nnd = pattern.knn(1) # here we have the indices of the nearest neighbors (nidx) and the distances (nnd)\n    lam = pattern.lambda_window\n    R = 2 * nnd.mean() * lam**(1/2)\n    n = nnd.shape[0]\n    var = 0.2732 / n\n    se = var**(1/2)\n    stat = (R - 1 )/ se\n    p_value = scipy.stats.norm.sf(abs(stat)) * 2\n    return R, stat, p_value\n\n\nR_test(csr)\n\n(1.1867513365512292, 2.7675724348891184, 0.005647549379017388)\n\n\n\nR_test(clustered)\n\n(0.13087134840769868, -12.880103258909552, 5.825756575218341e-38)\n\n\n\n\nInference via simulation\n\nimport pointpats\nimport numpy\nsamples = pointpats.PoissonPointProcess(csr.window, n, 99, asPP=True)\n\nr_tests = np.array([R_test(samples.realizations[k]) for k in samples.realizations])\n\nr_tests\n\narray([[ 9.81105197e-01, -2.80012645e-01,  7.79467803e-01],\n       [ 9.28415089e-01, -1.06085681e+00,  2.88754981e-01],\n       [ 1.05435206e+00,  8.05473519e-01,  4.20546481e-01],\n       [ 1.11821360e+00,  1.75187335e+00,  7.97955886e-02],\n       [ 1.12387424e+00,  1.83576157e+00,  6.63929272e-02],\n       [ 1.16724422e+00,  2.47848561e+00,  1.31941433e-02],\n       [ 1.10700813e+00,  1.58581325e+00,  1.12781678e-01],\n       [ 1.09108797e+00,  1.34988348e+00,  1.77053361e-01],\n       [ 9.69002352e-01, -4.59371472e-01,  6.45967431e-01],\n       [ 1.03760497e+00,  5.57289124e-01,  5.77329905e-01],\n       [ 1.02704882e+00,  4.00851546e-01,  6.88529426e-01],\n       [ 1.10508460e+00,  1.55730742e+00,  1.19397514e-01],\n       [ 1.15793416e+00,  2.34051463e+00,  1.92571837e-02],\n       [ 1.04250723e+00,  6.29938355e-01,  5.28734918e-01],\n       [ 1.10758728e+00,  1.59439606e+00,  1.10847354e-01],\n       [ 1.07177650e+00,  1.06369612e+00,  2.87466383e-01],\n       [ 1.02551521e+00,  3.78124168e-01,  7.05338356e-01],\n       [ 9.46840660e-01, -7.87797970e-01,  4.30814888e-01],\n       [ 1.03757582e+00,  5.56857151e-01,  5.77625033e-01],\n       [ 9.90592969e-01, -1.39408044e-01,  8.89127716e-01],\n       [ 1.04856233e+00,  7.19672330e-01,  4.71726767e-01],\n       [ 1.10811705e+00,  1.60224699e+00,  1.09101003e-01],\n       [ 9.54688187e-01, -6.71501077e-01,  5.01901374e-01],\n       [ 1.10995667e+00,  1.62950930e+00,  1.03205247e-01],\n       [ 1.15162238e+00,  2.24697675e+00,  2.46415129e-02],\n       [ 9.31182120e-01, -1.01985062e+00,  3.07799310e-01],\n       [ 9.71814911e-01, -4.17690588e-01,  6.76173355e-01],\n       [ 1.14149435e+00,  2.09688392e+00,  3.60038522e-02],\n       [ 1.07165594e+00,  1.06190948e+00,  2.88276781e-01],\n       [ 1.14437709e+00,  2.13960485e+00,  3.23867145e-02],\n       [ 9.26536025e-01, -1.08870370e+00,  2.76284570e-01],\n       [ 1.00659687e+00,  9.77627322e-02,  9.22120701e-01],\n       [ 1.19240160e+00,  2.85130688e+00,  4.35399261e-03],\n       [ 1.02219381e+00,  3.28902433e-01,  7.42229435e-01],\n       [ 1.17405296e+00,  2.57938804e+00,  9.89755372e-03],\n       [ 1.15649472e+00,  2.31918276e+00,  2.03851289e-02],\n       [ 9.45617539e-01, -8.05924083e-01,  4.20286624e-01],\n       [ 1.05893614e+00,  8.73407565e-01,  3.82440969e-01],\n       [ 1.09368529e+00,  1.38837464e+00,  1.65022993e-01],\n       [ 1.07631454e+00,  1.13094776e+00,  2.58077078e-01],\n       [ 9.78530574e-01, -3.18167420e-01,  7.50357945e-01],\n       [ 1.07064731e+00,  1.04696203e+00,  2.95117090e-01],\n       [ 1.08860485e+00,  1.31308486e+00,  1.89154355e-01],\n       [ 1.19551229e+00,  2.89740597e+00,  3.76262510e-03],\n       [ 1.07442246e+00,  1.10290799e+00,  2.70067126e-01],\n       [ 1.09651586e+00,  1.43032247e+00,  1.52624489e-01],\n       [ 1.10573862e+00,  1.56699976e+00,  1.17114749e-01],\n       [ 9.18296882e-01, -1.21080418e+00,  2.25970464e-01],\n       [ 9.75959250e-01, -3.56273307e-01,  7.21635897e-01],\n       [ 1.12270070e+00,  1.81837021e+00,  6.90075678e-02],\n       [ 1.16124927e+00,  2.38964311e+00,  1.68647518e-02],\n       [ 1.14288622e+00,  2.11751071e+00,  3.42165274e-02],\n       [ 8.95782250e-01, -1.54446109e+00,  1.22476670e-01],\n       [ 1.05289923e+00,  7.83943286e-01,  4.33073389e-01],\n       [ 1.21757563e+00,  3.22437488e+00,  1.26248010e-03],\n       [ 1.03138599e+00,  4.65126525e-01,  6.41840852e-01],\n       [ 1.07305973e+00,  1.08271294e+00,  2.78935859e-01],\n       [ 1.11578890e+00,  1.71594046e+00,  8.61729410e-02],\n       [ 9.58111837e-01, -6.20764093e-01,  5.34754852e-01],\n       [ 9.37476396e-01, -9.26572231e-01,  3.54148678e-01],\n       [ 9.63173031e-01, -5.45759439e-01,  5.85231308e-01],\n       [ 1.03085430e+00,  4.57247129e-01,  6.47493427e-01],\n       [ 1.12588430e+00,  1.86554984e+00,  6.21043732e-02],\n       [ 1.04886527e+00,  7.24161732e-01,  4.68966450e-01],\n       [ 1.11417108e+00,  1.69196511e+00,  9.06526264e-02],\n       [ 9.09443001e-01, -1.34201478e+00,  1.79591204e-01],\n       [ 1.11892848e+00,  1.76246763e+00,  7.79903206e-02],\n       [ 1.14728332e+00,  2.18267389e+00,  2.90598342e-02],\n       [ 9.66649872e-01, -4.94234183e-01,  6.21140802e-01],\n       [ 1.01975820e+00,  2.92807770e-01,  7.69669089e-01],\n       [ 1.06807716e+00,  1.00887343e+00,  3.13035340e-01],\n       [ 8.87332323e-01, -1.66968527e+00,  9.49816494e-02],\n       [ 1.06116818e+00,  9.06485480e-01,  3.64678947e-01],\n       [ 9.73221131e-01, -3.96851022e-01,  6.91477323e-01],\n       [ 1.10831264e+00,  1.60514552e+00,  1.08461783e-01],\n       [ 1.15121609e+00,  2.24095569e+00,  2.50289451e-02],\n       [ 1.06442269e+00,  9.54715861e-01,  3.39721407e-01],\n       [ 1.09045697e+00,  1.34053244e+00,  1.80072304e-01],\n       [ 1.21321146e+00,  3.15969983e+00,  1.57931759e-03],\n       [ 1.03753327e+00,  5.56226553e-01,  5.78055989e-01],\n       [ 9.92381824e-01, -1.12898015e-01,  9.10111410e-01],\n       [ 9.89597771e-01, -1.54156441e-01,  8.77486386e-01],\n       [ 8.69471915e-01, -1.93436865e+00,  5.30678183e-02],\n       [ 1.07456759e+00,  1.10505883e+00,  2.69134096e-01],\n       [ 1.01501782e+00,  2.22557427e-01,  8.23879974e-01],\n       [ 1.06339883e+00,  9.39542766e-01,  3.47452146e-01],\n       [ 1.04251892e+00,  6.30111598e-01,  5.28621572e-01],\n       [ 1.01595310e+00,  2.36417893e-01,  8.13108413e-01],\n       [ 9.76030325e-01, -3.55220016e-01,  7.22424770e-01],\n       [ 1.02700974e+00,  4.00272457e-01,  6.88955852e-01],\n       [ 1.05167838e+00,  7.65850808e-01,  4.43765079e-01],\n       [ 1.03268084e+00,  4.84315702e-01,  6.28161834e-01],\n       [ 1.07237329e+00,  1.07254022e+00,  2.83477458e-01],\n       [ 1.17864464e+00,  2.64743472e+00,  8.11050176e-03],\n       [ 1.00495856e+00,  7.34836527e-02,  9.41421252e-01],\n       [ 9.57599581e-01, -6.28355512e-01,  5.29771074e-01],\n       [ 1.01309195e+00,  1.94016950e-01,  8.46162610e-01],\n       [ 1.06834012e+00,  1.01277043e+00,  3.11169828e-01],\n       [ 1.04123161e+00,  6.11034274e-01,  5.41176890e-01]])\n\n\n\nR_csr = R_test(csr)\n\n\nR_csr[0]\n\n1.1867513365512292\n\n\n\n(r_tests[:,0] &gt;= R_csr[0]).sum()\n\n4\n\n\n\ndf = pd.DataFrame(data=r_tests, columns=['R', 'z', 'p'])\n\nsns.displot(df, kind='kde', x=\"R\")\nplt.axvline(R_csr[0], 0, 0.1, color='g');\n\n\n\n\n\nR_clustered = R_test(clustered)\n\n\ndf = pd.DataFrame(data=r_tests, columns=['R', 'z', 'p'])\n\nsns.displot(df, kind='kde', x=\"R\")\nplt.axvline(R_csr[0], 0, 0.1, color='g');\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nR\nz\np\n\n\n\n\ncount\n99.000000\n99.000000\n99.000000\n\n\nmean\n1.052286\n0.774857\n0.357230\n\n\nstd\n0.078631\n1.165280\n0.282182\n\n\nmin\n0.869472\n-1.934369\n0.001262\n\n\n25%\n0.991487\n-0.126153\n0.099093\n\n\n50%\n1.054352\n0.805474\n0.295117\n\n\n75%\n1.108215\n1.603696\n0.581644\n\n\nmax\n1.217576\n3.224375\n0.941421\n\n\n\n\n\n\n\n\n\nsamples = pointpats.PoissonPointProcess(csr.window, n, 999, asPP=True)\n\nr_tests = np.array([R_test(samples.realizations[k]) for k in samples.realizations])\n\nr_tests\n\narray([[ 1.04108274,  0.60882811,  0.54263838],\n       [ 1.08726785,  1.2932711 ,  0.19591731],\n       [ 0.96240692, -0.5571128 ,  0.57745036],\n       ...,\n       [ 1.18467435,  2.73679237,  0.00620414],\n       [ 1.05998867,  0.88900566,  0.37400004],\n       [ 0.92024492, -1.18193507,  0.23723146]])\n\n\n\ndf = pd.DataFrame(data=r_tests, columns=['R', 'z', 'p'])\n\nsns.displot(df, kind='kde', x=\"R\")\nplt.axvline(R_csr[0], 0, 0.1, color='g');\nplt.axvline(R_clustered[0], 0, 0.1, color='r');\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nR\nz\np\n\n\n\n\ncount\n999.000000\n999.000000\n999.000000\n\n\nmean\n1.057947\n0.858756\n0.357294\n\n\nstd\n0.077832\n1.153438\n0.296165\n\n\nmin\n0.805327\n-2.884969\n0.000001\n\n\n25%\n1.004814\n0.071334\n0.092286\n\n\n50%\n1.061867\n0.916842\n0.283196\n\n\n75%\n1.111235\n1.648461\n0.577714\n\n\nmax\n1.328334\n4.865765\n0.998563\n\n\n\n\n\n\n\n\n!pwd\n\n/home/serge/para/1_projects/course_385_spatial_data_analysis_f23/385f23/385f23/lectures/week-11"
  },
  {
    "objectID": "lectures/week-11/2023-10-30-quadrat-statistics.html",
    "href": "lectures/week-11/2023-10-30-quadrat-statistics.html",
    "title": "Quadrat Statistics",
    "section": "",
    "text": "Now that we have been introduced to the different statistical models than are used to represent point processes, we turn to the methods that are used to link observed point patterns back to the process that generated the pattern.\nMore specifically, the challenge that we face is as follows. Given an observed point pattern, we wish to make inferences about the process that generated the observed pattern.\nThe general approach that is used is to construct measures that characterise the observed point pattern, and then compare these against the proporties of the theoretical process models we explored previously.\nFor example, if we assume that the underlying process is CSR, we know what kinds of properties the empirical patterns from such a process should exhibit. The critical thing to keep in mind is that we never actually see the underlying process - we only see outcomes of the process (i.e., the pattern).\nThis raises a number of challenges that we will need to address later on, but for now we are going to build up an inituition of the general strategy for analyzing point patterns."
  },
  {
    "objectID": "lectures/week-11/2023-10-30-quadrat-statistics.html#introduction",
    "href": "lectures/week-11/2023-10-30-quadrat-statistics.html#introduction",
    "title": "Quadrat Statistics",
    "section": "",
    "text": "Now that we have been introduced to the different statistical models than are used to represent point processes, we turn to the methods that are used to link observed point patterns back to the process that generated the pattern.\nMore specifically, the challenge that we face is as follows. Given an observed point pattern, we wish to make inferences about the process that generated the observed pattern.\nThe general approach that is used is to construct measures that characterise the observed point pattern, and then compare these against the proporties of the theoretical process models we explored previously.\nFor example, if we assume that the underlying process is CSR, we know what kinds of properties the empirical patterns from such a process should exhibit. The critical thing to keep in mind is that we never actually see the underlying process - we only see outcomes of the process (i.e., the pattern).\nThis raises a number of challenges that we will need to address later on, but for now we are going to build up an inituition of the general strategy for analyzing point patterns."
  },
  {
    "objectID": "lectures/week-11/2023-10-30-quadrat-statistics.html#example-patterns",
    "href": "lectures/week-11/2023-10-30-quadrat-statistics.html#example-patterns",
    "title": "Quadrat Statistics",
    "section": "Example Patterns",
    "text": "Example Patterns\nTo begin we are going to create two different point patterns, one from a CSR process and one from a clustered process. We will use these two patterns to introduce the different statistical methods used to analyze the patterns. Here we are in the rare circumstance in which we actually know what process generated the pattern.\n\nCSR n=60\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(12345)\nn = 60\nxy = np.random.rand(60,2)\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\nsns.scatterplot(x='x', y='y', data=df);\n\n\n\n\n\nimport pointpats as pp\n\n\ncsr = pp.PointPattern(xy)\n\n\ncsr.summary()\n\nPoint Pattern\n60 points\nBounding rectangle [(0.00838829794155349,0.024676210429265266), (0.9940145858999619,0.9613067360728214)]\nArea of window: 0.9231676681785911\nIntensity estimate for window: 64.99361066054225\n          x         y\n0  0.929616  0.316376\n1  0.183919  0.204560\n2  0.567725  0.595545\n3  0.964515  0.653177\n4  0.748907  0.653570\n\n\n\nw = pp.Window([(0,0), (0,1), (1,1), (1,0), (0,0)])\ndraw = pp.PoissonClusterPointProcess(w, n, 2, 0.05, 1, asPP=True, conditioning=False)\ndraw.realizations[0].plot(window=True, title='Contagion Point Process (2 parents)')\n\n\n\n\n\nclustered = draw.realizations[0]\n\n\nclustered.summary()\n\nPoint Pattern\n60 points\nBounding rectangle [(0.47331760265312733,0.023178703349462502), (0.9696584457277277,0.6150208352748628)]\nArea of window: 1.0\nIntensity estimate for window: 60.0\n          x         y\n0  0.513060  0.541971\n1  0.473318  0.578385\n2  0.508373  0.536200\n3  0.881716  0.060328\n4  0.894221  0.059273"
  },
  {
    "objectID": "lectures/week-11/2023-10-30-quadrat-statistics.html#quadrat-statistics",
    "href": "lectures/week-11/2023-10-30-quadrat-statistics.html#quadrat-statistics",
    "title": "Quadrat Statistics",
    "section": "Quadrat Statistics",
    "text": "Quadrat Statistics\n\nimport pointpats.quadrat_statistics as qs\n\n\ncsr_qr = qs.QStatistic(csr, shape='rectangle', nx=3, ny=3)\ncsr_qr.plot()\n\n&lt;Axes: title={'center': 'Quadrat Count'}&gt;\n\n\n\n\n\n\ncsr_qr.chi2\n\n10.8\n\n\n\ncsr_qr.chi2_pvalue\n\n0.21329101843394052\n\n\n\nclustered_qr = qs.QStatistic(clustered, shape='rectangle', nx=3, ny=3)\nclustered_qr.plot()\n\n&lt;Axes: title={'center': 'Quadrat Count'}&gt;\n\n\n\n\n\n\nclustered_qr.chi2\n\n209.99999999999994\n\n\n\nclustered_qr.chi2_pvalue\n\n4.976940117448032e-41"
  },
  {
    "objectID": "lectures/week-11/2023-10-30-quadrat-statistics.html#quadrat-shape",
    "href": "lectures/week-11/2023-10-30-quadrat-statistics.html#quadrat-shape",
    "title": "Quadrat Statistics",
    "section": "Quadrat Shape",
    "text": "Quadrat Shape\n\ncsr_qr_hex = qs.QStatistic(csr, shape='hexagon', lh=0.2)\n\n\ncsr_qr_hex.plot()\n\n&lt;Axes: title={'center': 'Quadrat Count'}&gt;\n\n\n\n\n\n\ncsr_qr_hex.chi2\n\n20.733333333333334\n\n\n\ncsr_qr_hex.df\n\n13\n\n\n\ncsr_qr_hex.chi2_pvalue\n\n0.07837318677184964\n\n\n\nclustered_qr_hex = qs.QStatistic(clustered, shape='hexagon', lh=0.2)\n\n\nclustered_qr_hex.plot()\n\n&lt;Axes: title={'center': 'Quadrat Count'}&gt;\n\n\n\n\n\n\nclustered_qr_hex.chi2_pvalue\n\n2.005280432194402e-14"
  },
  {
    "objectID": "lectures/week-11/2023-10-30-quadrat-statistics.html#simulation-inference",
    "href": "lectures/week-11/2023-10-30-quadrat-statistics.html#simulation-inference",
    "title": "Quadrat Statistics",
    "section": "Simulation Inference",
    "text": "Simulation Inference\n\nfrom pointpats import PoissonPointProcess as poissonpp\n\n\nclustered_sim = poissonpp(clustered.window, clustered.n, 999, asPP=True)\n\n\nclustered_sim_hex = qs.QStatistic(clustered, shape='hexagon',\n                                  lh=0.2,\n                                  realizations=clustered_sim)\n\n\nclustered_sim_hex.chi2_r_pvalue\n\n0.001\n\n\n\nclustered_sim_hex.chi2_realizations\n\narray([26.8       , 18.86666667, 20.26666667, 19.8       , 15.6       ,\n       30.53333333, 15.6       , 28.2       , 23.53333333, 17.        ,\n       35.66666667, 11.4       , 19.33333333, 21.66666667, 18.4       ,\n       15.6       , 17.46666667, 24.93333333, 17.        , 20.73333333,\n       19.33333333, 15.6       , 38.        , 38.        , 16.06666667,\n       24.46666667, 21.2       , 21.2       ,  7.66666667, 18.86666667,\n       17.46666667, 24.93333333, 15.6       , 25.4       , 14.66666667,\n       26.8       , 21.2       , 18.86666667, 14.66666667, 24.        ,\n       11.86666667, 10.        , 17.        , 17.        ,  9.06666667,\n       31.93333333, 24.93333333, 31.        , 21.2       , 17.93333333,\n       11.4       , 17.        , 21.2       , 11.86666667, 21.2       ,\n       19.33333333, 19.8       , 17.        , 38.        , 31.        ,\n       34.73333333, 16.53333333, 33.8       , 17.93333333, 18.86666667,\n       10.        ,  9.53333333, 15.6       , 25.4       , 14.66666667,\n       24.46666667, 24.93333333, 18.4       , 14.2       , 24.        ,\n       21.2       , 23.06666667, 21.2       , 19.33333333, 37.06666667,\n       15.6       , 29.6       , 31.        , 20.26666667, 23.06666667,\n       20.73333333, 32.86666667, 19.33333333, 22.6       , 15.6       ,\n       13.26666667, 46.4       , 15.6       , 20.26666667, 33.33333333,\n       11.4       , 17.        , 26.33333333, 17.93333333, 22.13333333,\n       17.        , 33.33333333, 24.93333333, 12.33333333, 28.66666667,\n       42.2       , 17.        , 22.6       , 16.53333333, 22.6       ,\n       12.33333333, 15.6       , 18.4       , 17.        , 17.46666667,\n       30.06666667, 26.8       , 17.        , 24.46666667, 21.66666667,\n       21.66666667, 20.26666667, 17.93333333, 18.4       , 28.2       ,\n       29.13333333, 14.2       , 26.33333333,  9.06666667, 20.26666667,\n       21.66666667, 27.26666667, 22.6       , 22.13333333, 21.66666667,\n       21.2       , 10.93333333, 21.66666667, 22.13333333, 33.33333333,\n       11.4       , 15.13333333, 22.6       , 17.93333333, 18.86666667,\n       22.13333333, 17.        , 10.93333333, 16.53333333, 14.2       ,\n       19.33333333, 14.66666667, 16.53333333, 16.06666667, 15.13333333,\n       36.6       , 10.        , 14.66666667, 17.46666667, 12.33333333,\n       45.        , 26.33333333, 16.06666667, 21.2       , 32.4       ,\n       22.13333333, 23.53333333, 37.06666667, 22.13333333, 28.2       ,\n       27.73333333, 12.8       , 20.73333333, 18.4       , 16.06666667,\n       24.93333333, 11.4       , 20.73333333, 21.2       , 17.        ,\n       24.        , 13.73333333, 37.53333333, 18.86666667, 15.13333333,\n       15.13333333, 27.73333333, 16.06666667, 32.4       , 20.73333333,\n       33.8       , 24.46666667, 10.46666667, 27.73333333, 18.86666667,\n       15.13333333, 21.2       , 25.86666667, 15.6       , 25.4       ,\n       11.4       , 21.2       , 27.73333333, 20.73333333, 23.53333333,\n       30.06666667, 17.        , 17.46666667, 19.8       , 19.33333333,\n       22.6       , 13.26666667, 32.4       , 29.6       , 37.53333333,\n       21.2       , 20.26666667, 17.        , 23.06666667, 26.33333333,\n       26.33333333, 17.        , 39.86666667, 16.06666667, 24.93333333,\n       20.26666667, 13.26666667, 20.73333333, 28.66666667, 23.53333333,\n       19.33333333, 19.8       , 48.26666667, 19.8       , 16.06666667,\n       26.33333333, 20.26666667, 10.46666667, 15.13333333, 16.06666667,\n       23.53333333, 26.33333333, 15.6       , 23.53333333, 19.33333333,\n        6.26666667, 27.26666667, 27.73333333, 17.        , 32.4       ,\n       22.6       , 13.73333333, 28.66666667, 20.73333333, 17.46666667,\n       20.73333333, 31.        , 22.6       , 19.33333333, 23.53333333,\n       24.        , 18.4       , 16.06666667, 21.66666667, 23.53333333,\n       12.8       , 31.        , 14.2       , 16.53333333,  8.6       ,\n       14.66666667, 27.26666667, 17.46666667, 20.73333333, 31.93333333,\n       28.2       , 24.        , 27.73333333, 24.46666667, 30.53333333,\n       17.46666667, 15.13333333, 26.8       , 36.6       , 29.6       ,\n       32.86666667, 11.86666667, 15.13333333, 14.2       , 19.8       ,\n       24.46666667, 31.93333333, 35.66666667, 18.86666667, 13.73333333,\n       14.66666667, 14.2       , 23.06666667, 21.2       , 16.53333333,\n       31.93333333, 18.4       , 10.93333333, 25.4       , 21.66666667,\n       21.66666667, 20.73333333, 29.6       , 26.33333333, 33.33333333,\n       18.86666667, 13.73333333, 23.06666667, 20.26666667, 14.2       ,\n       27.73333333, 36.6       , 16.06666667, 18.86666667, 14.2       ,\n       25.4       , 27.26666667, 21.2       , 18.86666667, 24.        ,\n       28.66666667, 30.06666667, 19.8       , 24.        , 17.46666667,\n       24.        , 14.2       , 18.86666667, 24.        , 22.13333333,\n       23.53333333, 30.53333333, 37.06666667, 22.6       , 21.66666667,\n       23.53333333, 19.8       , 20.73333333, 19.8       , 21.2       ,\n       25.86666667, 27.73333333, 14.2       , 30.06666667, 17.        ,\n       17.        , 23.06666667, 30.06666667, 29.6       , 10.93333333,\n       35.2       , 30.53333333, 17.        , 17.46666667, 41.26666667,\n       21.66666667, 15.13333333, 24.46666667, 28.2       , 31.        ,\n       17.46666667, 29.13333333, 19.8       , 17.93333333, 15.13333333,\n       17.46666667, 25.4       , 17.93333333, 30.06666667, 16.06666667,\n       27.73333333, 14.2       , 15.13333333, 20.26666667, 36.13333333,\n       24.46666667, 25.4       , 25.4       , 36.13333333, 15.6       ,\n       20.73333333, 23.53333333, 14.2       , 25.86666667, 18.86666667,\n       25.4       , 18.4       , 23.53333333, 14.66666667, 17.        ,\n       22.13333333, 23.53333333, 18.4       , 20.26666667, 28.2       ,\n       20.26666667, 40.33333333, 28.2       , 37.06666667, 17.46666667,\n       31.        , 20.26666667, 26.8       , 35.2       , 14.66666667,\n       23.06666667, 23.06666667, 13.73333333, 35.2       , 23.53333333,\n       31.        , 31.93333333, 17.46666667, 38.        , 16.53333333,\n       31.46666667, 34.26666667, 22.13333333, 22.6       , 20.73333333,\n       16.53333333, 17.93333333, 29.6       , 10.46666667, 27.26666667,\n       18.86666667, 22.6       , 36.6       , 25.4       , 15.6       ,\n       15.13333333, 18.86666667, 19.8       , 29.6       , 22.6       ,\n       13.73333333, 46.86666667, 35.2       , 14.2       , 25.4       ,\n       41.73333333, 25.4       , 13.73333333, 23.53333333, 26.8       ,\n       25.4       , 16.06666667, 21.66666667, 37.53333333, 30.06666667,\n        7.2       , 15.6       , 31.46666667, 17.        , 35.66666667,\n       23.53333333, 19.33333333, 17.46666667, 24.93333333, 19.8       ,\n       13.26666667, 25.4       , 17.46666667, 24.46666667, 20.73333333,\n       33.33333333, 20.26666667, 26.33333333, 16.06666667, 21.2       ,\n       21.2       , 56.2       , 27.26666667, 26.8       , 21.66666667,\n       22.13333333,  7.66666667, 22.6       ,  8.6       , 17.93333333,\n       25.86666667, 30.53333333, 28.66666667, 24.93333333, 13.73333333,\n        9.53333333, 21.2       , 17.93333333, 32.4       , 24.        ,\n       24.46666667, 34.73333333, 31.46666667, 23.06666667, 17.93333333,\n       30.53333333, 15.13333333, 22.13333333, 19.8       , 17.        ,\n       25.4       , 18.86666667, 26.33333333, 19.33333333, 32.4       ,\n        9.53333333, 21.66666667, 11.86666667, 31.46666667, 30.06666667,\n       24.93333333, 24.46666667, 17.46666667, 21.66666667, 19.33333333,\n       23.06666667, 23.53333333, 23.53333333, 33.33333333, 22.6       ,\n       18.4       , 24.        , 23.53333333, 16.53333333, 19.8       ,\n       17.        , 24.93333333, 10.93333333, 30.53333333, 13.73333333,\n       16.06666667, 19.33333333, 17.93333333, 26.8       , 21.2       ,\n       19.8       , 40.33333333, 18.86666667, 10.46666667, 23.53333333,\n       16.53333333, 24.        , 21.2       , 19.33333333, 31.46666667,\n       12.33333333, 10.93333333, 16.53333333, 25.4       , 33.33333333,\n       33.33333333, 20.73333333, 12.8       , 20.26666667, 34.26666667,\n       33.33333333, 29.13333333, 18.4       , 16.53333333, 20.73333333,\n       22.13333333, 22.6       , 22.6       , 17.93333333, 10.        ,\n       14.66666667, 14.66666667, 16.06666667, 16.53333333, 21.2       ,\n       10.93333333, 17.46666667, 31.46666667, 14.2       , 18.86666667,\n       24.        , 10.46666667, 30.06666667, 25.4       , 25.4       ,\n       15.13333333, 24.93333333, 22.6       , 17.93333333, 12.8       ,\n       20.73333333, 10.46666667, 12.8       , 32.86666667, 23.53333333,\n       21.2       , 23.06666667, 17.        , 13.73333333, 16.53333333,\n       24.46666667, 14.2       , 38.46666667, 26.33333333, 15.6       ,\n       20.26666667, 31.93333333, 21.66666667, 18.86666667, 15.13333333,\n       20.26666667, 31.46666667, 25.4       , 17.        , 26.8       ,\n       24.46666667, 14.2       , 42.2       , 22.6       , 17.46666667,\n       20.26666667, 35.66666667, 26.8       , 24.93333333, 31.46666667,\n       16.53333333, 24.        , 23.06666667, 17.93333333, 31.46666667,\n       30.06666667, 20.26666667, 11.2       , 23.06666667, 38.93333333,\n       13.73333333, 17.93333333, 24.        , 25.4       , 25.86666667,\n       24.        , 18.86666667, 22.13333333, 21.2       , 11.86666667,\n       18.86666667, 14.2       , 22.6       , 32.4       , 18.4       ,\n       24.93333333, 18.86666667, 24.93333333, 10.46666667, 24.93333333,\n       23.06666667, 15.6       , 21.66666667, 23.53333333, 20.73333333,\n       24.93333333, 25.86666667, 12.8       , 20.73333333, 25.4       ,\n       19.8       , 34.73333333, 30.53333333, 33.8       , 17.        ,\n       25.4       , 28.66666667, 18.4       , 30.06666667, 19.8       ,\n       52.93333333, 19.33333333, 25.86666667, 19.33333333, 10.93333333,\n       18.4       , 28.66666667,  7.66666667, 20.73333333, 43.13333333,\n       22.6       , 24.46666667, 23.53333333, 25.4       , 12.33333333,\n       26.8       , 22.6       , 22.13333333, 26.33333333, 31.        ,\n       29.13333333, 18.86666667, 20.26666667, 22.13333333, 16.53333333,\n       22.6       , 12.8       , 37.06666667, 23.06666667, 31.93333333,\n       10.        , 16.53333333, 14.2       , 26.8       , 18.4       ,\n       16.06666667, 22.6       , 13.26666667, 19.33333333, 21.66666667,\n       15.6       , 16.06666667, 19.8       , 26.33333333, 21.66666667,\n       25.86666667, 23.06666667, 16.53333333,  8.6       , 14.66666667,\n       23.06666667, 15.13333333, 15.6       , 14.66666667, 29.6       ,\n       13.73333333, 19.8       , 17.        , 19.8       , 39.86666667,\n       31.        , 12.8       , 24.93333333, 31.        , 23.06666667,\n       21.66666667, 21.66666667, 17.46666667, 20.73333333, 19.8       ,\n       25.4       , 21.2       , 19.8       , 11.86666667, 31.        ,\n       19.8       , 19.33333333, 13.26666667, 33.8       , 18.86666667,\n       46.4       , 24.        , 16.53333333, 16.53333333, 31.46666667,\n       23.53333333, 26.33333333, 25.86666667, 31.93333333, 21.2       ,\n       21.66666667, 20.73333333, 29.6       , 24.        , 38.46666667,\n        7.66666667, 17.        , 13.73333333,  6.26666667, 20.26666667,\n       36.13333333, 21.66666667, 23.53333333, 25.86666667, 18.4       ,\n       14.2       , 15.6       , 20.73333333, 34.73333333, 26.8       ,\n       21.66666667, 24.93333333, 29.13333333, 31.93333333, 25.86666667,\n       14.2       , 12.33333333, 17.46666667, 24.        , 17.        ,\n       27.26666667, 17.46666667, 17.        , 34.26666667, 12.8       ,\n       29.6       , 24.93333333, 15.13333333, 13.73333333, 34.26666667,\n       18.86666667, 30.06666667, 19.8       , 37.53333333, 26.8       ,\n       19.33333333, 17.93333333, 22.6       , 16.53333333, 12.33333333,\n       20.73333333, 17.46666667, 32.4       , 10.46666667, 33.8       ,\n       21.66666667, 18.86666667, 26.8       , 32.86666667, 41.26666667,\n       27.73333333, 16.06666667, 24.46666667, 15.6       , 19.33333333,\n       23.53333333, 19.33333333, 12.8       , 21.66666667, 12.8       ,\n       24.46666667, 36.13333333, 17.        , 18.86666667, 16.06666667,\n       17.93333333, 33.33333333, 10.93333333, 14.66666667, 11.4       ,\n       18.4       , 31.46666667, 24.46666667, 20.73333333, 40.8       ,\n       16.06666667, 20.73333333, 15.6       , 26.8       , 14.66666667,\n       31.46666667, 20.26666667, 24.46666667, 11.86666667, 17.46666667,\n       24.93333333,  6.73333333, 17.93333333, 24.46666667, 18.4       ,\n       14.2       , 26.8       , 24.        , 40.33333333, 16.53333333,\n       47.33333333, 23.53333333, 20.73333333, 14.2       , 23.53333333,\n       18.86666667, 13.73333333, 31.        , 21.66666667, 22.6       ,\n       11.4       , 24.46666667, 32.86666667, 43.13333333, 22.6       ,\n       19.8       , 36.6       , 28.2       , 23.06666667, 29.13333333,\n       15.13333333, 17.        , 28.2       , 34.26666667, 25.86666667,\n       13.26666667, 32.4       ,  8.6       , 29.13333333, 26.33333333,\n       28.66666667, 15.6       , 19.8       , 17.        , 12.8       ,\n       23.06666667, 16.06666667, 30.06666667, 20.26666667, 19.8       ,\n       22.6       , 21.66666667, 25.86666667, 36.6       , 15.6       ,\n       24.46666667, 15.13333333, 19.33333333, 10.46666667, 26.33333333,\n       18.86666667, 23.06666667, 24.46666667, 22.13333333, 29.6       ,\n       17.46666667, 23.06666667, 16.        , 23.06666667, 28.66666667,\n       20.26666667, 17.93333333, 17.        , 26.33333333, 21.66666667,\n       13.73333333, 17.93333333, 16.06666667, 21.66666667, 26.8       ,\n       18.86666667, 24.93333333, 21.66666667, 34.26666667, 23.06666667,\n       23.06666667, 12.8       , 15.6       , 13.73333333, 35.2       ,\n       26.8       , 25.4       , 19.33333333, 25.4       , 22.6       ,\n       18.86666667, 23.06666667, 29.6       , 13.73333333, 13.26666667,\n       30.06666667, 31.93333333, 12.8       , 33.8       , 11.4       ,\n       15.13333333, 15.6       , 36.13333333, 16.06666667, 13.73333333,\n       42.2       , 14.2       , 30.06666667, 24.46666667, 45.93333333,\n       29.6       , 17.93333333, 29.13333333, 29.6       , 38.46666667,\n       17.93333333, 19.33333333, 12.8       , 22.13333333, 17.93333333,\n       19.33333333, 14.2       , 24.46666667, 14.66666667, 21.2       ,\n       29.6       , 18.4       , 20.73333333, 20.26666667, 22.6       ,\n       17.        , 33.33333333, 29.6       , 31.46666667])"
  },
  {
    "objectID": "lectures/week-11/2023-10-30-quadrat-statistics.html#influence-of-bounding-extent",
    "href": "lectures/week-11/2023-10-30-quadrat-statistics.html#influence-of-bounding-extent",
    "title": "Quadrat Statistics",
    "section": "Influence of Bounding Extent",
    "text": "Influence of Bounding Extent\n\nimport libpysal\nfrom libpysal.cg import shapely_ext\nimport geopandas\nimport numpy as np\n\n\nfrom pointpats import Window\n\n\nva = libpysal.io.open(libpysal.examples.get_path(\"vautm17n.shp\"))\n\n\npolys = [shp for shp in va]\nstate = shapely_ext.cascaded_union(polys)\n\n\nwindow = Window(state.parts)\n\n\nnp.random.seed(5)\n\n\nsamples = poissonpp(window, 200, 1, conditioning=False, asPP=False)\n\n\nsamples.realizations[0]\n\narray([[ 414659.62831296, 4098843.62306827],\n       [ 944877.32665295, 4064524.51881247],\n       [ 624464.14012849, 4135522.50140932],\n       [ 716322.44071544, 4158399.44765818],\n       [ 831143.93926953, 4237580.13537067],\n       [ 400509.95328193, 4096900.41002516],\n       [ 810686.07255281, 4283013.52195582],\n       [ 589382.5482221 , 4207114.63468694],\n       [ 464834.86653763, 4099680.23066161],\n       [ 729016.86342122, 4295967.88710233],\n       [ 692558.78034025, 4104007.16031022],\n       [ 707522.86530594, 4083889.38215422],\n       [ 368068.7091872 , 4091422.04350303],\n       [ 781939.22465452, 4097373.59625561],\n       [ 690938.87384332, 4295945.27613892],\n       [ 737215.08721821, 4168650.77104844],\n       [ 610370.4068341 , 4121670.2849925 ],\n       [ 557829.48604931, 4085935.58930553],\n       [ 866763.76550655, 4111032.57384731],\n       [ 834827.89687458, 4166256.04344958],\n       [ 621872.14784085, 4116134.29168132],\n       [ 731234.98798312, 4131270.48035054],\n       [ 682313.43606906, 4223967.01711812],\n       [ 733908.93745755, 4275494.4664081 ],\n       [ 529604.21979282, 4140192.20505708],\n       [ 667623.24339624, 4231046.01435233],\n       [ 852876.94714772, 4144452.31221088],\n       [ 736531.73670527, 4298186.25041488],\n       [ 628191.02243473, 4102712.26100424],\n       [ 695286.19214258, 4089811.49187367],\n       [ 335791.24699685, 4071456.16300708],\n       [ 976525.54090453, 4190170.9356442 ],\n       [ 269907.7423385 , 4053819.55125621],\n       [ 901005.55219154, 4135790.38279535],\n       [ 343450.39084153, 4069350.54999132],\n       [ 616962.39647439, 4060407.85366255],\n       [ 856890.64094909, 4088171.88687401],\n       [ 784246.96302866, 4158561.00287435],\n       [ 506667.94859104, 4057344.67213736],\n       [ 652994.7682644 , 4222285.369847  ],\n       [ 752892.56374042, 4051208.83003112],\n       [ 925790.59890195, 4067279.1569195 ],\n       [ 979545.31057954, 4216138.61394589],\n       [ 834141.62958798, 4078931.40609538],\n       [ 826072.78416898, 4141338.71410948],\n       [ 789540.24165872, 4134439.70394346],\n       [ 832204.18186126, 4197953.71447929],\n       [ 425655.57604877, 4089467.13633862],\n       [ 864217.85281509, 4188693.47802216],\n       [ 862328.44401525, 4076592.74052587],\n       [ 867441.18732928, 4116652.71820767],\n       [ 699661.2534777 , 4307801.55307259],\n       [ 751829.65031639, 4319968.92400562],\n       [ 693363.69978698, 4164890.1129782 ],\n       [ 746506.9340749 , 4128752.07645258],\n       [ 732083.67983923, 4248326.90577783],\n       [ 653338.32075661, 4156598.67265619],\n       [ 976606.02276802, 4197597.40929845],\n       [ 848350.52448247, 4064866.02614159],\n       [ 630634.31029898, 4214285.994748  ],\n       [ 710542.99205501, 4142311.94640686],\n       [ 818235.01391031, 4101202.27810456],\n       [ 823663.1391133 , 4155899.63963564],\n       [ 664444.47776103, 4236228.98363075],\n       [ 360769.23263894, 4085354.33832084],\n       [ 766550.94064044, 4292018.87615081],\n       [ 708574.49400057, 4316489.95362822],\n       [ 678464.74663622, 4157026.01869247],\n       [ 669386.8193962 , 4168943.73102981],\n       [ 739117.58013062, 4238904.58741115],\n       [ 802008.45139164, 4058821.42801494],\n       [ 719353.5399807 , 4310434.15086711],\n       [ 699010.43647616, 4235885.22615002],\n       [ 711302.81748723, 4189425.14715171],\n       [ 711004.15283455, 4307013.13265628],\n       [ 443641.20854179, 4112502.92242971],\n       [ 549017.70273327, 4122109.33477415],\n       [ 633880.98332554, 4180721.53522785],\n       [ 761505.3618409 , 4337608.44051833],\n       [ 628113.51741034, 4248999.17523924],\n       [ 612085.42234894, 4193531.31241006],\n       [ 773003.9526705 , 4191729.03945972],\n       [ 869260.59923287, 4095817.64598321],\n       [ 429695.78427436, 4083168.63837101],\n       [ 411407.60297276, 4095291.64838574],\n       [ 774204.90698235, 4114220.83806725],\n       [ 592778.43642512, 4048923.19755613],\n       [ 384376.48425827, 4088331.25454645],\n       [ 947440.38984378, 4151004.37886308],\n       [ 767483.69263252, 4178399.29510034],\n       [ 690935.63144369, 4235036.15530389],\n       [ 576980.01519074, 4070580.86201211],\n       [ 712493.43738419, 4089094.16426798],\n       [ 519079.03725716, 4062389.09073365],\n       [ 594894.13977775, 4126627.3577412 ],\n       [ 802962.14790058, 4255158.21686279],\n       [ 641802.18696761, 4198983.48922481],\n       [ 726954.63054571, 4344401.63706861],\n       [ 821460.13878318, 4132750.73296464],\n       [ 837220.98889414, 4222136.55345996],\n       [ 563543.98825404, 4153613.9629347 ],\n       [ 814038.74000601, 4093096.42208192],\n       [ 697273.58284353, 4163467.0010079 ],\n       [ 629072.6884094 , 4213295.94973238],\n       [ 819817.84709794, 4102394.60580934],\n       [ 526160.27728639, 4104037.23601285],\n       [ 836763.95559331, 4129516.28447547],\n       [ 736374.59850277, 4352644.80670343],\n       [ 841168.99204432, 4138747.2216095 ],\n       [ 751403.55221945, 4199922.61169574],\n       [ 557689.70136621, 4058695.77127365],\n       [ 878737.13093459, 4069134.69802166],\n       [ 645585.19035458, 4215659.45070671],\n       [ 848301.94947934, 4240704.72062012],\n       [ 811185.54323789, 4246760.16720487],\n       [ 802385.39914521, 4152499.83940512],\n       [ 967083.70536616, 4187628.68689018],\n       [ 566173.41485163, 4171863.51737594],\n       [ 695607.47904146, 4288180.87390924],\n       [ 677616.25847172, 4050830.91542181],\n       [ 861869.68527996, 4210372.43779312],\n       [ 654780.64776662, 4111656.98283347],\n       [ 585233.58031201, 4074992.10261737],\n       [ 781434.81304468, 4058413.05158421],\n       [ 822682.84769376, 4213417.77129642],\n       [ 768900.63289125, 4088226.44858065],\n       [ 791899.72825801, 4245563.76587488],\n       [ 831576.99118498, 4312162.65433444],\n       [ 878388.72674408, 4062578.7934681 ],\n       [ 823191.58958438, 4196021.45946538],\n       [ 378156.77607172, 4068466.52709272],\n       [ 862070.43546301, 4193252.59919724],\n       [ 817515.77267352, 4251166.78629974],\n       [ 690730.22340288, 4134039.36745731],\n       [ 582428.37056692, 4142295.17500245],\n       [ 701803.27707921, 4257761.15121192],\n       [ 794450.97333767, 4082458.53574423],\n       [ 683956.9175291 , 4263206.01583531],\n       [ 810304.08829204, 4189035.31606198],\n       [ 626831.94796862, 4183736.79813135],\n       [ 447925.51227984, 4053619.31963374],\n       [ 720176.25306412, 4080228.49511975],\n       [ 671706.30020031, 4057219.61544144],\n       [ 737091.10397116, 4186517.38432557],\n       [ 621943.7767598 , 4230899.1177717 ],\n       [ 781727.00521532, 4188771.47311895],\n       [ 533270.55041969, 4101228.87129633],\n       [ 422275.11061901, 4061412.90318906],\n       [ 623274.94414034, 4137159.04830485],\n       [ 613396.83788472, 4098031.58804004],\n       [ 918485.80482483, 4066117.04895843],\n       [ 835277.63162857, 4216987.74937433],\n       [ 840905.06186863, 4119589.82676569],\n       [ 776734.80434232, 4130379.65381659],\n       [ 822926.36295514, 4212704.28173775],\n       [ 577183.29908505, 4173970.73377266],\n       [ 844670.36825708, 4169964.16925377],\n       [ 726945.33864164, 4247843.41173161],\n       [ 522700.79684074, 4133457.24989219],\n       [ 781400.95255701, 4075836.20334854],\n       [ 807851.53178381, 4155393.2584169 ],\n       [ 528527.53915051, 4128711.54607999],\n       [ 733715.04344501, 4324209.51951477],\n       [ 761959.58353421, 4278337.10568371],\n       [ 462416.02583551, 4055426.72279291],\n       [ 783784.3588985 , 4219065.00137999],\n       [ 656981.17357128, 4081038.36317257],\n       [ 752519.5579292 , 4122287.87050136],\n       [ 785998.97275375, 4180386.72063339],\n       [ 515218.38308205, 4058619.31300199],\n       [ 665460.74168329, 4220305.65172169],\n       [ 635096.17082896, 4088479.03653079],\n       [ 525676.09274115, 4094679.99674041],\n       [ 652994.1969737 , 4196544.93157394],\n       [ 684221.48776759, 4279076.50535586],\n       [ 841203.32727666, 4091321.7151704 ],\n       [ 658966.29762168, 4152432.0546778 ],\n       [ 746057.91004075, 4159150.20681748],\n       [ 713023.65348106, 4313845.48665105],\n       [ 389367.77238998, 4131872.44048721],\n       [ 964384.90536381, 4177830.49754845],\n       [ 861956.74790132, 4163047.64826283],\n       [ 652335.83396067, 4066521.74757694],\n       [ 970927.39251655, 4191911.74445852],\n       [ 723112.44351288, 4066723.92870727],\n       [ 617299.64641549, 4215686.04441991],\n       [ 733460.51785091, 4209610.5745427 ],\n       [ 774366.73041553, 4083740.25237318],\n       [ 773520.40251126, 4336421.17625672],\n       [ 410898.72636993, 4079437.47953348],\n       [ 673646.58115341, 4145523.97371361],\n       [ 817877.62652755, 4319656.3622249 ],\n       [ 592810.59894755, 4136791.36885295],\n       [ 552979.14353052, 4059233.18378104],\n       [ 812842.56890396, 4121182.53606018],\n       [ 767558.36215433, 4227759.98274924],\n       [ 844106.62271791, 4069246.81629651],\n       [ 801080.39465864, 4189486.98256327],\n       [ 781265.07995516, 4324246.416531  ],\n       [ 628867.42811353, 4070282.90463193]])\n\n\n\nfrom pointpats import PointPattern\npp_csr = PointPattern(samples.realizations[0])\n\n\npp_csr.plot()\n\n\n\n\n\npp_csr.plot(window=True, hull=True, title='Random Point Pattern')\n\n\n\n\n\ncsr_qr = qs.QStatistic(pp_csr, shape='rectangle', nx=3, ny=3)\ncsr_qr.plot()\n\n&lt;Axes: title={'center': 'Quadrat Count'}&gt;\n\n\n\n\n\n\ncsr_qr.chi2_pvalue\n\n7.295335713511762e-19\n\n\nThe low p-value is an artifact of the shape of the window, since we will never see points in the top two cells in the first column. We did infact generate a CSR sample, so our test is incorrect here because of the shape of the window.\nTo correct for this, we can simulate other CSR samples in the same window and compare the value of our original statistic to this distribution to develop an alternative approach to inference.\n\nnp.random.seed(12345)\ncsr_samples = poissonpp(window, 200, 99, conditioning=False, asPP=True)\n\n\ncsr_samples.realizations[0]\n\n&lt;pointpats.pointpattern.PointPattern at 0x7f3f354b9450&gt;\n\n\n\nstats = []\nfor key in csr_samples.realizations:\n    realization = csr_samples.realizations[key]\n    test = qs.QStatistic(realization, shape='rectangle', nx=3, ny=3)\n    stats.append(test.chi2)\n\n\nstats = np.array(stats)\nnum = 1 + (stats &gt;= csr_qr.chi2).sum()\nden = 99 +1\np_value = num / den\np_value\n\n0.31\n\n\n\nimport seaborn as sns\n\n\nax =sns.distplot(stats)\nax.axvline(x = csr_qr.chi2, ymin=0, ymax=.25, color='r')\n\n&lt;matplotlib.lines.Line2D at 0x7f3f35205e50&gt;\n\n\n\n\n\n\ncsr_qr.chi2\n\n103.75000000000001"
  },
  {
    "objectID": "lectures/week-13/geostat_data.html",
    "href": "lectures/week-13/geostat_data.html",
    "title": "Sampling the raster for “observations”",
    "section": "",
    "text": "import rasterio\n\n\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\n\n\nras = 'data/stanford-td754wr4701-geotiff.tiff' # already interpolated but we will treat it as \"data\"\n\n\nshp = 'data/tl_2022_06073_faces.shp'\n\n\ngdf = gpd.read_file(shp)\n\n\ngdf.shape\n\n(57076, 46)\n\n\n\ngdf.head()\n\n\n\n\n\n\n\n\nTFID\nSTATEFP20\nCOUNTYFP20\nTRACTCE20\nBLKGRPCE20\nBLOCKCE20\nSUFFIX1CE\nZCTA5CE20\nUACE20\nPUMACE20\n...\nMETDIVFP\nCNECTAFP\nNECTAFP\nNCTADVFP\nLWFLAG\nOFFSET\nATOTAL\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n216102682\n06\n073\n016302\n2\n2010\nNone\n92021\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n22408\n+32.8008387\n-116.9453919\nPOLYGON ((-116.94621 32.80044, -116.94621 32.8...\n\n\n1\n216102683\n06\n073\n016302\n2\n2006\nNone\n92021\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n29220\n+32.8008392\n-116.9472819\nPOLYGON ((-116.94836 32.80045, -116.94836 32.8...\n\n\n2\n216102684\n06\n073\n016302\n2\n2007\nNone\n92021\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n18605\n+32.8012731\n-116.9487743\nPOLYGON ((-116.94919 32.80063, -116.94919 32.8...\n\n\n3\n226900092\n06\n073\n016301\n2\n2003\nNone\n92020\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n19567\n+32.7999949\n-116.9608656\nPOLYGON ((-116.96140 32.80088, -116.96073 32.8...\n\n\n4\n263464856\n06\n073\n016301\n2\n2001\nNone\n92020\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n23766\n+32.8019391\n-116.9605314\nPOLYGON ((-116.96149 32.80194, -116.96141 32.8...\n\n\n\n\n5 rows × 46 columns\n\n\n\n\ncounty = gdf.dissolve(by='COUNTYFP20')\ncounty.to_file(\"sdcounty.geojson\")\n\n\ncounty.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nrast = rasterio.open(ras)\n\n\ncounty = county.to_crs(rast.crs)\n\n\ncounty.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nrast.crs\n\nCRS.from_epsg(4326)\n\n\n\ncoords = gdf.geometry\nsrc = rast\ndf = county\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n\nclipped_array, clipped_transform = mask(dataset=src, shapes=coords, crop=True)\n\ndf = df.to_crs(src.crs)\nout_meta = src.meta.copy()\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": clipped_array.shape[1],## \n                 \"width\": clipped_array.shape[2],\n                 \"transform\": clipped_transform})\nout_tif= \"clipped_example.tif\"\nwith rasterio.open(out_tif, \"w\", **out_meta) as dest:\n    dest.write(clipped_array)\n    \nclipped = rasterio.open(out_tif)\nfig, ax = plt.subplots(figsize=(8, 6))\np1 = df.plot(color=None,facecolor='none',edgecolor='red',linewidth = 2,ax=ax)\nshow(clipped, ax=ax)\nax.axis('off');\n\n\n\n\n\nclipped\n\n&lt;open DatasetReader name='clipped_example.tif' mode='r'&gt;\n\n\n\nimport rioxarray\n\n\nd = rioxarray.open_rasterio(\"clipped_example.tif\")\n\n\nd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 336, x: 527)&gt;\n[177072 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -117.6 -117.6 -117.6 ... -116.1 -116.1 -116.1\n  * y            (y) float64 33.5 33.5 33.5 33.5 ... 32.54 32.53 32.53 32.53\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 336x: 527...[177072 values with dtype=uint8]Coordinates: (4)band(band)int641array([1])x(x)float64-117.6 -117.6 ... -116.1 -116.1array([-117.612517, -117.609603, -117.606689, ..., -116.085683, -116.082769,\n       -116.079855])y(y)float6433.5 33.5 33.5 ... 32.53 32.53array([33.504668, 33.501754, 33.498839, ..., 32.534232, 32.531318, 32.528403])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-117.61397387622179 0.002913805220117985 0.0 33.50612487372884 0.0 -0.0029142217548938525array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([-117.61251697361173, -117.60960316839162,  -117.6066893631715,\n       -117.60377555795138, -117.60086175273126, -117.59794794751114,\n       -117.59503414229103, -117.59212033707091, -117.58920653185079,\n       -117.58629272663067,\n       ...\n       -116.10607967481073, -116.10316586959061,  -116.1002520643705,\n       -116.09733825915038, -116.09442445393026, -116.09151064871014,\n       -116.08859684349002,  -116.0856830382699, -116.08276923304979,\n       -116.07985542782967],\n      dtype='float64', name='x', length=527))yPandasIndexPandasIndex(Index([ 33.50466776285139,   33.5017535410965, 33.498839319341606,\n       33.495925097586706, 33.493010875831814,  33.49009665407692,\n        33.48718243232203,  33.48426821056713,  33.48135398881224,\n       33.478439767057345,\n       ...\n        32.55463147075599,   32.5517172490011,  32.54880302724621,\n       32.545888805491316,  32.54297458373642, 32.540060361981524,\n        32.53714614022663,  32.53423191847174,  32.53131769671684,\n        32.52840347496195],\n      dtype='float64', name='y', length=336))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x2cd7e3af0&gt;\n\n\n\n\n\n\nd.values.max()\n\n49\n\n\n\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x301b7d630&gt;\n\n\n\n\n\n\ntype(d)\n\nxarray.core.dataarray.DataArray\n\n\n\nd.dims\n\n('band', 'y', 'x')\n\n\n\nd.values.mean()\n\n15.630963675792898\n\n\n\nimport numpy\n\n\nnumpy.median(d.values)\n\n17.0\n\n\n\nd.values.shape\n\n(1, 336, 527)\n\n\n\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x301bff700&gt;\n\n\n\n\n\n\nd.plot.hist()\n\n(array([44849., 17108., 10090., 29702., 31657., 23051., 11786.,  6411.,\n         2011.,   407.]),\n array([ 0. ,  4.9,  9.8, 14.7, 19.6, 24.5, 29.4, 34.3, 39.2, 44.1, 49. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\ntype(d)\n\n\nxarray.core.dataarray.DataArray\n\n\n\nimport numpy\nnumpy.random.seed(12345)\nsample_points = county.sample_points(50)\n\n\nm = county.explore()\nsample_points.explore(m=m, color='red')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nEnsure sample points are separated by some threshold\n\ncounty.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- undefined\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\norig_crs = county.crs\n\n\ncounty_utm = county.to_crs(county.estimate_utm_crs())\n\n\ncounty_utm.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nthreshold = 10000 # no pair of stations within 10000 meters of each other\n\n\nn_points = 25 # number of stations desired\n\n\nnumpy.random.seed(12345)\nthinning = True\nsample_points = county_utm.sample_points(n_points * 4).explode(index_parts=True)\ncandidates = []\nt2 = threshold**2\niter = 0\nwhile thinning:\n    p0 = numpy.random.choice(sample_points,1)[0]\n    #p0 = sample_points[0]\n    d0 = (sample_points.x - p0.x)**2 + (sample_points.y - p0.y)**2\n    candidates.append(p0)\n    if len(candidates) == n_points:\n        thinning=False\n    else:\n        sample_points = sample_points[d0&gt;t2]\n    #print('iter: ', iter, 'shape sp: ', sampled_points.shape)\n    iter += 1\n\n\n\n\ncp = gpd.GeoSeries(candidates)\n\n\ncp.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nm = county_utm.plot()\ncp.plot(ax=m, color='r');\n\n\n\n\n\ncp.crs = county_utm.crs\ncp = cp.to_crs(county.crs)\n\n\nm = county.explore()\ncp.explore(m=m, color='r')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ncoord_list = [(x, y) for x, y in zip(cp.x, cp.y)]\n\nobservations = [x[0] for x in clipped.sample(coord_list)]\nprecip_gdf = gpd.GeoDataFrame(data=observations, columns=['inches'], geometry=cp)\n\n\nprecip_gdf.plot(column='inches', legend=True);\n\n\n\n\n\nm = county.explore()\nprecip_gdf.explore(column='inches', m=m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nprecip_gdf.to_file(\"precip_sd.geojson\", driver='GeoJSON')\ncounty.to_file(\"sdcounty.geojson\", driver='GeoJSON')"
  },
  {
    "objectID": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html",
    "href": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "",
    "text": "Authors: Serge Rey sjsrey@gmail.com and Wei Kang weikang9009@gmail.com"
  },
  {
    "objectID": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#introduction",
    "href": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#introduction",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "Introduction",
    "text": "Introduction\nDistance based methods for point patterns are of three types:\n\nMean Nearest Neighbor Distance Statistics\nNearest Neighbor Distance Functions\nInterevent Distance Functions\n\nIn addition, we are going to introduce a computational technique Simulation Envelopes to aid in making inferences about the data generating process. An example is used to demonstrate how to use and interprete simulation envelopes.\n\nfrom scipy import spatial\nimport libpysal as ps\nimport numpy as np\nfrom pointpats import ripley\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nImportError: cannot import name 'ripley' from 'pointpats' (/home/serge/miniforge3/envs/385f23/lib/python3.11/site-packages/pointpats/__init__.py)"
  },
  {
    "objectID": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#mean-nearest-neighbor-distance-statistics",
    "href": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#mean-nearest-neighbor-distance-statistics",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "Mean Nearest Neighbor Distance Statistics",
    "text": "Mean Nearest Neighbor Distance Statistics\nThe nearest neighbor(s) for a point \\(u\\) is the point(s) \\(N(u)\\) which meet the condition \\[d_{u,N(u)} \\leq d_{u,j} \\forall j \\in S - u\\]\nThe distance between the nearest neighbor(s) \\(N(u)\\) and the point \\(u\\) is nearest neighbor distance for \\(u\\). After searching for nearest neighbor(s) for all the points and calculating the corresponding distances, we are able to calculate mean nearest neighbor distance by averaging these distances.\nIt was demonstrated by Clark and Evans(1954) that mean nearest neighbor distance statistics distribution is a normal distribution under null hypothesis (underlying spatial process is CSR). We can utilize the test statistics to determine whether the point pattern is the outcome of CSR. If not, is it the outcome of cluster or regular spatial process?\n\npoints = np.array([[66.22, 32.54], [22.52, 22.39], [31.01, 81.21],\n                   [9.47, 31.02],  [30.78, 60.10], [75.21, 58.93],\n                   [79.26,  7.68], [8.23, 39.93],  [98.73, 77.17],\n                   [89.78, 42.53], [65.19, 92.08], [54.46, 8.48]])"
  },
  {
    "objectID": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#nearest-neighbor-distance-functions",
    "href": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#nearest-neighbor-distance-functions",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "Nearest Neighbor Distance Functions",
    "text": "Nearest Neighbor Distance Functions\nNearest neighbour distance distribution functions (including the nearest “event-to-event” and “point-event” distance distribution functions) of a point process are cumulative distribution functions of several kinds – \\(G, F, J\\). By comparing the distance function of the observed point pattern with that of the point pattern from a CSR process, we are able to infer whether the underlying spatial process of the observed point pattern is CSR or not for a given confidence level.\n\n\\(G\\) function - event-to-event\nThe \\(G\\) function is a kind of “cumulative” density describing the distribution of distances within a point pattern. For a given distance \\(d\\), \\(G(d)\\) is the proportion of nearest neighbor distances that are less than \\(d\\). To express this, we first need to define the nearest neighbor distance, which is the smallest distance from each observation \\(i\\) to some other observation \\(j\\), where \\(j \\neq i\\): \\[ min_{j\\neq i}\\{d_{ij}\\} = d^*_i \\]\nWith this, we can define the \\(G\\) function as a cumulative density function: \\[G(d) = \\frac{1}{N}\\sum_{i=1}^N \\mathcal{I}(d^*_i &lt; d)\\] where \\(\\mathcal{I}(.)\\) is an indicator function that is \\(1\\) when the argument is true and is zero otherwise. In simple terms, \\(G(d)\\) gives the percentage of of nearest neighbor distances (\\(d^*_i\\)) that are smaller than \\(d\\); when \\(d\\) is very small, \\(G(d)\\) is close to zero. When \\(d\\) is large, \\(G(d)\\) approaches one.\nAnalytical results about \\(G\\) are available assuming that the “null” process of locating points in the study area is completely spatially random. In a completely spatially random process, the \\(G(d)\\) value should be: \\[\nG(d) = 1-e^{-\\lambda \\pi d^2}\n\\] Practically, we assess statistical significance for the \\(G(d)\\) function using simulations, where a known spatially-random process is generated and then analyzed. This partially accounts for issues with irregularly-shaped study areas, where locations of points are constrained.\nIn practice, we use the ripley.g_test function to conduct a test on the \\(G(d)\\). It estimates a value of \\(G(d)\\) for a set of values (called the support). To compute the \\(G\\) function for ten values of \\(d\\) ranging from the smallest possible to the largest values in the data:\n\ng_test = ripley.g_test(points, support=10)\n\nAll statistical tests in the pointpats.distance_statistics return a collections.namedtuple object with the following properties: - support, which contains the distance values (\\(d\\)) used to compute the distance statistic. - statistic, which expresses the value of the requested function at each value of \\(d\\) in the support. - pvalue, which expresses the fraction of observed simulations (under a completely spatially random process) that are more extreme than the observed statistics. - simulations, which stores the simulated values of the statistic under a spatially random process. Generally, this is not saved (for efficiency reasons), but can be requested using keep_simulations.\n\ng_test.support\n\narray([ 0.        ,  3.84791574,  7.69583148, 11.54374723, 15.39166297,\n       19.23957871, 23.08749445, 26.93541019, 30.78332593, 34.63124168])\n\n\n\ng_test.statistic\n\narray([0.        , 0.        , 0.        , 0.16666667, 0.16666667,\n       0.25      , 0.58333333, 0.83333333, 0.91666667, 1.        ])\n\n\n\ng_test.pvalue\n\narray([0.00e+00, 0.00e+00, 0.00e+00, 2.89e-02, 1.10e-03, 1.00e-04,\n       4.30e-03, 6.10e-02, 7.33e-02, 0.00e+00])\n\n\n\ng_test.simulations\n\nTo make a plot of the statistic, the statistic is generally plotted on the vertical axis and the support on the horizontal axis. Here, we will show the median simulated value of \\(G(d)\\) as well.\n\ng_test = ripley.g_test(points, support=10, keep_simulations=True)\n\n\nplt.plot(g_test.support, np.median(g_test.simulations, axis=0), \n         color='k', label='simulated')\nplt.plot(g_test.support, g_test.statistic, \n         marker='x', color='orangered', label='observed')\nplt.legend()\nplt.xlabel('Distance')\nplt.ylabel('G Function')\nplt.title('G Function Plot')\nplt.show()\n\n\n\n\nAs you can see, the \\(G\\) function increases very slowly at small distances and the line is below the typical simulated value (shown in black). We can verify the visual intuition here by looking at the p-value for each point and plotting the simulated \\(G(d)\\) curves, too:\n\n# grab the middle 95% of simulations using numpy:\nmiddle_95pct = np.percentile(g_test.simulations, q=(2.5, 97.5), axis=0)\n# use the fill_between function to color between the 2.5% and 97.5% envelope\nplt.fill_between(g_test.support, *middle_95pct, \n                 color='lightgrey', label='simulated')\n\n# plot the line for the observed value of G(d)\nplt.plot(g_test.support, g_test.statistic, \n         color='orangered', label='observed')\n# and plot the support points depending on whether their p-value is smaller than .05\nplt.scatter(g_test.support, g_test.statistic, \n            cmap='viridis', c=g_test.pvalue &lt; .01)\nplt.legend()\nplt.xlabel('Distance')\nplt.ylabel('G Function')\nplt.title('G Function Plot')\nplt.show()\n\n\n\n\nFrom this, we can see that there is statistically significant “dispersion” at small values of \\(d\\), since there are too few nearest neighbor distances observed between \\(0 &lt; d &lt; 25\\). Once we get to very large distances, the simulation envelope covers the observed statistic. As such, we can say that the point pattern recorded in points is unusally dispersed.\nTo evaluate the \\(G(d)\\) function without considering any statistical significance or simulations, you can use the g_function in the ripley module, which simply returns the distances & values of \\(G(d)\\).\n\nripley.g_function(points)\n\n(array([ 0.        ,  1.82269693,  3.64539386,  5.46809079,  7.29078772,\n         9.11348465, 10.93618158, 12.75887851, 14.58157544, 16.40427237,\n        18.2269693 , 20.04966623, 21.87236316, 23.69506009, 25.51775702,\n        27.34045395, 29.16315088, 30.98584782, 32.80854475, 34.63124168]),\n array([0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.25      ,\n        0.25      , 0.25      , 0.41666667, 0.58333333, 0.75      ,\n        0.83333333, 0.83333333, 0.91666667, 0.91666667, 1.        ]))\n\n\n\n\n\\(F\\) function - “point-event”\nWhen the number of events in a point pattern is small, \\(G\\) function is rough. For the pattern contained in points, there are only 12 observations! This means that there are only 12 nearest neighbor distances, and thus only 12 possible values for the \\(G(d)\\) statistic, at any \\(d\\).\nOne way to get around this is to turn to an alternative, the \\(F(d)\\) function. This is analogous to the \\(G(d)\\) function, but measures the nearest neighbor distance from a set of known randomly-distributed points to a point in the observed pattern. Another way of thinking about \\(F(d)\\) is that it reflects a between-pattern measure of dispersion, where one pattern is completely spatially random and the other pattern is our observed pattern. In contrast, \\(G(d)\\) is a within-pattern measure of dispersion.\nFor a randomly simulated point pattern of size \\(N_s\\), this makes the \\(F(d)\\) function:\n\\[F(d) = \\frac{1}{N_s} \\sum_k^{N_s} \\mathcal{I}(d^*_k &lt; d)\\]\nThis can have \\(N_s\\) possible values for any \\(d\\), and thus can give a much more fine-grained view of the point pattern. In this sense, the \\(F(d)\\) function is often called the empty space function, as it measures the distance from random points in “empty space” to the “filled” points in our point pattern. The number of those random points governs how “fine-grained” our measure of the observed point pattern can be.\nJust like the ripley.g_test, this function is evaluated for every \\(d\\) in a support. Further, we can provide custom values for support, just in case we have known distance values of interest.\nBelow, we’ll use the same ten support values from \\(G(d)\\) function. And, let’s constrain the “simulated” point patterns to fall within the convex hull of our original point pattern:\n\nf_test = ripley.f_test(points, support = g_test.support, keep_simulations=True, hull='convex')\n\nSince the \\(F(d)\\) function is very smooth, we can see the \\(F(d)\\) statistic and its simulations clearly by plotting their values directly as lines. For the simulated values, we will make them very transparent. As before we will visualize statistical significance using the pvalue attribute:\n\nplt.plot(f_test.support, f_test.simulations.T, alpha=.01, color='k')\nplt.plot(f_test.support, f_test.statistic, color='red')\n\nplt.scatter(f_test.support, f_test.statistic, \n            cmap='viridis', c=f_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('F Function')\nplt.title('F Function Plot')\nplt.show()\n\nplt.show()\n\n\n\n\nFrom this we see that the values of the \\(F\\) function are too high for distances from about 15 to 25, and (in contrast) for values between \\(5 &lt; d &lt; 10\\), the \\(F(d)\\) function has too few short distances. When the observed \\(F(d)\\) values are too large, then the pattern is too dispersed, or regular. If the empirical \\(F(d)\\) tends to fall below the simulated values, then it reflects clustering. This is the opposite of the interpretation of the \\(G(d)\\) function above, so be careful!\n\n\n\\(J\\) function - a combination of “event-event” and “point-event”\nThe \\(J\\) function combines the \\(G\\) and \\(F\\) function, in an attempt to provide an immediate graphical indication of the clustering both internally and with respect to the empty space distribution. Practically, the \\(J(d)\\) function is computed as a kind of “relative clustering ratio”:\n\\[J(d) = \\frac{1-G(d)}{1-F(d)}\\]\nwhere the numerator captures the clustering due to within-pattern distances and the denominator captures that for the pattern-to-empty distances. This means that when \\(J(d)&lt;1\\), the underlying point process is a cluster point process, and when \\(J(d)=1\\), the underlying point process is a random point process; otherwise, it is a dispersed point process.\nThis function can suffer from numerical stability issues; as \\(G(d)\\) and \\(F(d)\\) both approach \\(1\\), the \\(J\\) ratio can become chaotic. Further, when \\(G\\) or \\(F\\) reaches one, the \\(J\\) function changes abruptly. As such, the \\(J\\) function is often truncated to the first \\(1\\) (either in \\(F(d)\\) or \\(G(d)\\)), and any \\(d\\) where both \\(F\\) and \\(G\\) are \\(1\\) is assigned a \\(J\\) value of \\(1\\).\n\njp1 = ripley.j_test(points, support=20)\n\n/home/lw17329/Dropbox/dev/pointpats/pointpats/ripley.py:894: UserWarning: requested 20 bins to evaluate the J function, but it reaches infinity at d=25.5178, meaning only 14 bins will be used to characterize the J function.\n  tree, distances=distances, **core_kwargs\n\n\nAs you can see from the warning above, the \\(J\\) function did encounter numerical stability issues at about \\(d=25\\). To address this, pointpats truncated the \\(J\\) function to only have 14 values in its support, rather than the \\(20\\) requested.\n\nplt.plot(jp1.support, jp1.statistic, color='orangered')\nplt.axhline(1, linestyle=':', color='k')\nplt.xlabel('Distance')\nplt.ylabel('J Function')\n\nText(0, 0.5, 'J Function')\n\n\n\n\n\nFrom the above figure, we see that the \\(J\\) function is above the \\(J(d)=1\\) horizontal line, especially as \\(d\\) gets large. This suggests that the process is over-dispersed."
  },
  {
    "objectID": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#interevent-distance-functions",
    "href": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#interevent-distance-functions",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "Interevent Distance Functions",
    "text": "Interevent Distance Functions\nWhile both the \\(F(d)\\) and \\(G(d)\\) functions are useful, they only consider the distance between each point \\(i\\) and its nearest point. Earlier we spelled this distance \\(d_i^*\\), and the distance between \\(i\\) and \\(j\\) was \\(d_{ij}\\). So, note that \\(d_{i}^*\\) is the only term that matters for \\(F\\) and \\(G\\), if \\(d_{ij}\\) changes (but \\(j\\) isn’t closest to \\(i\\)), then the \\(F\\) and \\(G\\) functions generally remain the same.\nSo, further statistical summary functions have been developed to consider the whole distance distribution, not only the nearest neighbor distances. These functions (still considered part of the “Ripley” alphabet, are the \\(K\\), and \\(L\\) functions.\n\n\\(K\\) function\nThe \\(K(d)\\) function is a scaled version of the cumulative density function for all distances within a point pattern. As such, it’s a “relative” of the \\(G\\) function that considers all distances, not just the nearest neighbor distances. Practically, the \\(K(d)\\) function can be thought of as the percentage of all distances that are less than \\(d\\). Therefore, for a threshold distance \\(d\\), the \\(K\\) function is defined as:\n\\[K(d) = \\frac{1}{N\\hat\\lambda} \\underset{i=1}{\\overset{N}{\\sum}}\\underset{j=1}{\\overset{N}{\\sum}} \\mathcal{I}\\left(d_ij &lt; d\\right)\\]\nIn this equation, \\(\\hat\\lambda\\) is the intensity of the point process. This represents how many points (on average) you would expect in a unit area. You can think of this as an analogue to the density of the points in the pattern: large values of \\(\\hat\\lambda\\) mean many points per area, and small values of \\(\\hat\\lambda\\) mean there are fewer points per area. Generally, this parameter is unknown, and is modelled using the average number of points in the study area. This assumes that the intensity of the point pattern is constant or homogeneous over the study area.\nIn the same manner as before, we can construct a set of \\(K(d)\\) function evaluations for random point patterns, and compare them to the observed \\(K(d)\\) function we saw in our original data.\n\nk_test = ripley.k_test(points, keep_simulations=True)\n\n\nplt.plot(k_test.support, k_test.simulations.T, color='k', alpha=.01)\nplt.plot(k_test.support, k_test.statistic, color='orangered')\n\nplt.scatter(k_test.support, k_test.statistic, \n            cmap='viridis', c=k_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('K Function')\nplt.title('K Function Plot')\nplt.show()\n\n\n\n\nAgain, we can see that the envelopes are generally above the observed function, meaining that our point pattern is dispersed. We can draw this conclusion because the distances are too small, suggesting the pattern is less clustered than otherwise woudl be expected. When points are too regular, their distances tend to be smaller than if they were distributed randomly.\n\n\n\\(L\\) function - “interevent”\nThe \\(L\\) function is a scaled version of \\(K\\) function, defined in order to assist with interpretation. The expected value of the \\(K(d)\\) function increases with \\(d\\); this makes sense, since the number of pairs of points closer than \\(d\\) will increase as \\(d\\) increases. So, we can define a normalization of \\(K\\) that removes this increase as \\(d\\) increases.\n\\[L(d) = \\sqrt{\\frac{K(d)}{\\pi}}-d\\]\nFor a pattern that is spatially random, \\(L(d)\\) is \\(0\\) at all \\(d\\) values. So, we can use this standardization to make it easier to visualize the results of the \\(K\\) function:\n\nl_test = ripley.l_test(points, keep_simulations=True)\n\n\nplt.plot(l_test.support, l_test.simulations.T, color='k', alpha=.01)\nplt.plot(l_test.support, l_test.statistic, color='orangered')\n\nplt.scatter(l_test.support, l_test.statistic, \n            cmap='viridis', c=l_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('K Function')\nplt.title('K Function Plot')\nplt.show()"
  },
  {
    "objectID": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#csr-example",
    "href": "lectures/week-13/week-12/distance_statistics-numpy-oriented.html#csr-example",
    "title": "Distance Based Statistical Method for Planar Point Patterns",
    "section": "CSR Example",
    "text": "CSR Example\nIn this example, we are going to generate a point pattern as the “observed” point pattern. This ensures that the data generating process is completely spatially random. Then, we will simulate CSR in the same domain for 100 times and construct evaluate the ripley functions for these simulations.\n\nimport geopandas\ndf = geopandas.read_file(ps.examples.get_path(\"vautm17n.shp\"))\nstate = df.geometry.cascaded_union\n\nGenerate the point pattern pp (size 100) from CSR as the “observed” point pattern.\n\npattern = ripley.simulate(state, size=100)\n\nbefore we go any further, let’s visualize these simulated values:\n\ndf.plot()\nplt.scatter(*pattern.T, color='orangered', marker='.')\nplt.show()\n\n&lt;matplotlib.collections.PathCollection at 0x7fa1a2827890&gt;\n\n\n\n\n\nAnd, let’s check if there are 100 points:\n\npattern.shape\n\n(100, 2)\n\n\nYep! So, next to simulate a set of realizations in the same manner, we can use the size argument again, just like the numpy.random simulators. This means that, to simulate \\(K\\) realizations of a pattern of size \\(N\\), then we use simulate(hull, size=(N,K). For just one realization, we can use size=N.\n\nrandom_realizations = ripley.simulate(state, size=(100,100))\n\nTo show the random pattern is truly random, we can visualize all of the points:\n\ndf.plot(facecolor='none', edgecolor='k')\nplt.scatter(*random_realizations.T, marker='.', s=2)\nplt.scatter(*pattern.T, color='orangered', marker='.')\nplt.show()\n\n\n\n\nLet’s now compute the G function for the observed pattern as well as all the realizations we just made.\n\nobserved_g = ripley.g_function(pattern)\ncomparison_g = [ripley.g_function(realization, support=observed_g[0]) \n                for realization in random_realizations]\n\n\nplt.plot(*observed_g, color='orangered')\n[plt.plot(*comparison, color='k', alpha=.01) \n for comparison in comparison_g]\nplt.show()\n\n\n\n\nAll other functions work identically!"
  },
  {
    "objectID": "lectures/week-13/week-12/interpolation_kriging.html",
    "href": "lectures/week-13/week-12/interpolation_kriging.html",
    "title": "Spatial Interpolation: Kriging",
    "section": "",
    "text": "non-deterministic interpolation\nmeasures of uncertainty\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (16, 9)\nimport rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nimport pykrige\nprecip = gpd.read_file(\"precip_sd.geojson\")\nprecip.plot(column='inches', legend=True);\ncounty = gpd.read_file(\"sdcounty.geojson\")\nm = county.explore()\nprecip.explore(column='inches', m=m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "lectures/week-13/week-12/interpolation_kriging.html#fit",
    "href": "lectures/week-13/week-12/interpolation_kriging.html#fit",
    "title": "Spatial Interpolation: Kriging",
    "section": "Fit",
    "text": "Fit\n\nh3_cents = county_h3.centroid\nimport rioxarray\nimport rasterio\n\n\n# get observed values for all grid cells\n\n\nclipped = rasterio.open(\"clipped_example.tif\")\n\n\nclipped.meta\n\n{'driver': 'GTiff',\n 'dtype': 'uint8',\n 'nodata': None,\n 'width': 527,\n 'height': 336,\n 'count': 1,\n 'crs': CRS.from_epsg(4326),\n 'transform': Affine(0.002913805220117985, 0.0, -117.61397387622179,\n        0.0, -0.0029142217548938525, 33.50612487372884)}\n\n\n\nh3_cents_4326 = h3_cents.to_crs(clipped.meta['crs'])\ncp = h3_cents_4326\ncoord_list = [(x, y) for x, y in zip(cp.x, cp.y)]\n\n\nobservations = [x[0] for x in clipped.sample(coord_list)]\ncounty_h3['inches'] = observations\n\n\ncounty_h3.plot(column='inches', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# calculate fit for each approach (MAPE)\ndef mape(est, obs):\n    err = est-obs\n    aerr = numpy.abs(err)\n    den = obs + (obs == 0)\n    paerr = aerr / den\n    paerr *= 100\n    return paerr.mean()\n\n\nmape(county_h3.nn1_est, county_h3.inches)\n\n27.99203089869245\n\n\n\nmape(county_h3.nn5_est, county_h3.inches)\n\n40.242887541921576\n\n\n\nmape(county_h3.nn5id_est, county_h3.inches)\n\n33.161102090343014\n\n\n\nmape(county_h3.ok_est, county_h3.inches)\n\n23.964144515966048\n\n\n\n# plot fit\n\n\n# map errors for different models\n\n\nen1 = county_h3.nn1_est - county_h3.inches\n\ncounty_h3['nn1_error'] = en1\n\ncounty_h3.plot(column='nn1_error', legend=True, cmap='coolwarm')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nen5id = county_h3.nn5id_est - county_h3.inches\n\ncounty_h3['nn5id_error'] = en5id\n\ncounty_h3.plot(column='nn5id_error', legend=True, cmap='coolwarm')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(x=county_h3.inches, y=county_h3.nn5id_error);"
  },
  {
    "objectID": "lectures/week-13/week-12/interp.html",
    "href": "lectures/week-13/week-12/interp.html",
    "title": "Change of Support: Surface to Vector",
    "section": "",
    "text": "import rasterio\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nras = 'data/stanford-td754wr4701-geotiff.tiff' # already interpolated but we will treat it as \"data\"\nshp = 'data/tl_2022_06073_faces.shp'\ngdf = gpd.read_file(shp)\ngdf.shape\n\n(57076, 46)\ngdf.head()\n\n\n\n\n\n\n\n\nTFID\nSTATEFP20\nCOUNTYFP20\nTRACTCE20\nBLKGRPCE20\nBLOCKCE20\nSUFFIX1CE\nZCTA5CE20\nUACE20\nPUMACE20\n...\nMETDIVFP\nCNECTAFP\nNECTAFP\nNCTADVFP\nLWFLAG\nOFFSET\nATOTAL\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n216102682\n06\n073\n016302\n2\n2010\nNone\n92021\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n22408\n+32.8008387\n-116.9453919\nPOLYGON ((-116.94621 32.80044, -116.94621 32.8...\n\n\n1\n216102683\n06\n073\n016302\n2\n2006\nNone\n92021\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n29220\n+32.8008392\n-116.9472819\nPOLYGON ((-116.94836 32.80045, -116.94836 32.8...\n\n\n2\n216102684\n06\n073\n016302\n2\n2007\nNone\n92021\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n18605\n+32.8012731\n-116.9487743\nPOLYGON ((-116.94919 32.80063, -116.94919 32.8...\n\n\n3\n226900092\n06\n073\n016301\n2\n2003\nNone\n92020\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n19567\n+32.7999949\n-116.9608656\nPOLYGON ((-116.96140 32.80088, -116.96073 32.8...\n\n\n4\n263464856\n06\n073\n016301\n2\n2001\nNone\n92020\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n23766\n+32.8019391\n-116.9605314\nPOLYGON ((-116.96149 32.80194, -116.96141 32.8...\n\n\n\n\n5 rows × 46 columns\ncounty = gdf.dissolve(by='COUNTYFP20')\ncounty.plot()\n\n&lt;Axes: &gt;\nrast = rasterio.open(ras)\ncounty = county.to_crs(rast.crs)\ncounty.plot()\n\n&lt;Axes: &gt;\nrast.crs\n\nCRS.from_epsg(4326)\ncoords = gdf.geometry\nsrc = rast\ndf = county\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nclipped_array, clipped_transform = mask(dataset=src, shapes=coords, crop=True)\n\ndf = df.to_crs(src.crs)\nout_meta = src.meta.copy()\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": clipped_array.shape[1],\n                 \"width\": clipped_array.shape[2],\n                 \"transform\": clipped_transform})\nout_tif= \"clipped_example.tif\"\nwith rasterio.open(out_tif, \"w\", **out_meta) as dest:\n    dest.write(clipped_array)\n    \nclipped = rasterio.open(out_tif)\nfig, ax = plt.subplots(figsize=(8, 6))\np1 = df.plot(color=None,facecolor='none',edgecolor='red',linewidth = 2,ax=ax)\nshow(clipped, ax=ax)\nax.axis('off');\nclipped\n\n&lt;open DatasetReader name='clipped_example.tif' mode='r'&gt;\nimport rioxarray\nd = rioxarray.open_rasterio(\"clipped_example.tif\")\nd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 336, x: 527)&gt;\n[177072 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -117.6 -117.6 -117.6 ... -116.1 -116.1 -116.1\n  * y            (y) float64 33.5 33.5 33.5 33.5 ... 32.54 32.53 32.53 32.53\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 336x: 527...[177072 values with dtype=uint8]Coordinates: (4)band(band)int641array([1])x(x)float64-117.6 -117.6 ... -116.1 -116.1array([-117.612517, -117.609603, -117.606689, ..., -116.085683, -116.082769,\n       -116.079855])y(y)float6433.5 33.5 33.5 ... 32.53 32.53array([33.504668, 33.501754, 33.498839, ..., 32.534232, 32.531318, 32.528403])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-117.61397387622179 0.002913805220117985 0.0 33.50612487372884 0.0 -0.0029142217548938525array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([-117.61251697361173, -117.60960316839162,  -117.6066893631715,\n       -117.60377555795138, -117.60086175273126, -117.59794794751114,\n       -117.59503414229103, -117.59212033707091, -117.58920653185079,\n       -117.58629272663067,\n       ...\n       -116.10607967481073, -116.10316586959061,  -116.1002520643705,\n       -116.09733825915038, -116.09442445393026, -116.09151064871014,\n       -116.08859684349002,  -116.0856830382699, -116.08276923304979,\n       -116.07985542782967],\n      dtype='float64', name='x', length=527))yPandasIndexPandasIndex(Index([ 33.50466776285139,   33.5017535410965, 33.498839319341606,\n       33.495925097586706, 33.493010875831814,  33.49009665407692,\n        33.48718243232203,  33.48426821056713,  33.48135398881224,\n       33.478439767057345,\n       ...\n        32.55463147075599,   32.5517172490011,  32.54880302724621,\n       32.545888805491316,  32.54297458373642, 32.540060361981524,\n        32.53714614022663,  32.53423191847174,  32.53131769671684,\n        32.52840347496195],\n      dtype='float64', name='y', length=336))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7f334febdcd0&gt;\nd.values.max()\n\n49\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7f334fd42610&gt;\ntype(d)\n\nxarray.core.dataarray.DataArray\nd.dims\n\n('band', 'y', 'x')\nd.values.mean()\n\n15.630963675792898\nimport numpy\nnumpy.median(d.values)\n\n17.0\nd.values.shape\n\n(1, 336, 527)\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7f334fd89b10&gt;\nd.plot.hist()\n\n(array([44849., 17108., 10090., 29702., 31657., 23051., 11786.,  6411.,\n         2011.,   407.]),\n array([ 0. ,  4.9,  9.8, 14.7, 19.6, 24.5, 29.4, 34.3, 39.2, 44.1, 49. ]),\n &lt;BarContainer object of 10 artists&gt;)\ntype(d)\n\nxarray.core.dataarray.DataArray\nimport rasterstats\ngdf.head()\n\n\n\n\n\n\n\n\nTFID\nSTATEFP20\nCOUNTYFP20\nTRACTCE20\nBLKGRPCE20\nBLOCKCE20\nSUFFIX1CE\nZCTA5CE20\nUACE20\nPUMACE20\n...\nMETDIVFP\nCNECTAFP\nNECTAFP\nNCTADVFP\nLWFLAG\nOFFSET\nATOTAL\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n216102682\n06\n073\n016302\n2\n2010\nNone\n92021\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n22408\n+32.8008387\n-116.9453919\nPOLYGON ((-116.94621 32.80044, -116.94621 32.8...\n\n\n1\n216102683\n06\n073\n016302\n2\n2006\nNone\n92021\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n29220\n+32.8008392\n-116.9472819\nPOLYGON ((-116.94836 32.80045, -116.94836 32.8...\n\n\n2\n216102684\n06\n073\n016302\n2\n2007\nNone\n92021\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n18605\n+32.8012731\n-116.9487743\nPOLYGON ((-116.94919 32.80063, -116.94919 32.8...\n\n\n3\n226900092\n06\n073\n016301\n2\n2003\nNone\n92020\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n19567\n+32.7999949\n-116.9608656\nPOLYGON ((-116.96140 32.80088, -116.96073 32.8...\n\n\n4\n263464856\n06\n073\n016301\n2\n2001\nNone\n92020\n78661\n07313\n...\nNone\nNone\nNone\nNone\nL\nN\n23766\n+32.8019391\n-116.9605314\nPOLYGON ((-116.96149 32.80194, -116.96141 32.8...\n\n\n\n\n5 rows × 46 columns\ntracts = gdf.dissolve(by='TRACTCE20')\ntracts.shape\n\n(737, 45)\ntracts.plot()\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "lectures/week-13/week-12/interp.html#surface-to-polygon-interpolation",
    "href": "lectures/week-13/week-12/interp.html#surface-to-polygon-interpolation",
    "title": "Change of Support: Surface to Vector",
    "section": "Surface to Polygon Interpolation",
    "text": "Surface to Polygon Interpolation\n\nSpatial Join on Centroid\n\ncents = tracts.centroid\n\n\ncoord_list = [(x, y) for x, y in zip(cents.x, cents.y)]\ntracts['centest'] = [x[0] for x in clipped.sample(coord_list)]\ntracts.head()\n\n\n\n\n\n\n\n\ngeometry\nTFID\nSTATEFP20\nCOUNTYFP20\nBLKGRPCE20\nBLOCKCE20\nSUFFIX1CE\nZCTA5CE20\nUACE20\nPUMACE20\n...\nMETDIVFP\nCNECTAFP\nNECTAFP\nNCTADVFP\nLWFLAG\nOFFSET\nATOTAL\nINTPTLAT\nINTPTLON\ncentest\n\n\nTRACTCE20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n000100\nPOLYGON ((-117.18981 32.74761, -117.19013 32.7...\n265709739\n06\n073\n2\n2007\nNone\n92103\n78661\n07316\n...\nNone\nNone\nNone\nNone\nL\nN\n112711\n+32.7537001\n-117.1899099\n15\n\n\n000201\nPOLYGON ((-117.17483 32.74987, -117.17501 32.7...\n216103299\n06\n073\n2\n2005\nNone\n92103\n78661\n07316\n...\nNone\nNone\nNone\nNone\nL\nN\n11341\n+32.7553182\n-117.1727763\n15\n\n\n000202\nPOLYGON ((-117.17887 32.74009, -117.17917 32.7...\n216103238\n06\n073\n1\n1009\nNone\n92103\n78661\n07316\n...\nNone\nNone\nNone\nNone\nL\nN\n13631\n+32.7428449\n-117.1753608\n15\n\n\n000301\nPOLYGON ((-117.16418 32.74077, -117.16471 32.7...\n216103144\n06\n073\n3\n3003\nNone\n92103\n78661\n07316\n...\nNone\nNone\nNone\nNone\nL\nN\n6373\n+32.7411142\n-117.1644494\n15\n\n\n000302\nPOLYGON ((-117.16382 32.74146, -117.16382 32.7...\n216103151\n06\n073\n2\n2005\nNone\n92103\n78661\n07316\n...\nNone\nNone\nNone\nNone\nL\nN\n6488\n+32.7411150\n-117.1635466\n15\n\n\n\n\n5 rows × 46 columns\n\n\n\n\ntracts['centroid'] = tracts.centroid\ntracts.set_geometry('centroid', inplace=True)\n\n\ntracts.plot(column='centest', legend=True);\n\n\n\n\n\ntracts.set_geometry('geometry', inplace=True)\ntracts.plot(column='centest', legend=True);"
  },
  {
    "objectID": "lectures/week-13/week-12/interp.html#zonal-methods-of-surface-to-area-interpolation",
    "href": "lectures/week-13/week-12/interp.html#zonal-methods-of-surface-to-area-interpolation",
    "title": "Change of Support: Surface to Vector",
    "section": "Zonal Methods of Surface to Area Interpolation",
    "text": "Zonal Methods of Surface to Area Interpolation\n\nfrom rasterstats import zonal_stats\ntstats = zonal_stats(tracts, \"clipped_example.tif\",\n            stats=\"count min mean max median\")\n\n#elevations2 = zonal_stats(\n#    sd_tracts.to_crs(dem.rio.crs),  # Geotable with zones\n#    \"../data/nasadem/nasadem_sd.tif\",  # Path to surface file\n#)\n#elevations2 = pandas.DataFrame(elevations2)\n\n\ntstats[:5]\n\n[{'min': 15.0, 'max': 15.0, 'mean': 15.0, 'count': 17, 'median': 15.0},\n {'min': 15.0, 'max': 15.0, 'mean': 15.0, 'count': 11, 'median': 15.0},\n {'min': 15.0, 'max': 15.0, 'mean': 15.0, 'count': 14, 'median': 15.0},\n {'min': 15.0, 'max': 15.0, 'mean': 15.0, 'count': 4, 'median': 15.0},\n {'min': 15.0, 'max': 16.0, 'mean': 15.5, 'count': 8, 'median': 15.5}]\n\n\n\ntstats = pd.DataFrame(tstats)\n\n\ntstats.head()\n\n\n\n\n\n\n\n\nmin\nmax\nmean\ncount\nmedian\n\n\n\n\n0\n15.0\n15.0\n15.0\n17\n15.0\n\n\n1\n15.0\n15.0\n15.0\n11\n15.0\n\n\n2\n15.0\n15.0\n15.0\n14\n15.0\n\n\n3\n15.0\n15.0\n15.0\n4\n15.0\n\n\n4\n15.0\n16.0\n15.5\n8\n15.5\n\n\n\n\n\n\n\n\ntstats.shape\n\n(737, 5)\n\n\n\ntracts.shape\n\n(737, 47)\n\n\n\ntracts['mean'] = tstats['mean'].values\ntracts.plot(column='mean', legend=True);\n\n\n\n\n\ntracts['median'] = tstats['median'].values\ntracts.plot(column='median', legend=True);\n\n\n\n\n\ntracts['range'] = tstats['max'].values - tstats['min'].values\ntracts.plot(column='range', legend=True);\n\n\n\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(data=tracts, x='centest', y='mean')\nplt.plot([10, 40], [10, 40]);\n\n\n\n\n\nsns.scatterplot(data=tracts, x='median', y='mean')## \nplt.plot([10, 40], [10, 40]);"
  },
  {
    "objectID": "lectures/week-13/week-12/2023-11-13-interpolation_deterministic.html",
    "href": "lectures/week-13/week-12/2023-11-13-interpolation_deterministic.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (16, 9)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nprecip = gpd.read_file(\"precip_sd.geojson\")\nprecip.plot(column='inches', legend=True);\ncounty = gpd.read_file(\"sdcounty.geojson\")\nm = county.explore()\nprecip.explore(column='inches', m=m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "lectures/week-13/week-12/2023-11-13-interpolation_deterministic.html#interpolation-methods",
    "href": "lectures/week-13/week-12/2023-11-13-interpolation_deterministic.html#interpolation-methods",
    "title": "Spatial Interpolation",
    "section": "Interpolation Methods",
    "text": "Interpolation Methods\n\nVoronoi Partition Interpolation\n\nfrom libpysal.cg import voronoi_frames\npoints = [(10.2, 5.1), (4.7, 2.2), (5.3, 5.7), (2.7, 5.3)]\nregions_df, points_df = voronoi_frames(points)\nregions_df.shape\n(4, 1)\n\n(4, 1)\n\n\n\nregions_df.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n#libpysal.cg.voronoi_frames(points, radius=None, clip='extent')\npoints = precip.get_coordinates().values\n\n\npoints\n\narray([[-116.11392643,   32.75307635],\n       [-117.12067477,   33.22819891],\n       [-117.54677523,   33.41129941],\n       [-116.9663248 ,   33.16548448],\n       [-116.83479485,   32.61090996],\n       [-116.95845807,   33.33033923],\n       [-117.04029442,   32.63195628],\n       [-117.37505941,   33.30728301],\n       [-116.40414067,   32.66621206],\n       [-117.19270233,   32.77478741],\n       [-116.61247649,   33.33028661],\n       [-116.7430757 ,   33.34341133],\n       [-116.21969099,   32.71825351],\n       [-116.20299833,   33.31887981],\n       [-117.05371254,   32.73833341],\n       [-116.25599174,   32.86774382],\n       [-117.1993245 ,   33.16287784],\n       [-117.1449864 ,   32.67494802],\n       [-116.95429142,   32.89167509],\n       [-117.13095076,   32.89629192],\n       [-116.74322953,   32.77737283],\n       [-116.71168297,   32.62932523],\n       [-116.10634354,   33.3975104 ],\n       [-117.21780769,   33.2680768 ],\n       [-116.56091847,   33.22526384]])\n\n\n\nv_gdf, v_p = voronoi_frames(points)\n\nv_gdf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nv_gdf, v_p = voronoi_frames(points, clip=county.geometry[0])\n\nv_gdf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nv_gdf['inches'] = precip.inches\n\n\nv_gdf.plot(column='inches', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nbase = v_gdf.plot(column='inches', legend=True,\n                 edgecolor='gray')\nprecip.plot(ax=base, color='w');\n\n\n\n\n\n\nInterpolate to Grids\n\nhex3 the county\nnearest neighbor interpolators\n\n\n\nimport tobler\n\n\ncounty_utm = county.to_crs(county.estimate_utm_crs())\nprecip_utm = precip.to_crs(precip.estimate_utm_crs())\n\n\nfrom tobler.util import h3fy\n\n\ncounty_h3 = h3fy(county_utm)\n\n\ncounty_h3.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nm = county_h3.plot(color='grey')\nprecip_utm.plot(column='inches', ax=m, legend=True);\n\n\n\n\n\n\nNearest neighbor\n\nhcents = county_h3.centroid\n\n\nm = hcents.plot(color='r')\nprecip_utm.plot(column='inches', ax=m, legend=True);\n\n\n\n\n\ngpd.sjoin_nearest(county_h3, precip_utm, distance_col=\"distances\",\n    lsuffix=\"left\", rsuffix=\"right\", exclusive=True)\n\n\n\n\n\n\n\n\ngeometry\nindex_right\ninches\ndistances\n\n\nhex_id\n\n\n\n\n\n\n\n\n8629a6a0fffffff\nPOLYGON ((537048.394 3689328.650, 540042.930 3...\n10\n31\n1591.310439\n\n\n8629a6a07ffffff\nPOLYGON ((541604.239 3684268.446, 544599.450 3...\n10\n31\n1232.212148\n\n\n8629a6a57ffffff\nPOLYGON ((532494.502 3694386.661, 535488.364 3...\n10\n31\n7342.913667\n\n\n8629a6b57ffffff\nPOLYGON ((532618.678 3676253.158, 535613.850 3...\n10\n31\n5309.224420\n\n\n8629a6a17ffffff\nPOLYGON ((548376.217 3685745.697, 551371.779 3...\n10\n31\n8160.937377\n\n\n...\n...\n...\n...\n...\n\n\n8629a42d7ffffff\nPOLYGON ((453619.190 3683208.511, 456608.225 3...\n2\n20\n6756.531707\n\n\n8629a092fffffff\nPOLYGON ((455838.093 3689733.627, 458826.856 3...\n2\n20\n3101.906869\n\n\n8629a090fffffff\nPOLYGON ((458056.590 3696257.420, 461045.077 3...\n2\n20\n5122.554721\n\n\n8629a6937ffffff\nPOLYGON ((578262.158 3625515.280, 581264.072 3...\n0\n7\n4092.672359\n\n\n86485966fffffff\nPOLYGON ((582836.817 3620429.699, 585839.404 3...\n0\n7\n0.000000\n\n\n\n\n283 rows × 4 columns\n\n\n\n\ncounty_h3.shape\n\n(283, 1)\n\n\n\ncounty_h3['nn1_est'] = gpd.sjoin_nearest(county_h3, precip_utm, distance_col=\"distances\",\n    lsuffix=\"left\", rsuffix=\"right\", exclusive=True).inches\n\n\ncounty_h3.plot(column='nn1_est', legend=True);\n\n\n\n\n\n\nKnn5\n\nX = [[0], [1], [2], [3]]\ny = [0, 0, 1, 1]\nfrom sklearn.neighbors import KNeighborsRegressor\nneigh = KNeighborsRegressor(n_neighbors=2)\nneigh.fit(X, y)\nprint(neigh.predict([[1.5]]))\n\n\n[0.5]\n\n\n\n# Set number of neighbors to use\nneighbors = 5\n\n# Initialize KNN regressor\nknn_regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"uniform\") # no distance decay distance\n\n# Fit to observed locations\nknn_regressor.fit(precip_utm.get_coordinates(), precip_utm.inches)\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor()\n\n\n\nknn_regressor.predict(hcents.get_coordinates())\n\narray([23.6, 20.4, 20. ,  9.2, 23.6, 22.4, 15. , 19.4, 15. , 20.4, 21.8,\n       22.2, 21.4, 16.8, 23.6, 16.8, 16.8, 13.2, 15. , 16.8, 20.6, 18.6,\n       20.6, 22. , 21.8, 23.6, 22. , 24.8, 20. , 15. , 17.4, 23.6, 20. ,\n       15.6, 17.6, 27.4, 21.2, 19.4, 17.6, 25. , 19. , 22.6, 27.4, 20.6,\n       17.6, 25. , 20.4, 22.4, 27.6, 21.2,  9.2, 17. , 23.6, 19.4, 12.6,\n       24.2, 18.6, 16.8, 23.2, 21.8, 17. , 27.4, 25. , 22.6, 23.2, 17. ,\n       15. , 23. , 23.6, 17.6, 20.4, 22. , 25. , 15.6, 21.6, 24.2, 23.6,\n       21.4, 20.6, 17. , 18.8, 20.6, 20. , 15. , 16.8, 23.6, 16.8, 15. ,\n       25.6, 22. , 24.2, 19.2, 19. , 25.4, 17.6, 19. , 15.6, 16.8, 15. ,\n       21.2, 20.6, 19. , 21.8, 15. , 16.8, 19. , 25.6, 27.4, 18.6, 21.8,\n       25. , 23. , 17. , 20.6, 15.6, 24.4, 27.4, 20.4, 15.6, 13.2, 23.6,\n       26. , 13.2, 20. , 19.2, 16.8, 20.4, 27.4, 22.4, 27.6, 17. , 16.8,\n       22. , 16.8, 26. , 22.4, 27.4, 20.6, 26.6, 16.8, 19. , 19. , 10.2,\n       23.6, 22.6, 26.6, 20.6, 20.6, 21.8, 20.6, 23.2, 25.4, 20.6, 27.4,\n       13.2, 20.4, 16.8, 21.8, 15. , 21.8, 21.2, 25.6, 10.2, 23.6, 18.4,\n       19.4, 19.6, 23.6, 16.8, 27.4, 19. , 20.6, 15. , 27.4, 27.4, 21.8,\n       12.6, 24.2, 27.4, 15.6, 15.6, 25. , 16.8, 20.6, 27.4, 23.2, 25. ,\n       12.6, 21.2, 19. , 16.4, 25. , 23.6, 21.8, 23.6, 23.6, 15.6, 21.8,\n       16.8, 16.8, 15.6, 27.4, 16.4, 27.6, 27.4, 21.4, 12.6, 27.4, 23.2,\n       16.8, 16.8, 15. , 23.6, 19.4, 24.2, 27.4, 22.4, 23. , 23.6, 27.4,\n       19.4, 16.8, 20.6, 22.4, 20.6, 17. , 13.2, 23.6, 19.4, 21.8, 16.8,\n       20.6, 17.6, 23.6, 15.6, 20.4, 15.6, 16.6, 21.4, 25.8, 25. , 20.6,\n       19. , 20.6, 19. , 15. , 27.6, 27.4, 21.2, 21.2, 20. , 20.6, 20.6,\n       19.4, 21.8, 19.4, 19. , 19. , 20.6, 19.4, 17. , 16.8, 20. , 19. ,\n       16.4, 16.8, 15. , 25. , 19. , 25. , 23.2, 20.6, 10.2, 20. , 17. ,\n       20. , 19. , 20.8, 24.2, 20.6, 17. , 15. , 19. ])\n\n\n\ncounty_h3['nn5_est'] = knn_regressor.predict(hcents.get_coordinates())\n\n\ncounty_h3.plot(column='nn5_est', legend=True);\n\n\n\n\n\nimport matplotlib.pyplot as plt\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\ncounty_h3.plot(column='nn1_est',ax=ax1)\ncounty_h3.plot(column='nn5_est',ax=ax2)\nax1.set_title(\"knn=1\")\nax2.set_title(\"knn=5\");\n\n\n\n\n\n\n# Set number of neighbors to use\nneighbors = 5\n\n# Initialize KNN regressor\nknn_regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"distance\") # inverse distance weighting\n# Fit to observed locations\nknn_regressor.fit(precip_utm.get_coordinates(), precip_utm.inches)\n\nKNeighborsRegressor(weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor(weights='distance')\n\n\n\ncounty_h3['nn5id_est'] = knn_regressor.predict(hcents.get_coordinates())\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\ncounty_h3.plot(column='nn5_est',ax=ax1)\ncounty_h3.plot(column='nn5id_est',ax=ax2)\nax1.set_title(\"knn=5 (Unweighted)\")\nax2.set_title(\"knn=5 (Inverse Distance Weights)\");"
  },
  {
    "objectID": "lectures/week-13/2023-11-13-interp-data.html",
    "href": "lectures/week-13/2023-11-13-interp-data.html",
    "title": "Raster Data on California Precipitation",
    "section": "",
    "text": "import rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nras = 'data/stanford-td754wr4701-geotiff.tiff' # already interpolated but we will treat it as \"data\"\n\n# source: https://earthworks.stanford.edu/catalog/stanford-td754wr4701\nrast = rasterio.open(ras)\nfrom rasterio.plot import show\nshow(rast)\n\n\n\n\n&lt;Axes: &gt;\nfrom rasterio.plot import show_hist\n\nshow_hist(rast, lw=0.0, stacked=False, alpha=0.3, \n          histtype='stepfilled', title=\"Histogram\");"
  },
  {
    "objectID": "lectures/week-13/2023-11-13-interp-data.html#clip-to-san-diego-county",
    "href": "lectures/week-13/2023-11-13-interp-data.html#clip-to-san-diego-county",
    "title": "Raster Data on California Precipitation",
    "section": "Clip to San Diego County",
    "text": "Clip to San Diego County\n\ngdf = gpd.read_file('sdcounty.geojson')\n\n\ngdf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ncounty = gdf.to_crs(rast.crs)\n\n\ncounty.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nrast.crs\n\nCRS.from_epsg(4326)\n\n\n\ncoords = gdf.geometry\nsrc = rast\ndf = county\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n\nclipped_array, clipped_transform = mask(dataset=src, shapes=coords, crop=True)\n\ndf = df.to_crs(src.crs)\nout_meta = src.meta.copy()\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": clipped_array.shape[1],## \n                 \"width\": clipped_array.shape[2],\n                 \"transform\": clipped_transform})\nout_tif= \"clipped_example.tif\"\nwith rasterio.open(out_tif, \"w\", **out_meta) as dest:\n    dest.write(clipped_array)\n    \nclipped = rasterio.open(out_tif)\nfig, ax = plt.subplots(figsize=(8, 6))\np1 = df.plot(color=None,facecolor='none',edgecolor='red',linewidth = 2,ax=ax)\nshow(clipped, ax=ax)\nax.axis('off');\n\n\n\n\n\nclipped\n\n&lt;open DatasetReader name='clipped_example.tif' mode='r'&gt;\n\n\n\nimport rioxarray\n\n\nd = rioxarray.open_rasterio(\"clipped_example.tif\")\n\n\nd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 336, x: 527)&gt;\n[177072 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -117.6 -117.6 -117.6 ... -116.1 -116.1 -116.1\n  * y            (y) float64 33.5 33.5 33.5 33.5 ... 32.54 32.53 32.53 32.53\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 336x: 527...[177072 values with dtype=uint8]Coordinates: (4)band(band)int641array([1])x(x)float64-117.6 -117.6 ... -116.1 -116.1array([-117.612517, -117.609603, -117.606689, ..., -116.085683, -116.082769,\n       -116.079855])y(y)float6433.5 33.5 33.5 ... 32.53 32.53array([33.504668, 33.501754, 33.498839, ..., 32.534232, 32.531318, 32.528403])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-117.61397387622179 0.002913805220117985 0.0 33.50612487372884 0.0 -0.0029142217548938525array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([-117.61251697361173, -117.60960316839162,  -117.6066893631715,\n       -117.60377555795138, -117.60086175273126, -117.59794794751114,\n       -117.59503414229103, -117.59212033707091, -117.58920653185079,\n       -117.58629272663067,\n       ...\n       -116.10607967481073, -116.10316586959061,  -116.1002520643705,\n       -116.09733825915038, -116.09442445393026, -116.09151064871014,\n       -116.08859684349002,  -116.0856830382699, -116.08276923304979,\n       -116.07985542782967],\n      dtype='float64', name='x', length=527))yPandasIndexPandasIndex(Index([ 33.50466776285139,   33.5017535410965, 33.498839319341606,\n       33.495925097586706, 33.493010875831814,  33.49009665407692,\n        33.48718243232203,  33.48426821056713,  33.48135398881224,\n       33.478439767057345,\n       ...\n        32.55463147075599,   32.5517172490011,  32.54880302724621,\n       32.545888805491316,  32.54297458373642, 32.540060361981524,\n        32.53714614022663,  32.53423191847174,  32.53131769671684,\n        32.52840347496195],\n      dtype='float64', name='y', length=336))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7fdcad553cd0&gt;\n\n\n\n\n\n\nd.values.max()\n\n49\n\n\n\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7fdc9ea4bd90&gt;\n\n\n\n\n\n\ntype(d)\n\nxarray.core.dataarray.DataArray\n\n\n\nd.dims\n\n('band', 'y', 'x')\n\n\n\nd.values.mean()\n\n15.630963675792898\n\n\n\nimport numpy\n\n\nnumpy.median(d.values)\n\n17.0\n\n\n\nd.values.shape\n\n(1, 336, 527)\n\n\n\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7fdc9e951c10&gt;\n\n\n\n\n\n\nd.plot.hist()\n\n(array([44849., 17108., 10090., 29702., 31657., 23051., 11786.,  6411.,\n         2011.,   407.]),\n array([ 0. ,  4.9,  9.8, 14.7, 19.6, 24.5, 29.4, 34.3, 39.2, 44.1, 49. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\ntype(d)\n\n\nxarray.core.dataarray.DataArray\n\n\n\nSampling the raster for “observations”\n\nimport numpy\nnumpy.random.seed(12345)\nsample_points = county.sample_points(50)\n\n\nm = county.explore()\nsample_points.explore(m=m, color='red')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nEnsure sample points are separated by some threshold\n\ncounty.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- undefined\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\norig_crs = county.crs\n\n\ncounty_utm = county.to_crs(county.estimate_utm_crs())\n\n\ncounty_utm.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nthreshold = 10000 # no pair of stations within 10000 meters of each other\n\n\nn_points = 25 # number of stations desired\n\n\nnumpy.random.seed(12345)\nthinning = True\nsample_points = county_utm.sample_points(n_points * 4).explode(index_parts=True)\ncandidates = []\nt2 = threshold**2\niter = 0\nwhile thinning:\n    p0 = numpy.random.choice(sample_points,1)[0]\n    #p0 = sample_points[0]\n    d0 = (sample_points.x - p0.x)**2 + (sample_points.y - p0.y)**2\n    candidates.append(p0)\n    if len(candidates) == n_points:\n        thinning=False\n    else:\n        sample_points = sample_points[d0&gt;t2]\n    #print('iter: ', iter, 'shape sp: ', sampled_points.shape)\n    iter += 1\n\n\n\n\ncp = gpd.GeoSeries(candidates)\n\n\ncp.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nm = county_utm.plot()\ncp.plot(ax=m, color='r');\n\n\n\n\n\ncp.crs = county_utm.crs\ncp = cp.to_crs(county.crs)\n\n\nm = county.explore()\ncp.explore(m=m, color='r')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ncoord_list = [(x, y) for x, y in zip(cp.x, cp.y)]\n\nobservations = [x[0] for x in clipped.sample(coord_list)]\nprecip_gdf = gpd.GeoDataFrame(data=observations, columns=['inches'], geometry=cp)\n\n\nprecip_gdf.plot(column='inches', legend=True);\n\n\n\n\n\nm = county.explore()\nprecip_gdf.explore(column='inches', m=m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nprecip_gdf.to_file(\"precip_sd.geojson\", driver='GeoJSON')\ncounty.to_file(\"sdcounty.geojson\", driver='GeoJSON')"
  },
  {
    "objectID": "lectures/week-02/2023-08-28-python-introduction.html",
    "href": "lectures/week-02/2023-08-28-python-introduction.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "high level language\nlow learning curve\nopen source\nwidespread use\nfun"
  },
  {
    "objectID": "lectures/week-02/2023-08-28-python-introduction.html#why-python",
    "href": "lectures/week-02/2023-08-28-python-introduction.html#why-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "high level language\nlow learning curve\nopen source\nwidespread use\nfun"
  },
  {
    "objectID": "lectures/week-02/2023-08-28-python-introduction.html#python-concepts",
    "href": "lectures/week-02/2023-08-28-python-introduction.html#python-concepts",
    "title": "Introduction to Python",
    "section": "Python Concepts",
    "text": "Python Concepts\n\n2 + 3\n\n5\n\n\n\n5 * 7\n\n35\n\n\n\n2**4\n\n16\n\n\n\n7 / 4\n\n1.75\n\n\n\nx = 8/3\n\n\nx\n\n2.6666666666666665\n\n\n\nimport math\n\n\nmath.pi\n\n3.141592653589793\n\n\n\nmath.sqrt(4)\n\n2.0\n\n\n\ntemp_celsius = 10.0\n\n\ntemp_celsius\n\n10.0\n\n\n\nprint(\"Temperature in Fahrenheit: \", 9/5 * temp_celsius + 32)\n\nTemperature in Fahrenheit:  50.0\n\n\n\nprint(\"Temperature in Fahrenheit: \", (9/5 * temp_celsius) + 32)\n\nTemperature in Fahrenheit:  50.0\n\n\n\ntemp_celsius\n\n10.0\n\n\n\ntemp_celsius = 40.0\n\n\nprint(\"Temperature in Fahrenheit: \", (9/5 * temp_celsius) + 32)\n\nTemperature in Fahrenheit:  104.0\n\n\n\nprint(\"Temperature in Fahrenheit: \", (9/5 * temp_celsius) + 32)\n\nTemperature in Fahrenheit:  104.0"
  },
  {
    "objectID": "lectures/week-02/2023-08-28-python-introduction.html#data-types",
    "href": "lectures/week-02/2023-08-28-python-introduction.html#data-types",
    "title": "Introduction to Python",
    "section": "Data Types",
    "text": "Data Types\n\nint\nfloat\nstr\nbool\n\n\nweatherForecast = \"Hot\"\n\n\ntype(weatherForecast)\n\nstr\n\n\n\nx = 10.1\ntype(x)\n\nfloat\n\n\n\ny = 7\ntype(y)\n\nint\n\n\n\ndir(weatherForecast)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\n\nweatherForecast.upper()\n\n'HOT'\n\n\n\nweatherForecast.startswith('f')\n\nFalse\n\n\n\ntitle = \"This is the title of my book\"\n\n\ntitle\n\n'This is the title of my book'\n\n\n\ntitle.upper()\n\n'THIS IS THE TITLE OF MY BOOK'\n\n\n\ntitle.title()\n\n'This Is The Title Of My Book'\n\n\n\ntitle.split()\n\n['This', 'is', 'the', 'title', 'of', 'my', 'book']\n\n\n\ndir(title)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\n\ntitle\n\n'This is the title of my book'\n\n\n\ntitle.center(40)\n\n'      This is the title of my book      '\n\n\n\nwords = title.split()\n\n\nwords\n\n['This', 'is', 'the', 'title', 'of', 'my', 'book']\n\n\n\ntype(words)\n\nlist\n\n\n\ndir(words)\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\n\nwords\n\n['This', 'is', 'the', 'title', 'of', 'my', 'book']\n\n\n\nlen(words)\n\n7\n\n\n\nlen(title)\n\n28\n\n\n\nwords[0]\n\n'This'\n\n\n\nwords[1]\n\n'is'\n\n\n\nwords[7]\n\nIndexError: list index out of range\n\n\n\nwords[6]\n\n'book'\n\n\n\ntitle\n\n'This is the title of my book'\n\n\n\ntitle[0]\n\n'T'\n\n\n\nlen(title)\n\n28\n\n\n\ntitle[-1]\n\n'k'\n\n\n\ntitle[-2]\n\n'o'\n\n\n\ntitle[0:-4]\n\n'This is the title of my '\n\n\n\ntitle[5:10]\n\n'is th'\n\n\n\ntitle[10]\n\n'e'\n\n\n\nwords\n\n['This', 'is', 'the', 'title', 'of', 'my', 'book']\n\n\n\nwords[1:-2]\n\n['is', 'the', 'title', 'of']\n\n\n\ntype(words)\n\nlist\n\n\n\ntype(words[0])\n\nstr\n\n\n\nfor word in words:\n    print(word)\n\nThis\nis\nthe\ntitle\nof\nmy\nbook\n\n\n\nfor i, word in enumerate(words):\n    print(i, word)\n\n0 This\n1 is\n2 the\n3 title\n4 of\n5 my\n6 book\n\n\n\nfor i, word in enumerate(words):\n    print(i, word, type(word))\n\n0 This &lt;class 'str'&gt;\n1 is &lt;class 'str'&gt;\n2 the &lt;class 'str'&gt;\n3 title &lt;class 'str'&gt;\n4 of &lt;class 'str'&gt;\n5 my &lt;class 'str'&gt;\n6 book &lt;class 'str'&gt;\n\n\n\nlen(words)\n\n7\n\n\n\nif len(words) &gt; 7:\n    print('no its not longer than 7')\n\n\nif len(words) == 7:\n    print('it is of length 7')\n\nit is of length 7\n\n\n\nif len(words) &gt; 7:\n    print('no its not longer than 7')\nelse:\n    print('It must be 7 or less in length')\n\nIt must be 7 or less in length\n\n\n\nif len(words) &gt; 7:\n    print('no its not longer than 7')\nelif len(words)==7:\n    print('its 7')\nelse:\n    print('It is less than 7 in length')\n\nits 7"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html",
    "href": "lectures/week-03/2023-09-06.html",
    "title": "Unique Values",
    "section": "",
    "text": "import pandas as pd\ndata = pd.read_csv(\"data/shared/weather/Kumpula-June-2016-w-metadata.txt\")\ndata.head(10)\n\n\n\n\n\n\n\n\n# Data file contents: Daily temperatures (mean\nmin\nmax) for Kumpula\nHelsinki\n\n\n\n\n0\n# for June 1-30\n2016\nNaN\nNaN\n\n\n1\n# Data source: https://www.ncdc.noaa.gov/cdo-w...\nNaN\nNaN\nNaN\n\n\n2\n# Data processing: Extracted temperatures from...\nconverted to\nNaN\nNaN\n\n\n3\n# comma-separated format\nNaN\nNaN\nNaN\n\n\n4\n#\nNaN\nNaN\nNaN\n\n\n5\n# David Whipp - 02.10.2017\nNaN\nNaN\nNaN\n\n\n6\nYEARMODA\nTEMP\nMAX\nMIN\n\n\n7\n20160601\n65.5\n73.6\n54.7\n\n\n8\n20160602\n65.8\n80.8\n55.0\n\n\n9\n20160603\n68.4\n77.9\n55.6\ndata.tail()\n\n\n\n\n\n\n\n\n# Data file contents: Daily temperatures (mean\nmin\nmax) for Kumpula\nHelsinki\n\n\n\n\n32\n20160626\n69.6\n77.7\n60.3\n\n\n33\n20160627\n60.7\n70.0\n57.6\n\n\n34\n20160628\n65.4\n73.0\n55.8\n\n\n35\n20160629\n65.8\n73.2\n59.7\n\n\n36\n20160630\n65.7\n72.7\n59.2\ndata = pd.read_csv(\"data/shared/weather/Kumpula-June-2016-w-metadata.txt\", skiprows=8)\ntype(data)\n\npandas.core.frame.DataFrame\ndata.head()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n\n\n1\n20160602\n65.8\n80.8\n55.0\n\n\n2\n20160603\n68.4\n77.9\n55.6\n\n\n3\n20160604\n57.5\n70.9\n47.3\n\n\n4\n20160605\n51.4\n58.3\n43.2\ndata.tail()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\n\n\n\n\n25\n20160626\n69.6\n77.7\n60.3\n\n\n26\n20160627\n60.7\n70.0\n57.6\n\n\n27\n20160628\n65.4\n73.0\n55.8\n\n\n28\n20160629\n65.8\n73.2\n59.7\n\n\n29\n20160630\n65.7\n72.7\n59.2\ntemp_data = pd.read_csv(\n    \"data/shared/weather/Kumpula-June-2016-w-metadata.txt\", \n    skiprows=8, usecols=[\"YEARMODA\", \"TEMP\"]\n)\n\ntemp_data.head()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\n\n\n\n\n0\n20160601\n65.5\n\n\n1\n20160602\n65.8\n\n\n2\n20160603\n68.4\n\n\n3\n20160604\n57.5\n\n\n4\n20160605\n51.4\noutput_fp = \"Kumpula_temps_June_2016.csv\"\ndata.to_csv(output_fp, sep=\",\")\noutput_fp = \"Kumpula_temps_June_2016.csv\"\ndata.to_csv(output_fp, sep=\",\", index=False, float_format=\"%.0f\")\ntype(data)\n\npandas.core.frame.DataFrame\nlen(data)\n\n30\ndata.shape\n\n(30, 4)\ndata.columns\n\nIndex(['YEARMODA', 'TEMP', 'MAX', 'MIN'], dtype='object')\ndata.columns.values\n\narray(['YEARMODA', 'TEMP', 'MAX', 'MIN'], dtype=object)\ndata.index\n\nRangeIndex(start=0, stop=30, step=1)\ndata.dtypes\n\nYEARMODA      int64\nTEMP        float64\nMAX         float64\nMIN         float64\ndtype: object\nselection = data[[\"YEARMODA\", \"TEMP\"]]\nselection.head()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\n\n\n\n\n0\n20160601\n65.5\n\n\n1\n20160602\n65.8\n\n\n2\n20160603\n68.4\n\n\n3\n20160604\n57.5\n\n\n4\n20160605\n51.4\nselection.shape\n\n(30, 2)\ndata[\"TEMP\"].mean()\n\n59.730000000000004\ndata.TEMP.mean()\n\n59.730000000000004\ndata.mean()\n\nYEARMODA    20160615.50\nTEMP              59.73\nMAX               67.94\nMIN               51.75\ndtype: float64\ndata.describe()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\n\n\n\n\ncount\n3.000000e+01\n30.000000\n30.000000\n30.000000\n\n\nmean\n2.016062e+07\n59.730000\n67.940000\n51.750000\n\n\nstd\n8.803408e+00\n5.475472\n6.651761\n5.634484\n\n\nmin\n2.016060e+07\n49.400000\n54.100000\n41.700000\n\n\n25%\n2.016061e+07\n56.450000\n63.150000\n47.300000\n\n\n50%\n2.016062e+07\n60.050000\n69.000000\n54.050000\n\n\n75%\n2.016062e+07\n64.900000\n72.375000\n55.750000\n\n\nmax\n2.016063e+07\n69.600000\n80.800000\n60.300000\ndata['MIN'].hasnans\n\nFalse\ndata.head(30)\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n\n\n1\n20160602\n65.8\n80.8\n55.0\n\n\n2\n20160603\n68.4\n77.9\n55.6\n\n\n3\n20160604\n57.5\n70.9\n47.3\n\n\n4\n20160605\n51.4\n58.3\n43.2\n\n\n5\n20160606\n52.2\n59.7\n42.8\n\n\n6\n20160607\n56.9\n65.1\n45.9\n\n\n7\n20160608\n54.2\n60.4\n47.5\n\n\n8\n20160609\n49.4\n54.1\n45.7\n\n\n9\n20160610\n49.5\n55.9\n43.0\n\n\n10\n20160611\n54.0\n62.1\n41.7\n\n\n11\n20160612\n55.4\n64.2\n46.0\n\n\n12\n20160613\n58.3\n68.2\n47.3\n\n\n13\n20160614\n59.7\n67.8\n47.8\n\n\n14\n20160615\n63.4\n70.3\n49.3\n\n\n15\n20160616\n57.8\n67.5\n55.6\n\n\n16\n20160617\n60.4\n70.7\n55.9\n\n\n17\n20160618\n57.3\n62.8\n54.0\n\n\n18\n20160619\n56.3\n59.2\n54.1\n\n\n19\n20160620\n59.3\n69.1\n52.2\n\n\n20\n20160621\n62.6\n71.4\n50.4\n\n\n21\n20160622\n61.7\n70.2\n55.4\n\n\n22\n20160623\n60.9\n67.1\n54.9\n\n\n23\n20160624\n61.1\n68.9\n56.7\n\n\n24\n20160625\n65.7\n75.4\n57.9\n\n\n25\n20160626\n69.6\n77.7\n60.3\n\n\n26\n20160627\n60.7\n70.0\n57.6\n\n\n27\n20160628\n65.4\n73.0\n55.8\n\n\n28\n20160629\n65.8\n73.2\n59.7\n\n\n29\n20160630\n65.7\n72.7\n59.2\ndata['TEMP'].unique()\n\narray([65.5, 65.8, 68.4, 57.5, 51.4, 52.2, 56.9, 54.2, 49.4, 49.5, 54. ,\n       55.4, 58.3, 59.7, 63.4, 57.8, 60.4, 57.3, 56.3, 59.3, 62.6, 61.7,\n       60.9, 61.1, 65.7, 69.6, 60.7, 65.4])\nprint(\n    \"There were\",\n    data[\"TEMP\"].nunique(),\n    \"days with unique mean temperatures in June 2016.\",\n)\n\nThere were 28 days with unique mean temperatures in June 2016.\ndata.TEMP.mean()\n\n59.730000000000004\ndata.mean()\n\nYEARMODA    20160615.50\nTEMP              59.73\nMAX               67.94\nMIN               51.75\ndtype: float64\ndata.describe()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\n\n\n\n\ncount\n3.000000e+01\n30.000000\n30.000000\n30.000000\n\n\nmean\n2.016062e+07\n59.730000\n67.940000\n51.750000\n\n\nstd\n8.803408e+00\n5.475472\n6.651761\n5.634484\n\n\nmin\n2.016060e+07\n49.400000\n54.100000\n41.700000\n\n\n25%\n2.016061e+07\n56.450000\n63.150000\n47.300000\n\n\n50%\n2.016062e+07\n60.050000\n69.000000\n54.050000\n\n\n75%\n2.016062e+07\n64.900000\n72.375000\n55.750000\n\n\nmax\n2.016063e+07\n69.600000\n80.800000\n60.300000"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#constructing-pandas-objects",
    "href": "lectures/week-03/2023-09-06.html#constructing-pandas-objects",
    "title": "Unique Values",
    "section": "Constructing Pandas objects",
    "text": "Constructing Pandas objects\n\nnumber_series = pd.Series([4, 5, 6, 7.0])\nprint(number_series)\n\n0    4.0\n1    5.0\n2    6.0\n3    7.0\ndtype: float64\n\n\n\nnumber_series = pd.Series([4, 5, 6, 7.0], index=[\"a\", \"b\", \"c\", \"d\"])\nprint(number_series)\n\na    4.0\nb    5.0\nc    6.0\nd    7.0\ndtype: float64\n\n\n\nstations = [\"Hanko\", \"Heinola\", \"Kaisaniemi\", \"Malmi\"]\nlatitudes = [59.77, 61.2, 60.18, 60.25]\nlongitudes = [22.95, 26.05, 24.94, 25.05]\n\n\nnew_data = pd.DataFrame(data={\"station\": stations, \"lat\": latitudes, \"lon\": longitudes})\nnew_data\n\n\n\n\n\n\n\n\nstation\nlat\nlon\n\n\n\n\n0\nHanko\n59.77\n22.95\n\n\n1\nHeinola\n61.20\n26.05\n\n\n2\nKaisaniemi\n60.18\n24.94\n\n\n3\nMalmi\n60.25\n25.05\n\n\n\n\n\n\n\n\ndictionaries = [\n    {\"station\": \"Hanko\", \"lat\": 59.77, \"lon\": 22.95},\n    {\"station\": \"Heinola\", \"lat\": 61.2, \"lon\": 26.05},\n    {\"station\": \"Kaisaniemi\", \"lat\": 60.18, \"lon\": 24.94},\n    {\"station\": \"Malmi\", \"lat\": 60.25, \"lon\": 25.05},\n]\n\n# Pass the list into the DataFrame constructor\nnew_data_2 = pd.DataFrame(dictionaries)\nnew_data_2\n\n\n\n\n\n\n\n\nstation\nlat\nlon\n\n\n\n\n0\nHanko\n59.77\n22.95\n\n\n1\nHeinola\n61.20\n26.05\n\n\n2\nKaisaniemi\n60.18\n24.94\n\n\n3\nMalmi\n60.25\n25.05\n\n\n\n\n\n\n\n\ndf = pd.DataFrame()\nprint(df)\n\nEmpty DataFrame\nColumns: []\nIndex: []\n\n\n\ndf[\"lon\"] = longitudes\ndf[\"lat\"] = latitudes\ndf\n\n\n\n\n\n\n\n\nlon\nlat\n\n\n\n\n0\n22.95\n59.77\n\n\n1\n26.05\n61.20\n\n\n2\n24.94\n60.18\n\n\n3\n25.05\n60.25"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#common-tabular-operations",
    "href": "lectures/week-03/2023-09-06.html#common-tabular-operations",
    "title": "Unique Values",
    "section": "Common tabular operations",
    "text": "Common tabular operations\n\ndata.head()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n\n\n1\n20160602\n65.8\n80.8\n55.0\n\n\n2\n20160603\n68.4\n77.9\n55.6\n\n\n3\n20160604\n57.5\n70.9\n47.3\n\n\n4\n20160605\n51.4\n58.3\n43.2\n\n\n\n\n\n\n\n\ndata['DIFF'] = 0.0\ndata.head()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n0.0\n\n\n1\n20160602\n65.8\n80.8\n55.0\n0.0\n\n\n2\n20160603\n68.4\n77.9\n55.6\n0.0\n\n\n3\n20160604\n57.5\n70.9\n47.3\n0.0\n\n\n4\n20160605\n51.4\n58.3\n43.2\n0.0\n\n\n\n\n\n\n\n\ndata['DIFF'].dtypes\n\ndtype('float64')\n\n\n\ndata['DIFF'] = data['MAX'] - data['MIN']\ndata.head()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n18.9\n\n\n1\n20160602\n65.8\n80.8\n55.0\n25.8\n\n\n2\n20160603\n68.4\n77.9\n55.6\n22.3\n\n\n3\n20160604\n57.5\n70.9\n47.3\n23.6\n\n\n4\n20160605\n51.4\n58.3\n43.2\n15.1\n\n\n\n\n\n\n\n\ndata[\"TEMP_CELSIUS\"] = (data[\"TEMP\"] - 32) / (9 / 5)\ndata.head()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n18.9\n18.611111\n\n\n1\n20160602\n65.8\n80.8\n55.0\n25.8\n18.777778\n\n\n2\n20160603\n68.4\n77.9\n55.6\n22.3\n20.222222\n\n\n3\n20160604\n57.5\n70.9\n47.3\n23.6\n14.166667\n\n\n4\n20160605\n51.4\n58.3\n43.2\n15.1\n10.777778"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#selecting-and-updating-data",
    "href": "lectures/week-03/2023-09-06.html#selecting-and-updating-data",
    "title": "Unique Values",
    "section": "Selecting and updating data",
    "text": "Selecting and updating data\n\nselection = data[0:5]\nselection\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n18.9\n18.611111\n\n\n1\n20160602\n65.8\n80.8\n55.0\n25.8\n18.777778\n\n\n2\n20160603\n68.4\n77.9\n55.6\n22.3\n20.222222\n\n\n3\n20160604\n57.5\n70.9\n47.3\n23.6\n14.166667\n\n\n4\n20160605\n51.4\n58.3\n43.2\n15.1\n10.777778\n\n\n\n\n\n\n\n\n# Select temp column values on rows 0-5\nselection = data.loc[0:5, \"TEMP\"]\nselection\n\n0    65.5\n1    65.8\n2    68.4\n3    57.5\n4    51.4\n5    52.2\nName: TEMP, dtype: float64\n\n\n\nSelecting a single row\n\nrow = data.loc[4]\nrow\n\nYEARMODA        2.016060e+07\nTEMP            5.140000e+01\nMAX             5.830000e+01\nMIN             4.320000e+01\nDIFF            1.510000e+01\nTEMP_CELSIUS    1.077778e+01\nName: 4, dtype: float64\n\n\n\nrow[\"TEMP\"]\n\n51.4\n\n\n\n\nselection = data.loc[0:5, [\"TEMP\", \"TEMP_CELSIUS\"]]\nselection\n\n\n\n\n\n\n\n\nTEMP\nTEMP_CELSIUS\n\n\n\n\n0\n65.5\n18.611111\n\n\n1\n65.8\n18.777778\n\n\n2\n68.4\n20.222222\n\n\n3\n57.5\n14.166667\n\n\n4\n51.4\n10.777778\n\n\n5\n52.2\n11.222222\n\n\n\n\n\n\n\n\nselection.at[0, \"TEMP\"]\n\n65.5\n\n\n\nselection.loc[0, \"TEMP\"]\n\n65.5\n\n\n\n\nSelection by position\n\n# Check the first rows\nprint(data.head())\nprint()\nprint(\"Value at position (0,0) is\", data.iloc[0, 0])\n\n   YEARMODA  TEMP   MAX   MIN  DIFF  TEMP_CELSIUS\n0  20160601  65.5  73.6  54.7  18.9     18.611111\n1  20160602  65.8  80.8  55.0  25.8     18.777778\n2  20160603  68.4  77.9  55.6  22.3     20.222222\n3  20160604  57.5  70.9  47.3  23.6     14.166667\n4  20160605  51.4  58.3  43.2  15.1     10.777778\n\nValue at position (0,0) is 20160601\n\n\n\ndata.iloc[-1, -1]\n\n18.722222222222225\n\n\n\ndata.tail()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\n\n\n\n\n25\n20160626\n69.6\n77.7\n60.3\n17.4\n20.888889\n\n\n26\n20160627\n60.7\n70.0\n57.6\n12.4\n15.944444\n\n\n27\n20160628\n65.4\n73.0\n55.8\n17.2\n18.555556\n\n\n28\n20160629\n65.8\n73.2\n59.7\n13.5\n18.777778\n\n\n29\n20160630\n65.7\n72.7\n59.2\n13.5\n18.722222\n\n\n\n\n\n\n\n\ndata.iloc[-1, -2]\n\n13.5"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#selections-using-listed-criteria",
    "href": "lectures/week-03/2023-09-06.html#selections-using-listed-criteria",
    "title": "Unique Values",
    "section": "Selections using listed criteria",
    "text": "Selections using listed criteria\n\n# List of values that will be used as basis for selecting the rows\nselection_criteria = [20160601, 20160608, 20160609]\n\n# Do the selection based on criteria applied to YEARMODA column\ndata.loc[data[\"YEARMODA\"].isin(selection_criteria)]\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n18.9\n18.611111\n\n\n7\n20160608\n54.2\n60.4\n47.5\n12.9\n12.333333\n\n\n8\n20160609\n49.4\n54.1\n45.7\n8.4\n9.666667"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#conditional-selection",
    "href": "lectures/week-03/2023-09-06.html#conditional-selection",
    "title": "Unique Values",
    "section": "Conditional selection",
    "text": "Conditional selection\n\ndata.TEMP_CELSIUS &gt; 15\n\n0      True\n1      True\n2      True\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10    False\n11    False\n12    False\n13     True\n14     True\n15    False\n16     True\n17    False\n18    False\n19     True\n20     True\n21     True\n22     True\n23     True\n24     True\n25     True\n26     True\n27     True\n28     True\n29     True\nName: TEMP_CELSIUS, dtype: bool\n\n\n\nwarm_temps = data[data.TEMP_CELSIUS &gt; 15]\nwarm_temps\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n18.9\n18.611111\n\n\n1\n20160602\n65.8\n80.8\n55.0\n25.8\n18.777778\n\n\n2\n20160603\n68.4\n77.9\n55.6\n22.3\n20.222222\n\n\n13\n20160614\n59.7\n67.8\n47.8\n20.0\n15.388889\n\n\n14\n20160615\n63.4\n70.3\n49.3\n21.0\n17.444444\n\n\n16\n20160617\n60.4\n70.7\n55.9\n14.8\n15.777778\n\n\n19\n20160620\n59.3\n69.1\n52.2\n16.9\n15.166667\n\n\n20\n20160621\n62.6\n71.4\n50.4\n21.0\n17.000000\n\n\n21\n20160622\n61.7\n70.2\n55.4\n14.8\n16.500000\n\n\n22\n20160623\n60.9\n67.1\n54.9\n12.2\n16.055556\n\n\n23\n20160624\n61.1\n68.9\n56.7\n12.2\n16.166667\n\n\n24\n20160625\n65.7\n75.4\n57.9\n17.5\n18.722222\n\n\n25\n20160626\n69.6\n77.7\n60.3\n17.4\n20.888889\n\n\n26\n20160627\n60.7\n70.0\n57.6\n12.4\n15.944444\n\n\n27\n20160628\n65.4\n73.0\n55.8\n17.2\n18.555556\n\n\n28\n20160629\n65.8\n73.2\n59.7\n13.5\n18.777778\n\n\n29\n20160630\n65.7\n72.7\n59.2\n13.5\n18.722222\n\n\n\n\n\n\n\n\nwarm_temps = data[(data[\"TEMP_CELSIUS\"] &gt; 15) & (data[\"YEARMODA\"] &gt;= 20160615)]\nwarm_temps\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\n\n\n\n\n14\n20160615\n63.4\n70.3\n49.3\n21.0\n17.444444\n\n\n16\n20160617\n60.4\n70.7\n55.9\n14.8\n15.777778\n\n\n19\n20160620\n59.3\n69.1\n52.2\n16.9\n15.166667\n\n\n20\n20160621\n62.6\n71.4\n50.4\n21.0\n17.000000\n\n\n21\n20160622\n61.7\n70.2\n55.4\n14.8\n16.500000\n\n\n22\n20160623\n60.9\n67.1\n54.9\n12.2\n16.055556\n\n\n23\n20160624\n61.1\n68.9\n56.7\n12.2\n16.166667\n\n\n24\n20160625\n65.7\n75.4\n57.9\n17.5\n18.722222\n\n\n25\n20160626\n69.6\n77.7\n60.3\n17.4\n20.888889\n\n\n26\n20160627\n60.7\n70.0\n57.6\n12.4\n15.944444\n\n\n27\n20160628\n65.4\n73.0\n55.8\n17.2\n18.555556\n\n\n28\n20160629\n65.8\n73.2\n59.7\n13.5\n18.777778\n\n\n29\n20160630\n65.7\n72.7\n59.2\n13.5\n18.722222\n\n\n\n\n\n\n\nSelectoins### Reset index\n\nwarm_temps = warm_temps.reset_index(drop=True)\nwarm_temps\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\n\n\n\n\n0\n20160615\n63.4\n70.3\n49.3\n21.0\n17.444444\n\n\n1\n20160617\n60.4\n70.7\n55.9\n14.8\n15.777778\n\n\n2\n20160620\n59.3\n69.1\n52.2\n16.9\n15.166667\n\n\n3\n20160621\n62.6\n71.4\n50.4\n21.0\n17.000000\n\n\n4\n20160622\n61.7\n70.2\n55.4\n14.8\n16.500000\n\n\n5\n20160623\n60.9\n67.1\n54.9\n12.2\n16.055556\n\n\n6\n20160624\n61.1\n68.9\n56.7\n12.2\n16.166667\n\n\n7\n20160625\n65.7\n75.4\n57.9\n17.5\n18.722222\n\n\n8\n20160626\n69.6\n77.7\n60.3\n17.4\n20.888889\n\n\n9\n20160627\n60.7\n70.0\n57.6\n12.4\n15.944444\n\n\n10\n20160628\n65.4\n73.0\n55.8\n17.2\n18.555556\n\n\n11\n20160629\n65.8\n73.2\n59.7\n13.5\n18.777778\n\n\n12\n20160630\n65.7\n72.7\n59.2\n13.5\n18.722222\n\n\n\n\n\n\n\n\nwarm_temps[\"MIN\"].hasnans\n\nFalse"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#sorting-data",
    "href": "lectures/week-03/2023-09-06.html#sorting-data",
    "title": "Unique Values",
    "section": "Sorting Data",
    "text": "Sorting Data\n\ndata.sort_values(by=\"TEMP\").head()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\n\n\n\n\n8\n20160609\n49.4\n54.1\n45.7\n8.4\n9.666667\n\n\n9\n20160610\n49.5\n55.9\n43.0\n12.9\n9.722222\n\n\n4\n20160605\n51.4\n58.3\n43.2\n15.1\n10.777778\n\n\n5\n20160606\n52.2\n59.7\n42.8\n16.9\n11.222222\n\n\n10\n20160611\n54.0\n62.1\n41.7\n20.4\n12.222222\n\n\n\n\n\n\n\n\ndata.sort_values(by=\"TEMP\", ascending=False).head()\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\n\n\n\n\n25\n20160626\n69.6\n77.7\n60.3\n17.4\n20.888889\n\n\n2\n20160603\n68.4\n77.9\n55.6\n22.3\n20.222222\n\n\n1\n20160602\n65.8\n80.8\n55.0\n25.8\n18.777778\n\n\n28\n20160629\n65.8\n73.2\n59.7\n13.5\n18.777778\n\n\n29\n20160630\n65.7\n72.7\n59.2\n13.5\n18.722222\n\n\n\n\n\n\n\n\n# Create a list of weekdays that matches with our data\n# The data covers 4 weeks + 2 days (i.e. altogether 30 days)\nweek_days = [\"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\", \"Mon\", \"Tue\"]\nday_list = week_days * 4 + week_days[:2]\n\n# Add the weekdays to our DataFrame\ndata[\"WEEKDAY\"] = day_list\ndata.head(10)\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\nWEEKDAY\n\n\n\n\n0\n20160601\n65.5\n73.6\n54.7\n18.9\n18.611111\nWed\n\n\n1\n20160602\n65.8\n80.8\n55.0\n25.8\n18.777778\nThu\n\n\n2\n20160603\n68.4\n77.9\n55.6\n22.3\n20.222222\nFri\n\n\n3\n20160604\n57.5\n70.9\n47.3\n23.6\n14.166667\nSat\n\n\n4\n20160605\n51.4\n58.3\n43.2\n15.1\n10.777778\nSun\n\n\n5\n20160606\n52.2\n59.7\n42.8\n16.9\n11.222222\nMon\n\n\n6\n20160607\n56.9\n65.1\n45.9\n19.2\n13.833333\nTue\n\n\n7\n20160608\n54.2\n60.4\n47.5\n12.9\n12.333333\nWed\n\n\n8\n20160609\n49.4\n54.1\n45.7\n8.4\n9.666667\nThu\n\n\n9\n20160610\n49.5\n55.9\n43.0\n12.9\n9.722222\nFri\n\n\n\n\n\n\n\n\ndata.sort_values(by=[\"WEEKDAY\", \"TEMP_CELSIUS\"], ascending=[True, False]).head(10)\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\nWEEKDAY\n\n\n\n\n2\n20160603\n68.4\n77.9\n55.6\n22.3\n20.222222\nFri\n\n\n23\n20160624\n61.1\n68.9\n56.7\n12.2\n16.166667\nFri\n\n\n16\n20160617\n60.4\n70.7\n55.9\n14.8\n15.777778\nFri\n\n\n9\n20160610\n49.5\n55.9\n43.0\n12.9\n9.722222\nFri\n\n\n26\n20160627\n60.7\n70.0\n57.6\n12.4\n15.944444\nMon\n\n\n19\n20160620\n59.3\n69.1\n52.2\n16.9\n15.166667\nMon\n\n\n12\n20160613\n58.3\n68.2\n47.3\n20.9\n14.611111\nMon\n\n\n5\n20160606\n52.2\n59.7\n42.8\n16.9\n11.222222\nMon\n\n\n24\n20160625\n65.7\n75.4\n57.9\n17.5\n18.722222\nSat\n\n\n3\n20160604\n57.5\n70.9\n47.3\n23.6\n14.166667\nSat\n\n\n\n\n\n\n\n\ndata.sort_values(by=[\"WEEKDAY\"], ascending=[True]).head(10)\n\n\n\n\n\n\n\n\nYEARMODA\nTEMP\nMAX\nMIN\nDIFF\nTEMP_CELSIUS\nWEEKDAY\n\n\n\n\n2\n20160603\n68.4\n77.9\n55.6\n22.3\n20.222222\nFri\n\n\n16\n20160617\n60.4\n70.7\n55.9\n14.8\n15.777778\nFri\n\n\n23\n20160624\n61.1\n68.9\n56.7\n12.2\n16.166667\nFri\n\n\n9\n20160610\n49.5\n55.9\n43.0\n12.9\n9.722222\nFri\n\n\n12\n20160613\n58.3\n68.2\n47.3\n20.9\n14.611111\nMon\n\n\n19\n20160620\n59.3\n69.1\n52.2\n16.9\n15.166667\nMon\n\n\n5\n20160606\n52.2\n59.7\n42.8\n16.9\n11.222222\nMon\n\n\n26\n20160627\n60.7\n70.0\n57.6\n12.4\n15.944444\nMon\n\n\n17\n20160618\n57.3\n62.8\n54.0\n8.8\n14.055556\nSat\n\n\n10\n20160611\n54.0\n62.1\n41.7\n20.4\n12.222222\nSat"
  },
  {
    "objectID": "lectures/week-03/2023-09-06.html#visualization",
    "href": "lectures/week-03/2023-09-06.html#visualization",
    "title": "Unique Values",
    "section": "Visualization",
    "text": "Visualization\n\nimport seaborn as sns\nsns.set()\n\n\nimport pandas\n\n\ndf = pandas.read_csv(\"data/shared/covid/covid_combined.csv\",\n                     index_col='date', parse_dates=True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nstate\nfips\ncases\ndeaths\ndtc100\npopulation\ndeaths100k\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2020-01-21\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-22\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-23\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-24\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n2020-01-25\nWashington\n53\n1\n0\n0.0\n7614893\n0.0\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nstate\nfips\ncases\ndeaths\ndtc100\npopulation\ndeaths100k\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2020-07-29\nNorthern Mariana Islands\n69\n40\n2\n5.000000\n55194\n3.623582\n\n\n2020-07-30\nNorthern Mariana Islands\n69\n42\n2\n4.761905\n55194\n3.623582\n\n\n2020-07-31\nNorthern Mariana Islands\n69\n42\n2\n4.761905\n55194\n3.623582\n\n\n2020-08-01\nNorthern Mariana Islands\n69\n44\n2\n4.545455\n55194\n3.623582\n\n\n2020-08-02\nNorthern Mariana Islands\n69\n45\n2\n4.444444\n55194\n3.623582\n\n\n\n\n\n\n\n\ndf.shape\n\n(8287, 7)\n\n\n\ndf.index\n\nDatetimeIndex(['2020-01-21', '2020-01-22', '2020-01-23', '2020-01-24',\n               '2020-01-25', '2020-01-26', '2020-01-27', '2020-01-28',\n               '2020-01-29', '2020-01-30',\n               ...\n               '2020-07-24', '2020-07-25', '2020-07-26', '2020-07-27',\n               '2020-07-28', '2020-07-29', '2020-07-30', '2020-07-31',\n               '2020-08-01', '2020-08-02'],\n              dtype='datetime64[ns]', name='date', length=8287, freq=None)\n\n\n\nlast_df = df.loc['2020-08-02']\n\n\nlast_df.shape\n\n(54, 7)\n\n\n\nlast_df.reset_index(inplace=True)\nlast_df\n\n\n\n\n\n\n\n\ndate\nstate\nfips\ncases\ndeaths\ndtc100\npopulation\ndeaths100k\n\n\n\n\n0\n2020-08-02\nWashington\n53\n60161\n1680\n2.792507\n7614893\n22.062030\n\n\n1\n2020-08-02\nIllinois\n17\n183662\n7718\n4.202285\n12671821\n60.906795\n\n\n2\n2020-08-02\nCalifornia\n6\n515937\n9399\n1.821734\n39512223\n23.787576\n\n\n3\n2020-08-02\nArizona\n4\n178473\n3769\n2.111804\n7278717\n51.781104\n\n\n4\n2020-08-02\nMassachusetts\n25\n118458\n8638\n7.292036\n6949503\n124.296658\n\n\n5\n2020-08-02\nWisconsin\n55\n58990\n956\n1.620614\n5822434\n16.419250\n\n\n6\n2020-08-02\nTexas\n48\n452826\n7515\n1.659578\n28995881\n25.917474\n\n\n7\n2020-08-02\nNebraska\n31\n26702\n338\n1.265823\n1934408\n17.473046\n\n\n8\n2020-08-02\nUtah\n49\n41175\n313\n0.760170\n3205958\n9.763072\n\n\n9\n2020-08-02\nOregon\n41\n19097\n331\n1.733257\n4217737\n7.847810\n\n\n10\n2020-08-02\nFlorida\n12\n487124\n7083\n1.454045\n21477737\n32.978335\n\n\n11\n2020-08-02\nNew York\n36\n421008\n32401\n7.696053\n19453561\n166.555624\n\n\n12\n2020-08-02\nRhode Island\n44\n19022\n1007\n5.293870\n1059361\n95.057303\n\n\n13\n2020-08-02\nGeorgia\n13\n177556\n3758\n2.116515\n10617423\n35.394653\n\n\n14\n2020-08-02\nNew Hampshire\n33\n6634\n417\n6.285800\n1359711\n30.668282\n\n\n15\n2020-08-02\nNorth Carolina\n37\n125339\n1996\n1.592481\n10488084\n19.031121\n\n\n16\n2020-08-02\nNew Jersey\n34\n184225\n15836\n8.596010\n8882190\n178.289363\n\n\n17\n2020-08-02\nColorado\n8\n47799\n1846\n3.862005\n5758736\n32.055646\n\n\n18\n2020-08-02\nMaryland\n24\n90835\n3515\n3.869654\n6045680\n58.140689\n\n\n19\n2020-08-02\nNevada\n32\n50270\n833\n1.657052\n3080156\n27.044085\n\n\n20\n2020-08-02\nTennessee\n47\n106804\n1062\n0.994345\n6833174\n15.541826\n\n\n21\n2020-08-02\nHawaii\n15\n2219\n25\n1.126634\n1415872\n1.765696\n\n\n22\n2020-08-02\nIndiana\n18\n69531\n2975\n4.278667\n6732219\n44.190482\n\n\n23\n2020-08-02\nKentucky\n21\n31966\n759\n2.374398\n4467673\n16.988710\n\n\n24\n2020-08-02\nMinnesota\n27\n55987\n1654\n2.954257\n5639632\n29.328155\n\n\n25\n2020-08-02\nOklahoma\n40\n38201\n550\n1.439753\n3956971\n13.899521\n\n\n26\n2020-08-02\nPennsylvania\n42\n118038\n7274\n6.162422\n12801989\n56.819296\n\n\n27\n2020-08-02\nSouth Carolina\n45\n91788\n1777\n1.935983\n5148714\n34.513473\n\n\n28\n2020-08-02\nDistrict of Columbia\n11\n12274\n586\n4.774320\n705749\n83.032353\n\n\n29\n2020-08-02\nKansas\n20\n28341\n361\n1.273773\n2913314\n12.391387\n\n\n30\n2020-08-02\nMissouri\n29\n52550\n1311\n2.494767\n6137428\n21.360739\n\n\n31\n2020-08-02\nVermont\n50\n1426\n57\n3.997195\n623989\n9.134776\n\n\n32\n2020-08-02\nVirginia\n51\n91782\n2218\n2.416596\n8535519\n25.985532\n\n\n33\n2020-08-02\nConnecticut\n9\n49810\n4432\n8.897812\n3565287\n124.309768\n\n\n34\n2020-08-02\nIowa\n19\n45723\n878\n1.920259\n3155070\n27.828226\n\n\n35\n2020-08-02\nLouisiana\n22\n119861\n4007\n3.343039\n4648794\n86.194398\n\n\n36\n2020-08-02\nOhio\n39\n93031\n3529\n3.793359\n11689100\n30.190519\n\n\n37\n2020-08-02\nMichigan\n26\n91857\n6460\n7.032670\n9986857\n64.685016\n\n\n38\n2020-08-02\nSouth Dakota\n46\n8955\n135\n1.507538\n884659\n15.260117\n\n\n39\n2020-08-02\nArkansas\n5\n43810\n464\n1.059119\n3017825\n15.375312\n\n\n40\n2020-08-02\nDelaware\n10\n14949\n585\n3.913305\n973764\n60.076158\n\n\n41\n2020-08-02\nMississippi\n28\n60553\n1703\n2.812412\n2976149\n57.221597\n\n\n42\n2020-08-02\nNew Mexico\n35\n21016\n654\n3.111915\n2096829\n31.189954\n\n\n43\n2020-08-02\nNorth Dakota\n38\n6664\n109\n1.635654\n762062\n14.303298\n\n\n44\n2020-08-02\nWyoming\n56\n2808\n26\n0.925926\n578759\n4.492371\n\n\n45\n2020-08-02\nAlaska\n2\n3982\n22\n0.552486\n731545\n3.007334\n\n\n46\n2020-08-02\nMaine\n23\n3958\n123\n3.107630\n1344212\n9.150342\n\n\n47\n2020-08-02\nAlabama\n1\n91444\n1627\n1.779231\n4903185\n33.182513\n\n\n48\n2020-08-02\nIdaho\n16\n21465\n197\n0.917773\n1787065\n11.023662\n\n\n49\n2020-08-02\nMontana\n30\n4193\n61\n1.454806\n1068778\n5.707453\n\n\n50\n2020-08-02\nPuerto Rico\n72\n18411\n230\n1.249253\n3193694\n7.201692\n\n\n51\n2020-08-02\nGuam\n66\n1328\n6\n0.451807\n165718\n3.620609\n\n\n52\n2020-08-02\nWest Virginia\n54\n6854\n117\n1.707032\n1792147\n6.528482\n\n\n53\n2020-08-02\nNorthern Mariana Islands\n69\n45\n2\n4.444444\n55194\n3.623582\n\n\n\n\n\n\n\n\n_ = sns.displot(last_df.deaths100k)\n\n\n\n\n\n_ = sns.displot(last_df.deaths100k, rug=True)"
  },
  {
    "objectID": "lectures/0821/0821.html",
    "href": "lectures/0821/0821.html",
    "title": "Course Introduction",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "lectures/week-02/2023-08-30-functions-scripts.html",
    "href": "lectures/week-02/2023-08-30-functions-scripts.html",
    "title": "Functions",
    "section": "",
    "text": "def celsius_to_fahr(temp):\n    return 9 / 5 * temp + 32\ncelsius_to_fahr(40)\n\n104.0\ncelsius_to_fahr(0)\n\n32.0\ndef how_long(a_string):\n    print(len(a_string))\nhow_long('abcd')\n\n4\ndef how_long(a_string):\n    len(a_string)\nhow_long('abcde')\ndef kelvins_to_celsius(temp_kelvins):\n    return temp_kelvins - 273.15\nkelvins_to_celsius(temp_kelvins=0)\n\n-273.15\nprint(\"Absolute zero in Celsius is:\", kelvins_to_celsius(0))\n\nAbsolute zero in Celsius is: -273.15\ndef kelvins_to_fahr(temp_kelvins):\n    temp_celsius = kelvins_to_celsius(temp_kelvins)\n    temp_fahr = celsius_to_fahr(temp_celsius)\n    return temp_fahr\nkelvins_to_fahr(0)\n\n-459.66999999999996\ndef kelvins_to_celsius(temp_kelvins):\n    \"\"\"Convers temperature in Kelvins to degrees celsius.\"\"\"\n    return temp_kelvins - 273.15\nkelvins_to_celsius?\n\n\nSignature: kelvins_to_celsius(temp_kelvins)\nDocstring: Convers temperature in Kelvins to degrees celsius.\nFile:      /tmp/ipykernel_1023841/522041369.py\nType:      function\nkelvins_to_celsius??\n\n\nSignature: kelvins_to_celsius(temp_kelvins)\nSource:   \ndef kelvins_to_celsius(temp_kelvins):\n    \"\"\"Convers temperature in Kelvins to degrees celsius.\"\"\"\n    return temp_kelvins - 273.15\nFile:      /tmp/ipykernel_1023841/522041369.py\nType:      function\ndef kelvins_to_celsius(temp_kelvins):\n    \"\"\"Convers temperature in Kelvins to degrees celsius.\n    \n    Parameters\n    ----------\n    temp_kelvins: &lt;numerical&gt;\n        Temperature in Kelvins\n        \n    Returns\n    -------\n    &lt;float&gt;\n       Converted temperature\n    \n    \"\"\"\n    return temp_kelvins - 273.15\nkelvins_to_celsius?\n\n\nSignature: kelvins_to_celsius(temp_kelvins)\nDocstring:\nConvers temperature in Kelvins to degrees celsius.\nParameters\n----------\ntemp_kelvins: &lt;numerical&gt;\n    Temperature in Kelvins\n    \nReturns\n-------\n&lt;float&gt;\n   Converted temperature\nFile:      /tmp/ipykernel_1023841/1189373821.py\nType:      function"
  },
  {
    "objectID": "lectures/week-02/2023-08-30-functions-scripts.html#using-modules-and-scripts",
    "href": "lectures/week-02/2023-08-30-functions-scripts.html#using-modules-and-scripts",
    "title": "Functions",
    "section": "Using modules and scripts",
    "text": "Using modules and scripts\n\n%ls\n\n2023-08-23.ipynb                      2023-08-30-functions-scripts.ipynb\n2023-08-28-python-introduction.ipynb  temp_converter.py\n\n\n\n%pwd\n\n'/home/jupyter-serge/sessions'\n\n\n\nfrom temp_converter import celsius_to_fahr\n\n\ncelsius_to_fahr(100)\n\nFrom our cool function\n\n\n212.0\n\n\n\nimport math"
  },
  {
    "objectID": "lectures/week-13/2023-11-13-interpolation_deterministic.html",
    "href": "lectures/week-13/2023-11-13-interpolation_deterministic.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (16, 9)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nprecip = gpd.read_file(\"precip_sd.geojson\")\nprecip.plot(column='inches', legend=True);\ncounty = gpd.read_file(\"sdcounty.geojson\")\nm = county.explore()\nprecip.explore(column='inches', m=m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "lectures/week-13/2023-11-13-interpolation_deterministic.html#interpolation-methods",
    "href": "lectures/week-13/2023-11-13-interpolation_deterministic.html#interpolation-methods",
    "title": "Spatial Interpolation",
    "section": "Interpolation Methods",
    "text": "Interpolation Methods\n\nVoronoi Partition Interpolation\n\nfrom libpysal.cg import voronoi_frames\npoints = [(10.2, 5.1), (4.7, 2.2), (5.3, 5.7), (2.7, 5.3)]\nregions_df, points_df = voronoi_frames(points)\nregions_df.shape\n(4, 1)\n\n(4, 1)\n\n\n\nregions_df.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n#libpysal.cg.voronoi_frames(points, radius=None, clip='extent')\npoints = precip.get_coordinates().values\n\n\npoints\n\narray([[-116.11392643,   32.75307635],\n       [-117.12067477,   33.22819891],\n       [-117.54677523,   33.41129941],\n       [-116.9663248 ,   33.16548448],\n       [-116.83479485,   32.61090996],\n       [-116.95845807,   33.33033923],\n       [-117.04029442,   32.63195628],\n       [-117.37505941,   33.30728301],\n       [-116.40414067,   32.66621206],\n       [-117.19270233,   32.77478741],\n       [-116.61247649,   33.33028661],\n       [-116.7430757 ,   33.34341133],\n       [-116.21969099,   32.71825351],\n       [-116.20299833,   33.31887981],\n       [-117.05371254,   32.73833341],\n       [-116.25599174,   32.86774382],\n       [-117.1993245 ,   33.16287784],\n       [-117.1449864 ,   32.67494802],\n       [-116.95429142,   32.89167509],\n       [-117.13095076,   32.89629192],\n       [-116.74322953,   32.77737283],\n       [-116.71168297,   32.62932523],\n       [-116.10634354,   33.3975104 ],\n       [-117.21780769,   33.2680768 ],\n       [-116.56091847,   33.22526384]])\n\n\n\nv_gdf, v_p = voronoi_frames(points)\n\nv_gdf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nv_gdf, v_p = voronoi_frames(points, clip=county.geometry[0])\n\nv_gdf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nv_gdf['inches'] = precip.inches\n\n\nv_gdf.plot(column='inches', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nbase = v_gdf.plot(column='inches', legend=True,\n                 edgecolor='gray')\nprecip.plot(ax=base, color='w');\n\n\n\n\n\n\nInterpolate to Grids\n\nhex3 the county\nnearest neighbor interpolators\n\n\n\nimport tobler\n\n\ncounty_utm = county.to_crs(county.estimate_utm_crs())\nprecip_utm = precip.to_crs(precip.estimate_utm_crs())\n\n\nfrom tobler.util import h3fy\n\n\ncounty_h3 = h3fy(county_utm)\n\n\ncounty_h3.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nm = county_h3.plot(color='grey')\nprecip_utm.plot(column='inches', ax=m, legend=True);\n\n\n\n\n\n\nNearest neighbor\n\nhcents = county_h3.centroid\n\n\nm = hcents.plot(color='r')\nprecip_utm.plot(column='inches', ax=m, legend=True);\n\n\n\n\n\ngpd.sjoin_nearest(county_h3, precip_utm, distance_col=\"distances\",\n    lsuffix=\"left\", rsuffix=\"right\", exclusive=True)\n\n\n\n\n\n\n\n\ngeometry\nindex_right\ninches\ndistances\n\n\nhex_id\n\n\n\n\n\n\n\n\n8629a6a0fffffff\nPOLYGON ((537048.394 3689328.650, 540042.930 3...\n10\n31\n1591.310439\n\n\n8629a6a07ffffff\nPOLYGON ((541604.239 3684268.446, 544599.450 3...\n10\n31\n1232.212148\n\n\n8629a6a57ffffff\nPOLYGON ((532494.502 3694386.661, 535488.364 3...\n10\n31\n7342.913667\n\n\n8629a6b57ffffff\nPOLYGON ((532618.678 3676253.158, 535613.850 3...\n10\n31\n5309.224420\n\n\n8629a6a17ffffff\nPOLYGON ((548376.217 3685745.697, 551371.779 3...\n10\n31\n8160.937377\n\n\n...\n...\n...\n...\n...\n\n\n8629a42d7ffffff\nPOLYGON ((453619.190 3683208.511, 456608.225 3...\n2\n20\n6756.531707\n\n\n8629a092fffffff\nPOLYGON ((455838.093 3689733.627, 458826.856 3...\n2\n20\n3101.906869\n\n\n8629a090fffffff\nPOLYGON ((458056.590 3696257.420, 461045.077 3...\n2\n20\n5122.554721\n\n\n8629a6937ffffff\nPOLYGON ((578262.158 3625515.280, 581264.072 3...\n0\n7\n4092.672359\n\n\n86485966fffffff\nPOLYGON ((582836.817 3620429.699, 585839.404 3...\n0\n7\n0.000000\n\n\n\n\n283 rows × 4 columns\n\n\n\n\ncounty_h3.shape\n\n(283, 1)\n\n\n\ncounty_h3['nn1_est'] = gpd.sjoin_nearest(county_h3, precip_utm, distance_col=\"distances\",\n    lsuffix=\"left\", rsuffix=\"right\", exclusive=True).inches\n\n\ncounty_h3.plot(column='nn1_est', legend=True);\n\n\n\n\n\n\nKnn5\n\nX = [[0], [1], [2], [3]]\ny = [0, 0, 1, 1]\nfrom sklearn.neighbors import KNeighborsRegressor\nneigh = KNeighborsRegressor(n_neighbors=2)\nneigh.fit(X, y)\nprint(neigh.predict([[1.5]]))\n\n\n[0.5]\n\n\n\n# Set number of neighbors to use\nneighbors = 5\n\n# Initialize KNN regressor\nknn_regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"uniform\") # no distance decay distance\n\n# Fit to observed locations\nknn_regressor.fit(precip_utm.get_coordinates(), precip_utm.inches)\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor()\n\n\n\nknn_regressor.predict(hcents.get_coordinates())\n\narray([23.6, 20.4, 20. ,  9.2, 23.6, 22.4, 15. , 19.4, 15. , 20.4, 21.8,\n       22.2, 21.4, 16.8, 23.6, 16.8, 16.8, 13.2, 15. , 16.8, 20.6, 18.6,\n       20.6, 22. , 21.8, 23.6, 22. , 24.8, 20. , 15. , 17.4, 23.6, 20. ,\n       15.6, 17.6, 27.4, 21.2, 19.4, 17.6, 25. , 19. , 22.6, 27.4, 20.6,\n       17.6, 25. , 20.4, 22.4, 27.6, 21.2,  9.2, 17. , 23.6, 19.4, 12.6,\n       24.2, 18.6, 16.8, 23.2, 21.8, 17. , 27.4, 25. , 22.6, 23.2, 17. ,\n       15. , 23. , 23.6, 17.6, 20.4, 22. , 25. , 15.6, 21.6, 24.2, 23.6,\n       21.4, 20.6, 17. , 18.8, 20.6, 20. , 15. , 16.8, 23.6, 16.8, 15. ,\n       25.6, 22. , 24.2, 19.2, 19. , 25.4, 17.6, 19. , 15.6, 16.8, 15. ,\n       21.2, 20.6, 19. , 21.8, 15. , 16.8, 19. , 25.6, 27.4, 18.6, 21.8,\n       25. , 23. , 17. , 20.6, 15.6, 24.4, 27.4, 20.4, 15.6, 13.2, 23.6,\n       26. , 13.2, 20. , 19.2, 16.8, 20.4, 27.4, 22.4, 27.6, 17. , 16.8,\n       22. , 16.8, 26. , 22.4, 27.4, 20.6, 26.6, 16.8, 19. , 19. , 10.2,\n       23.6, 22.6, 26.6, 20.6, 20.6, 21.8, 20.6, 23.2, 25.4, 20.6, 27.4,\n       13.2, 20.4, 16.8, 21.8, 15. , 21.8, 21.2, 25.6, 10.2, 23.6, 18.4,\n       19.4, 19.6, 23.6, 16.8, 27.4, 19. , 20.6, 15. , 27.4, 27.4, 21.8,\n       12.6, 24.2, 27.4, 15.6, 15.6, 25. , 16.8, 20.6, 27.4, 23.2, 25. ,\n       12.6, 21.2, 19. , 16.4, 25. , 23.6, 21.8, 23.6, 23.6, 15.6, 21.8,\n       16.8, 16.8, 15.6, 27.4, 16.4, 27.6, 27.4, 21.4, 12.6, 27.4, 23.2,\n       16.8, 16.8, 15. , 23.6, 19.4, 24.2, 27.4, 22.4, 23. , 23.6, 27.4,\n       19.4, 16.8, 20.6, 22.4, 20.6, 17. , 13.2, 23.6, 19.4, 21.8, 16.8,\n       20.6, 17.6, 23.6, 15.6, 20.4, 15.6, 16.6, 21.4, 25.8, 25. , 20.6,\n       19. , 20.6, 19. , 15. , 27.6, 27.4, 21.2, 21.2, 20. , 20.6, 20.6,\n       19.4, 21.8, 19.4, 19. , 19. , 20.6, 19.4, 17. , 16.8, 20. , 19. ,\n       16.4, 16.8, 15. , 25. , 19. , 25. , 23.2, 20.6, 10.2, 20. , 17. ,\n       20. , 19. , 20.8, 24.2, 20.6, 17. , 15. , 19. ])\n\n\n\ncounty_h3['nn5_est'] = knn_regressor.predict(hcents.get_coordinates())\n\n\ncounty_h3.plot(column='nn5_est', legend=True);\n\n\n\n\n\nimport matplotlib.pyplot as plt\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\ncounty_h3.plot(column='nn1_est',ax=ax1)\ncounty_h3.plot(column='nn5_est',ax=ax2)\nax1.set_title(\"knn=1\")\nax2.set_title(\"knn=5\");\n\n\n\n\n\n\n# Set number of neighbors to use\nneighbors = 5\n\n# Initialize KNN regressor\nknn_regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"distance\") # inverse distance weighting\n# Fit to observed locations\nknn_regressor.fit(precip_utm.get_coordinates(), precip_utm.inches)\n\nKNeighborsRegressor(weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor(weights='distance')\n\n\n\ncounty_h3['nn5id_est'] = knn_regressor.predict(hcents.get_coordinates())\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\ncounty_h3.plot(column='nn5_est',ax=ax1)\ncounty_h3.plot(column='nn5id_est',ax=ax2)\nax1.set_title(\"knn=5 (Unweighted)\")\nax2.set_title(\"knn=5 (Inverse Distance Weights)\");"
  },
  {
    "objectID": "lectures/week-13/2023-11-13-interpolation.html",
    "href": "lectures/week-13/2023-11-13-interpolation.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (16, 9)\nimport rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nprecip = gpd.read_file(\"precip_sd.geojson\")\nprecip.plot(column='inches', legend=True);\ncounty = gpd.read_file(\"sdcounty.geojson\")\nm = county.explore()\nprecip.explore(column='inches', m=m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "lectures/week-13/2023-11-13-interpolation.html#interpolation-methods",
    "href": "lectures/week-13/2023-11-13-interpolation.html#interpolation-methods",
    "title": "Spatial Interpolation",
    "section": "Interpolation Methods",
    "text": "Interpolation Methods\n\nVoronoi Partition Interpolation\n\nfrom libpysal.cg import voronoi_frames\npoints = [(10.2, 5.1), (4.7, 2.2), (5.3, 5.7), (2.7, 5.3)]\nregions_df, points_df = voronoi_frames(points)\nregions_df.shape\n(4, 1)\n\n(4, 1)\n\n\n\nregions_df.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n#libpysal.cg.voronoi_frames(points, radius=None, clip='extent')\npoints = precip.get_coordinates().values\n\n\npoints\n\narray([[-116.11485557,   33.06671401],\n       [-116.9527008 ,   32.95272273],\n       [-117.28707993,   32.94648004],\n       [-116.81087912,   32.74126523],\n       [-116.67642472,   33.06180036],\n       [-116.75536004,   33.38198687],\n       [-116.853125  ,   32.91276871],\n       [-117.2845249 ,   32.8166362 ],\n       [-116.30785686,   32.87339294],\n       [-117.05423452,   33.2113287 ],\n       [-117.08576883,   32.62829508],\n       [-116.97452738,   32.73489854],\n       [-117.04123146,   33.30555124],\n       [-116.7363531 ,   32.5860575 ],\n       [-116.6968592 ,   32.91063226],\n       [-116.30902366,   32.61490005],\n       [-117.12014811,   33.02207574],\n       [-116.23936964,   32.79254988],\n       [-116.25780398,   32.95910185],\n       [-116.54036451,   32.68174022],\n       [-116.39372071,   32.9806681 ],\n       [-116.3174487 ,   32.71622031],\n       [-116.12021022,   33.21206119],\n       [-116.39070088,   33.42723824],\n       [-116.51450478,   33.11339592]])\n\n\n\nv_gdf, v_p = voronoi_frames(points)\n\nv_gdf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nv_gdf, v_p = voronoi_frames(points, clip=county.geometry[0])\n\nv_gdf.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nv_gdf['inches'] = precip.inches\n\n\nv_gdf.plot(column='inches', legend=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nbase = v_gdf.plot(column='inches', legend=True,\n                 edgecolor='gray')\nprecip.plot(ax=base, color='w');\n\n\n\n\n\n\nInterpolate to Grids\n\nhex3 the county\nnearest neighbor interpolators\n\n\n\nimport tobler\n\n\ncounty_utm = county.to_crs(county.estimate_utm_crs())\nprecip_utm = precip.to_crs(precip.estimate_utm_crs())\n\n\nfrom tobler.util import h3fy\n\n\ncounty_h3 = h3fy(county_utm)\n\n/home/serge/miniforge3/envs/385f23/lib/python3.11/site-packages/pyproj/crs/crs.py:1293: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n  proj = self._crs.to_proj4(version=version)\n\n\n\ncounty_h3.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nm = county_h3.plot(color='grey')\nprecip_utm.plot(column='inches', ax=m, legend=True);\n\n\n\n\n\n\nNearest neighbor\n\nhcents = county_h3.centroid\n\n\nm = hcents.plot(color='r')\nprecip_utm.plot(column='inches', ax=m, legend=True);\n\n\n\n\n\ngpd.sjoin_nearest(county_h3, precip_utm, distance_col=\"distances\",\n    lsuffix=\"left\", rsuffix=\"right\", exclusive=True)\n\n\n\n\n\n\n\n\ngeometry\nindex_right\ninches\ndistances\n\n\nhex_id\n\n\n\n\n\n\n\n\n8629a40cfffffff\nPOLYGON ((474069.136 3651385.625, 477061.797 3...\n2\n15\n6062.970244\n\n\n8629a42a7ffffff\nPOLYGON ((460461.590 3666569.984, 463452.235 3...\n2\n15\n24697.772908\n\n\n8629a40f7ffffff\nPOLYGON ((483150.719 3641251.976, 486144.722 3...\n2\n15\n5521.788637\n\n\n8629a439fffffff\nPOLYGON ((464995.449 3661510.686, 467986.766 3...\n2\n15\n18072.743746\n\n\n8629a42b7ffffff\nPOLYGON ((467214.759 3668042.842, 470205.811 3...\n2\n15\n23421.236954\n\n\n...\n...\n...\n...\n...\n\n\n8629a681fffffff\nPOLYGON ((575763.560 3655337.115, 578763.172 3...\n0\n5\n3987.765641\n\n\n8629a6817ffffff\nPOLYGON ((580334.753 3650261.424, 583335.039 3...\n0\n5\n2137.557172\n\n\n8629a68e7ffffff\nPOLYGON ((571194.247 3660410.644, 574193.184 3...\n0\n5\n9356.459172\n\n\n8629a68afffffff\nPOLYGON ((582548.849 3656813.176, 585548.807 3...\n0\n5\n0.000000\n\n\n8629a68f7ffffff\nPOLYGON ((577977.582 3661886.970, 580976.865 3...\n0\n5\n5225.115803\n\n\n\n\n283 rows × 4 columns\n\n\n\n\ncounty_h3.shape\n\n(283, 1)\n\n\n\ncounty_h3['nn1_est'] = gpd.sjoin_nearest(county_h3, precip_utm, distance_col=\"distances\",\n    lsuffix=\"left\", rsuffix=\"right\", exclusive=True).inches\n\n\ncounty_h3.plot(column='nn1_est', legend=True);\n\n\n\n\n\n\nKnn5\n\nX = [[0], [1], [2], [3]]\ny = [0, 0, 1, 1]\nfrom sklearn.neighbors import KNeighborsRegressor\nneigh = KNeighborsRegressor(n_neighbors=2)\nneigh.fit(X, y)\nprint(neigh.predict([[1.5]]))\n\n\n[0.5]\n\n\n\n# Set number of neighbors to use\nneighbors = 5\n\n# Initialize KNN regressor\nknn_regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"uniform\") # no distance decay distance\n\n# Fit to observed locations\nknn_regressor.fit(precip_utm.get_coordinates(), precip_utm.inches)\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor()\n\n\n\nknn_regressor.predict(hcents.get_coordinates())\n\narray([19.8, 21.2, 19.4, 24.8, 19.4, 26. , 12.4, 21.8, 20.8, 20.2, 15.8,\n       22.4,  9.2, 23.6, 19.4, 23.6, 15.4, 20.8, 21.8, 24.6, 24.8,  9.8,\n       23.6, 10.6, 19.4, 23.8, 24.8,  9.2, 20.2, 23.2, 10.6,  9.8, 26.4,\n       24.8, 20.4, 24.8, 26. , 10.4, 22. , 22.2, 19.4, 19.4, 18.6, 20.8,\n       14.6, 23.8, 24.6, 24.8,  8.4, 22.6, 24.6, 21.8, 18.6, 22.6, 18.4,\n       21.8, 21. , 18.6, 19.4, 26. , 21.8, 15.4, 26. , 17.4, 12.4, 20.8,\n       15.4, 25. ,  7.4, 21.8, 23.4, 19.4, 23.4, 24.8, 21.8, 25. , 21.8,\n       21.8, 21.8,  7.4,  7.8, 26. , 19. , 20.2, 20.2, 24.8, 21.2, 20.2,\n       15.4, 22.6, 15.4,  9.8,  7.4, 20.2, 19.8, 15.6, 15.4, 11.4, 21.8,\n       19. , 25.6, 18.4,  8.4, 12.4, 15.4, 18.4, 18.6, 20.2, 17.6, 24.8,\n       15.4, 21.2, 20.2, 26.4, 10.6, 21.8, 19. , 18.4, 19.4, 24.8, 21.8,\n       21.4, 21.6, 21.8, 25. , 24.6, 22.6, 22. , 13.4, 19. , 19.4, 21.8,\n       22.4, 23.6, 24.8, 24.8, 20.8, 22.2, 21.8, 25. , 12.4, 20. ,  8.4,\n       21. , 19.4,  8.4, 19.4, 19.6, 20.2, 17.6, 23.6, 22.2, 11.4, 19.4,\n       21.8, 21.8, 24.8, 17.6, 19.4, 20.2, 23.8, 17.4, 25. , 23.6, 23.6,\n       14.2, 19.6, 18.4, 21. , 20.2,  8.4, 21.8,  9.8, 16.6, 23.6, 24.8,\n       20.2, 21. ,  9.8, 19.6, 18.6, 16.6, 15.4, 23.2, 19.4, 23.8, 21. ,\n       16.4, 10.4, 21.8, 23.4, 20.2, 21.8, 25.6, 21.6,  9.2, 11.4,  8.4,\n       20.2, 22.2, 22.4, 17.6, 23.6, 22.6, 18.4, 19.8, 19.4, 24.8, 21.8,\n       23.4,  9.8, 19.8, 19.4,  9.2, 23.4, 21.8, 10.6, 24.8, 25. , 24.8,\n        9.8, 18.6, 23.6, 25. , 22.6, 16.6, 21.6, 18.4, 25. ,  8.4,  9.8,\n       19. , 19.4, 15.4, 25.8, 19.4, 19.8, 21.8,  9.8, 23.6, 19.4, 21.8,\n       24.8, 19.4, 19.4, 25. , 24.8, 25. , 24.6, 13.4,  7.8, 13.4, 24.8,\n        9.2, 20.8, 16.6, 22.2,  7.4, 19.4, 18.6, 16.6,  7.4, 19.6, 19.8,\n       22.2, 12.4, 21.8, 23.6,  7.4, 21. , 10.6, 22.6, 21.8, 12.4, 19.4,\n       16.6, 20.2, 23.6, 24.8, 21.6, 18.4, 24.8, 23.6])\n\n\n\ncounty_h3['nn5_est'] = knn_regressor.predict(hcents.get_coordinates())\n\n\ncounty_h3.plot(column='nn5_est', legend=True);\n\n\n\n\n\nimport matplotlib.pyplot as plt\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\ncounty_h3.plot(column='nn1_est',ax=ax1)\ncounty_h3.plot(column='nn5_est',ax=ax2)\nax1.set_title(\"knn=1\")\nax2.set_title(\"knn=5\");\n\n\n\n\n\n\n# Set number of neighbors to use\nneighbors = 5\n\n# Initialize KNN regressor\nknn_regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"distance\") # inverse distance weighting\n# Fit to observed locations\nknn_regressor.fit(precip_utm.get_coordinates(), precip_utm.inches)\n\nKNeighborsRegressor(weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor(weights='distance')\n\n\n\ncounty_h3['nn5id_est'] = knn_regressor.predict(hcents.get_coordinates())\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\ncounty_h3.plot(column='nn5_est',ax=ax1)\ncounty_h3.plot(column='nn5id_est',ax=ax2)\nax1.set_title(\"knn=5 (Unweighted)\")\nax2.set_title(\"knn=5 (Inverse Distance Weights)\");"
  },
  {
    "objectID": "lectures/week-13/2023-11-13-interpolation.html#next",
    "href": "lectures/week-13/2023-11-13-interpolation.html#next",
    "title": "Spatial Interpolation",
    "section": "Next",
    "text": "Next\n\ntracts = gdf.dissolve(by='TRACTCE20')\n\ntracts.shape\n\nNameError: name 'gdf' is not defined\n\n\n\nSurface to Area Interpolation\n\nSpatial Join on Centroid\n\ncents = tracts.centroid\n\n\ncents.plot()\n\n\ntype(cents)\n\n\ncoord_list = [(x, y) for x, y in zip(cents.x, cents.y)]\ntracts['centest'] = [x[0] for x in clipped.sample(coord_list)]\ntracts.head()\n\n\ntracts['centroid'] = tracts.centroid\ntracts.set_geometry('centroid', inplace=True)\n\n\ntracts.plot(column='centest', legend=True);\n\n\ntracts.set_geometry('geometry', inplace=True)\ntracts.plot(column='centest', legend=True);\n\n\n\nZonal Methods of Surface to Area Interpolation\n\nimport rasterstats\n\n\ngdf.head()\n\n\ntracts.plot()\n\n\nfrom rasterstats import zonal_stats\ntstats = zonal_stats(tracts, \"clipped_example.tif\",\n            stats=\"count min mean max median\")\n\n#elevations2 = zonal_stats(\n#    sd_tracts.to_crs(dem.rio.crs),  # Geotable with zones\n#    \"../data/nasadem/nasadem_sd.tif\",  # Path to surface file\n#)\n#elevations2 = pandas.DataFrame(elevations2)\n\n\ntstats[:5]\n\n\ntstats = pd.DataFrame(tstats)\n\n\ntstats.head()\n\n\ntstats.shape\n\n\ntracts.shape\n\n\ntracts['mean'] = tstats['mean'].values\ntracts.plot(column='mean', legend=True);\n\n\ntracts['median'] = tstats['median'].values\ntracts.plot(column='median', legend=True);\n\n\ntracts['range'] = tstats['max'].values - tstats['min'].values\ntracts.plot(column='range', legend=True);\n\n\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(data=tracts, x='centest', y='mean')\nplt.plot([10, 40], [10, 40]);\n\n\nsns.scatterplot(data=tracts, x='median', y='mean')\nplt.plot([10, 40], [10, 40]);"
  },
  {
    "objectID": "lectures/week-13/week-12/2023-11-06-distance-based.html",
    "href": "lectures/week-13/week-12/2023-11-06-distance-based.html",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "",
    "text": "from scipy import spatial\nimport libpysal as ps\nimport numpy as np\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\npoints = np.array([[66.22, 32.54], [22.52, 22.39], [31.01, 81.21],\n                   [9.47, 31.02],  [30.78, 60.10], [75.21, 58.93],\n                   [79.26,  7.68], [8.23, 39.93],  [98.73, 77.17],\n                   [89.78, 42.53], [65.19, 92.08], [54.46, 8.48]])\nimport pointpats\npointpats.__version__\n\n'2.3.0'\nimport sklearn\nsklearn.__version__\n\n'1.3.2'"
  },
  {
    "objectID": "lectures/week-13/week-12/2023-11-06-distance-based.html#nearest-neighbor-distance-functions",
    "href": "lectures/week-13/week-12/2023-11-06-distance-based.html#nearest-neighbor-distance-functions",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "Nearest Neighbor Distance Functions",
    "text": "Nearest Neighbor Distance Functions\nNearest neighbour distance distribution functions (including the nearest “event-to-event” and “point-event” distance distribution functions) of a point process are cumulative distribution functions of several kinds – \\(G, F, J\\). By comparing the distance function of the observed point pattern with that of the point pattern from a CSR process, we are able to infer whether the underlying spatial process of the observed point pattern is CSR or not for a given confidence level.\n\n\\(G\\) function - event-to-event\nThe \\(G\\) function is a kind of “cumulative” density describing the distribution of distances within a point pattern. For a given distance \\(d\\), \\(G(d)\\) is the proportion of nearest neighbor distances that are less than \\(d\\). To express this, we first need to define the nearest neighbor distance, which is the smallest distance from each observation \\(i\\) to some other observation \\(j\\), where \\(j \\neq i\\): \\[ min_{j\\neq i}\\{d_{ij}\\} = d^*_i \\]\nWith this, we can define the \\(G\\) function as a cumulative density function: \\[G(d) = \\frac{1}{N}\\sum_{i=1}^N \\mathcal{I}(d^*_i &lt; d)\\] where \\(\\mathcal{I}(.)\\) is an indicator function that is \\(1\\) when the argument is true and is zero otherwise. In simple terms, \\(G(d)\\) gives the percentage of of nearest neighbor distances (\\(d^*_i\\)) that are smaller than \\(d\\); when \\(d\\) is very small, \\(G(d)\\) is close to zero. When \\(d\\) is large, \\(G(d)\\) approaches one.\nAnalytical results about \\(G\\) are available assuming that the “null” process of locating points in the study area is completely spatially random. In a completely spatially random process, the \\(G(d)\\) value should be: \\[\nG(d) = 1-e^{-\\lambda \\pi d^2}\n\\] Practically, we assess statistical significance for the \\(G(d)\\) function using simulations, where a known spatially-random process is generated and then analyzed. This partially accounts for issues with irregularly-shaped study areas, where locations of points are constrained.\nIn practice, we use the ripley.g_test function to conduct a test on the \\(G(d)\\). It estimates a value of \\(G(d)\\) for a set of values (called the support). To compute the \\(G\\) function for ten values of \\(d\\) ranging from the smallest possible to the largest values in the data:\n\nfrom pointpats import g_test\n\n\ng_res = g_test(points, support=10)\n\n\ng_res.support\n\narray([ 0.        ,  3.84791574,  7.69583148, 11.54374723, 15.39166297,\n       19.23957871, 23.08749445, 26.93541019, 30.78332593, 34.63124168])\n\n\n\ng_res.statistic\n\narray([0.        , 0.        , 0.        , 0.16666667, 0.16666667,\n       0.25      , 0.58333333, 0.83333333, 0.91666667, 1.        ])\n\n\n\ng_res.pvalue\n\narray([0.00e+00, 0.00e+00, 0.00e+00, 2.92e-02, 1.10e-03, 1.00e-04,\n       4.50e-03, 6.28e-02, 7.30e-02, 0.00e+00])\n\n\n\nKeeping simulations\nTo make a plot of the statistic, the statistic is generally plotted on the vertical axis and the support on the horizontal axis. Here, we will show the median simulated value of \\(G(d)\\) as well.\n\ng_res = g_test(points, support=10, keep_simulations=True)\n\n\nplt.plot(g_res.support, np.median(g_res.simulations, axis=0), \n         color='k', label='simulated')\nplt.plot(g_res.support, g_res.statistic, \n         marker='x', color='orangered', label='observed')\nplt.legend()\nplt.xlabel('Distance')\nplt.ylabel('G Function')\nplt.title('G Function Plot')\nplt.show()\n\n\n\n\n\n# grab the middle 95% of simulations using numpy:\nmiddle_95pct = np.percentile(g_res.simulations, q=(2.5, 97.5), axis=0)\n# use the fill_between function to color between the 2.5% and 97.5% envelope\nplt.fill_between(g_res.support, *middle_95pct, \n                 color='lightgrey', label='simulated')\n\n# plot the line for the observed value of G(d)\nplt.plot(g_res.support, g_res.statistic, \n         color='orangered', label='observed')\n# and plot the support points depending on whether their p-value is smaller than .05\nplt.scatter(g_res.support, g_res.statistic, \n            cmap='viridis', c=g_res.pvalue &lt; .01)\nplt.legend()\nplt.xlabel('Distance')\nplt.ylabel('G Function')\nplt.title('G Function Plot')\nplt.show()\n\n\n\n\n\n\n\n\\(F\\) function - “point-event”\nWhen the number of events in a point pattern is small, \\(G\\) function is rough. For the pattern contained in points, there are only 12 observations! This means that there are only 12 nearest neighbor distances, and thus only 12 possible values for the \\(G(d)\\) statistic, at any \\(d\\).\nOne way to get around this is to turn to an alternative, the \\(F(d)\\) function. This is analogous to the \\(G(d)\\) function, but measures the nearest neighbor distance from a set of known randomly-distributed points to a point in the observed pattern. Another way of thinking about \\(F(d)\\) is that it reflects a between-pattern measure of dispersion, where one pattern is completely spatially random and the other pattern is our observed pattern. In contrast, \\(G(d)\\) is a within-pattern measure of dispersion.\nFor a randomly simulated point pattern of size \\(N_s\\), this makes the \\(F(d)\\) function:\n\\[F(d) = \\frac{1}{N_s} \\sum_k^{N_s} \\mathcal{I}(d^*_k &lt; d)\\]\nThis can have \\(N_s\\) possible values for any \\(d\\), and thus can give a much more fine-grained view of the point pattern. In this sense, the \\(F(d)\\) function is often called the empty space function, as it measures the distance from random points in “empty space” to the “filled” points in our point pattern. The number of those random points governs how “fine-grained” our measure of the observed point pattern can be.\nJust like the ripley.g_test, this function is evaluated for every \\(d\\) in a support. Further, we can provide custom values for support, just in case we have known distance values of interest.\nBelow, we’ll use the same ten support values from \\(G(d)\\) function. And, let’s constrain the “simulated” point patterns to fall within the convex hull of our original point pattern:\n\nfrom pointpats import f_test as f\n\n\nf_test = f(points, support = g_res.support, keep_simulations=True, hull='convex', n_simulations=999)\n\n\nplt.plot(f_test.support, f_test.simulations.T, alpha=.01, color='k')\nplt.plot(f_test.support, f_test.statistic, color='red')\n\nplt.scatter(f_test.support, f_test.statistic, \n            cmap='viridis', c=f_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('F Function')\nplt.title('F Function Plot')\nplt.show()\n\nplt.show()\n\n\n\n\nFrom this we see that the values of the \\(F\\) function are too high for distances from about 15 to 25, and (in contrast) for values between \\(5 &lt; d &lt; 10\\), the \\(F(d)\\) function has too few short distances. When the observed \\(F(d)\\) values are too large, then the pattern is too dispersed, or regular. If the empirical \\(F(d)\\) tends to fall below the simulated values, then it reflects clustering. This is the opposite of the interpretation of the \\(G(d)\\) function above, so be careful!\n\n\n\\(J\\) function - a combination of “event-event” and “point-event”\nThe \\(J\\) function combines the \\(G\\) and \\(F\\) function, in an attempt to provide an immediate graphical indication of the clustering both internally and with respect to the empty space distribution. Practically, the \\(J(d)\\) function is computed as a kind of “relative clustering ratio”:\n\\[J(d) = \\frac{1-G(d)}{1-F(d)}\\]\nwhere the numerator captures the clustering due to within-pattern distances and the denominator captures that for the pattern-to-empty distances. This means that when \\(J(d)&lt;1\\), the underlying point process is a cluster point process, and when \\(J(d)=1\\), the underlying point process is a random point process; otherwise, it is a dispersed point process.\nThis function can suffer from numerical stability issues; as \\(G(d)\\) and \\(F(d)\\) both approach \\(1\\), the \\(J\\) ratio can become chaotic. Further, when \\(G\\) or \\(F\\) reaches one, the \\(J\\) function changes abruptly. As such, the \\(J\\) function is often truncated to the first \\(1\\) (either in \\(F(d)\\) or \\(G(d)\\)), and any \\(d\\) where both \\(F\\) and \\(G\\) are \\(1\\) is assigned a \\(J\\) value of \\(1\\).\n\nfrom pointpats import j_test as j\n\n\njp1 = j(points, support=20)\n\n/tmp/ipykernel_26245/134666654.py:1: UserWarning: requested 20 bins to evaluate the J function, but it reaches infinity at d=34.6312, meaning only 20 bins will be used to characterize the J function.\n  jp1 = j(points, support=20)\n\n\nAs you can see from the warning above, the \\(J\\) function did encounter numerical stability issues at about \\(d=25\\). To address this, pointpats truncated the \\(J\\) function to only have 14 values in its support, rather than the \\(20\\) requested.\n\nplt.plot(jp1.support, jp1.statistic, color='orangered')\nplt.axhline(1, linestyle=':', color='k')\nplt.xlabel('Distance')\nplt.ylabel('J Function')\n\nText(0, 0.5, 'J Function')\n\n\n\n\n\nFrom the above figure, we see that the \\(J\\) function is above the \\(J(d)=1\\) horizontal line, especially as \\(d\\) gets large. This suggests that the process is over-dispersed."
  },
  {
    "objectID": "lectures/week-13/week-12/2023-11-06-distance-based.html#interevent-distance-functions",
    "href": "lectures/week-13/week-12/2023-11-06-distance-based.html#interevent-distance-functions",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "Interevent Distance Functions",
    "text": "Interevent Distance Functions\nWhile both the \\(F(d)\\) and \\(G(d)\\) functions are useful, they only consider the distance between each point \\(i\\) and its nearest point. Earlier we spelled this distance \\(d_i^*\\), and the distance between \\(i\\) and \\(j\\) was \\(d_{ij}\\). So, note that \\(d_{i}^*\\) is the only term that matters for \\(F\\) and \\(G\\), if \\(d_{ij}\\) changes (but \\(j\\) isn’t closest to \\(i\\)), then the \\(F\\) and \\(G\\) functions generally remain the same.\nSo, further statistical summary functions have been developed to consider the whole distance distribution, not only the nearest neighbor distances. These functions (still considered part of the “Ripley” alphabet, are the \\(K\\), and \\(L\\) functions.\n\n\\(K\\) function\nThe \\(K(d)\\) function is a scaled version of the cumulative density function for all distances within a point pattern. As such, it’s a “relative” of the \\(G\\) function that considers all distances, not just the nearest neighbor distances. Practically, the \\(K(d)\\) function can be thought of as the percentage of all distances that are less than \\(d\\). Therefore, for a threshold distance \\(d\\), the \\(K\\) function is defined as:\n\\[K(d) = \\frac{1}{N\\hat\\lambda} \\underset{i=1}{\\overset{N}{\\sum}}\\underset{j=1}{\\overset{N}{\\sum}} \\mathcal{I}\\left(d_ij &lt; d\\right)\\]\nIn this equation, \\(\\hat\\lambda\\) is the intensity of the point process. This represents how many points (on average) you would expect in a unit area. You can think of this as an analogue to the density of the points in the pattern: large values of \\(\\hat\\lambda\\) mean many points per area, and small values of \\(\\hat\\lambda\\) mean there are fewer points per area. Generally, this parameter is unknown, and is modelled using the average number of points in the study area. This assumes that the intensity of the point pattern is constant or homogeneous over the study area.\nIn the same manner as before, we can construct a set of \\(K(d)\\) function evaluations for random point patterns, and compare them to the observed \\(K(d)\\) function we saw in our original data.\n\nfrom pointpats import k_test as k\n\n\nk_test = k(points, keep_simulations=True, n_simulations=99)\n\n\nplt.plot(k_test.support, k_test.simulations.T, color='k', alpha=.01)\nplt.plot(k_test.support, k_test.statistic, color='orangered')\n\nplt.scatter(k_test.support, k_test.statistic, \n            cmap='viridis', c=k_test.pvalue &lt; .05,\n            zorder=4 # make sure they plot on top\n           )\n\nplt.xlabel('Distance')\nplt.ylabel('K Function')\nplt.title('K Function Plot')\nplt.show()\n\n\n\n\nAgain, we can see that the envelopes are generally above the observed function, meaining that our point pattern is dispersed. We can draw this conclusion because the distances are too small, suggesting the pattern is less clustered than otherwise woudl be expected. When points are too regular, their distances tend to be smaller than if they were distributed randomly."
  },
  {
    "objectID": "lectures/week-13/week-12/2023-11-06-distance-based.html#csr-example",
    "href": "lectures/week-13/week-12/2023-11-06-distance-based.html#csr-example",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "CSR Example",
    "text": "CSR Example\nIn this example, we are going to generate a point pattern as the “observed” point pattern. This ensures that the data generating process is completely spatially random. Then, we will simulate CSR in the same domain for 100 times and construct evaluate the ripley functions for these simulations.\n\nimport geopandas\nimport libpysal as ps\ndf = geopandas.read_file(ps.examples.get_path(\"vautm17n.shp\"))\nstate = df.geometry.cascaded_union\n\n/tmp/ipykernel_26245/3264935310.py:4: FutureWarning: The 'cascaded_union' attribute is deprecated, use 'unary_union' instead\n  state = df.geometry.cascaded_union\n\n\n\nfrom pointpats.random import poisson\n\n\ncsr = poisson(state, size=100)\n\n\ncsr[0:5]\n\narray([[ 392528.04579551, 4077062.60765435],\n       [ 711782.74903107, 4245360.48305302],\n       [ 593647.22771944, 4102442.85622552],\n       [ 616166.00712074, 4230904.07751182],\n       [ 731984.57021936, 4201204.81536705]])\n\n\n\ndf.plot()\nplt.scatter(*csr.T, color='orangered', marker=\".\")\nplt.show()\n\n\n\n\n\nrealizations = poisson(state, size=(100,100))\n\n\ndf.plot(facecolor='none', edgecolor='k')\nplt.scatter(*realizations.T, marker='.', s=2)\nplt.scatter(*csr.T, color='orangered', marker='.')\nplt.show()\n\n\n\n\nLet’s now compute the G function for the observed pattern as well as all the realizations we just made.\n\nfrom pointpats import g\n\n\nobserved_g = g(csr)\n\ncomparison_g = [ g(realization, support=observed_g[0])\n                for realization in realizations]\n\nplt.plot(*observed_g, color='orangered')\n[plt.plot(*comparison, color='k', alpha=.01) \n for comparison in comparison_g]\nplt.show()"
  },
  {
    "objectID": "lectures/week-13/week-12/2023-11-06-distance-based.html#clustered-process",
    "href": "lectures/week-13/week-12/2023-11-06-distance-based.html#clustered-process",
    "title": "Distance Based Statistics for Point Patterns",
    "section": "Clustered Process",
    "text": "Clustered Process\n\nfrom pointpats.random import cluster_poisson\nimport numpy\n\n\nnumpy.random.seed(1234567)\nclust = cluster_poisson(state, size=100, n_seeds=3,\n                       cluster_radius=100000)\n\n\ndf.plot()\nplt.scatter(*clust.T, color='orangered', marker=\".\")\nplt.show()\n\n\n\n\n\nobserved_g_csr = g(csr)\n\n\nobserved_g_clust = g(clust)\n\ncomparison_g = [ g(realization, support=observed_g[0])\n                for realization in realizations]\n\nplt.plot(*observed_g_csr, color='orangered', label='CSR')\nplt.plot(*observed_g_clust, color='blue', label='Clustered')\n[plt.plot(*comparison, color='k', alpha=.01) \n for comparison in comparison_g]\nplt.ylabel(\"G(d)\")\nplt.xlabel(\"d\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "lectures/week-13/week-12/2023-11-08-geostat-data.html",
    "href": "lectures/week-13/week-12/2023-11-08-geostat-data.html",
    "title": "Sampling the raster for “observations”",
    "section": "",
    "text": "import rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nras = 'data/stanford-td754wr4701-geotiff.tiff' # already interpolated but we will treat it as \"data\"\nshp = 'data/tl_2022_06073_faces.shp'\ngdf = gpd.read_file(shp)\ngdf.shape\ngdf.head()\ncounty = gdf.dissolve(by='COUNTYFP20')\ncounty.plot()\nrast = rasterio.open(ras)\ncounty = county.to_crs(rast.crs)\ncounty.plot()\nrast.crs\ncoords = gdf.geometry\nsrc = rast\ndf = county\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nclipped_array, clipped_transform = mask(dataset=src, shapes=coords, crop=True)\n\ndf = df.to_crs(src.crs)\nout_meta = src.meta.copy()\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": clipped_array.shape[1],\n                 \"width\": clipped_array.shape[2],\n                 \"transform\": clipped_transform})\nout_tif= \"clipped_example.tif\"\nwith rasterio.open(out_tif, \"w\", **out_meta) as dest:\n    dest.write(clipped_array)\n    \nclipped = rasterio.open(out_tif)\nfig, ax = plt.subplots(figsize=(8, 6))\np1 = df.plot(color=None,facecolor='none',edgecolor='red',linewidth = 2,ax=ax)\nshow(clipped, ax=ax)\nax.axis('off');\nclipped\nimport rioxarray\nd = rioxarray.open_rasterio(\"clipped_example.tif\")\nd\nd.plot()\nd.values.max()\nd.plot()\ntype(d)\nd.dims\nd.values.mean()\nimport numpy\nnumpy.median(d.values)\nd.values.shape\nd.plot()\nd.plot.hist()\ntype(d)\nimport numpy\nnumpy.random.seed(12345)\nsample_points = county.sample_points(50)\nm = county.explore()\nsample_points.explore(m=m, color='red')"
  },
  {
    "objectID": "lectures/week-13/week-12/2023-11-08-geostat-data.html#interpolation-methods",
    "href": "lectures/week-13/week-12/2023-11-08-geostat-data.html#interpolation-methods",
    "title": "Sampling the raster for “observations”",
    "section": "Interpolation Methods",
    "text": "Interpolation Methods\n\ntracts = gdf.dissolve(by='TRACTCE20')\n\ntracts.shape\n\n\nSurface to Area Interpolation\n\nSpatial Join on Centroid\n\ncents = tracts.centroid\n\n\ncents.plot()\n\n\ntype(cents)\n\n\ncoord_list = [(x, y) for x, y in zip(cents.x, cents.y)]\ntracts['centest'] = [x[0] for x in clipped.sample(coord_list)]\ntracts.head()\n\n\ntracts['centroid'] = tracts.centroid\ntracts.set_geometry('centroid', inplace=True)\n\n\ntracts.plot(column='centest', legend=True);\n\n\ntracts.set_geometry('geometry', inplace=True)\ntracts.plot(column='centest', legend=True);\n\n\n\nZonal Methods of Surface to Area Interpolation\n\nimport rasterstats\n\n\ngdf.head()\n\n\ntracts.plot()\n\n\nfrom rasterstats import zonal_stats\ntstats = zonal_stats(tracts, \"clipped_example.tif\",\n            stats=\"count min mean max median\")\n\n#elevations2 = zonal_stats(\n#    sd_tracts.to_crs(dem.rio.crs),  # Geotable with zones\n#    \"../data/nasadem/nasadem_sd.tif\",  # Path to surface file\n#)\n#elevations2 = pandas.DataFrame(elevations2)\n\n\ntstats[:5]\n\n\ntstats = pd.DataFrame(tstats)\n\n\ntstats.head()\n\n\ntstats.shape\n\n\ntracts.shape\n\n\ntracts['mean'] = tstats['mean'].values\ntracts.plot(column='mean', legend=True);\n\n\ntracts['median'] = tstats['median'].values\ntracts.plot(column='median', legend=True);\n\n\ntracts['range'] = tstats['max'].values - tstats['min'].values\ntracts.plot(column='range', legend=True);\n\n\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(data=tracts, x='centest', y='mean')\nplt.plot([10, 40], [10, 40]);\n\n\nsns.scatterplot(data=tracts, x='median', y='mean')\nplt.plot([10, 40], [10, 40]);"
  },
  {
    "objectID": "lectures/week-13/week-12/ex4.html",
    "href": "lectures/week-13/week-12/ex4.html",
    "title": "Geog385F23",
    "section": "",
    "text": "import geopandas as gpd\n\n\n!pwd\n\n/Users/srey/para/1_projects/course_385_spatial_data_analysis_f23/385f23/385f23/lectures/week-12\n\n\n\nroads = gpd.read_file('data/roa"
  },
  {
    "objectID": "lectures/week-13/week-12/geostat_data.html",
    "href": "lectures/week-13/week-12/geostat_data.html",
    "title": "Sampling the raster for “observations”",
    "section": "",
    "text": "import rasterio\n\n\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\n\n\nras = 'data/stanford-td754wr4701-geotiff.tiff' # already interpolated but we will treat it as \"data\"\n\n\nshp = 'data/tl_2022_06073_faces.shp'\n\n\ngdf = gpd.read_file(shp)\n\n\ngdf.shape\n\n(57076, 46)\n\n\n\ngdf.head()\n\n\n\n\n\n\n\n\nTFID\nSTATEFP20\nCOUNTYFP20\nTRACTCE20\nBLKGRPCE20\nBLOCKCE20\nSUFFIX1CE\nZCTA5CE20\nUACE20\nPUMACE20\n...\nMETDIVFP\nCNECTAFP\nNECTAFP\nNCTADVFP\nLWFLAG\nOFFSET\nATOTAL\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n216102682\n06\n073\n016302\n2\n2010\nNaN\n92021\n78661\n07313\n...\nNaN\nNaN\nNaN\nNaN\nL\nN\n22408\n+32.8008387\n-116.9453919\nPOLYGON ((-116.94621 32.80044, -116.94621 32.8...\n\n\n1\n216102683\n06\n073\n016302\n2\n2006\nNaN\n92021\n78661\n07313\n...\nNaN\nNaN\nNaN\nNaN\nL\nN\n29220\n+32.8008392\n-116.9472819\nPOLYGON ((-116.94836 32.80045, -116.94836 32.8...\n\n\n2\n216102684\n06\n073\n016302\n2\n2007\nNaN\n92021\n78661\n07313\n...\nNaN\nNaN\nNaN\nNaN\nL\nN\n18605\n+32.8012731\n-116.9487743\nPOLYGON ((-116.94919 32.80063, -116.94919 32.8...\n\n\n3\n226900092\n06\n073\n016301\n2\n2003\nNaN\n92020\n78661\n07313\n...\nNaN\nNaN\nNaN\nNaN\nL\nN\n19567\n+32.7999949\n-116.9608656\nPOLYGON ((-116.96140 32.80088, -116.96073 32.8...\n\n\n4\n263464856\n06\n073\n016301\n2\n2001\nNaN\n92020\n78661\n07313\n...\nNaN\nNaN\nNaN\nNaN\nL\nN\n23766\n+32.8019391\n-116.9605314\nPOLYGON ((-116.96149 32.80194, -116.96141 32.8...\n\n\n\n\n5 rows × 46 columns\n\n\n\n\ncounty = gdf.dissolve(by='COUNTYFP20')\n\n\ncounty.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nrast = rasterio.open(ras)\n\n\ncounty = county.to_crs(rast.crs)\n\n\ncounty.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nrast.crs\n\nCRS.from_epsg(4326)\n\n\n\ncoords = gdf.geometry\nsrc = rast\ndf = county\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n\nclipped_array, clipped_transform = mask(dataset=src, shapes=coords, crop=True)\n\ndf = df.to_crs(src.crs)\nout_meta = src.meta.copy()\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": clipped_array.shape[1],\n                 \"width\": clipped_array.shape[2],\n                 \"transform\": clipped_transform})\nout_tif= \"clipped_example.tif\"\nwith rasterio.open(out_tif, \"w\", **out_meta) as dest:\n    dest.write(clipped_array)\n    \nclipped = rasterio.open(out_tif)\nfig, ax = plt.subplots(figsize=(8, 6))\np1 = df.plot(color=None,facecolor='none',edgecolor='red',linewidth = 2,ax=ax)\nshow(clipped, ax=ax)\nax.axis('off');\n\n\n\n\n\nclipped\n\n&lt;open DatasetReader name='clipped_example.tif' mode='r'&gt;\n\n\n\nimport rioxarray\n\n\nd = rioxarray.open_rasterio(\"clipped_example.tif\")\n\n\nd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 336, x: 527)&gt;\n[177072 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -117.6 -117.6 -117.6 ... -116.1 -116.1 -116.1\n  * y            (y) float64 33.5 33.5 33.5 33.5 ... 32.54 32.53 32.53 32.53\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 336x: 527...[177072 values with dtype=uint8]Coordinates: (4)band(band)int641array([1])x(x)float64-117.6 -117.6 ... -116.1 -116.1array([-117.612517, -117.609603, -117.606689, ..., -116.085683, -116.082769,\n       -116.079855])y(y)float6433.5 33.5 33.5 ... 32.53 32.53array([33.504668, 33.501754, 33.498839, ..., 32.534232, 32.531318, 32.528403])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-117.61397387622179 0.002913805220117985 0.0 33.50612487372884 0.0 -0.0029142217548938525array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([-117.61251697361173, -117.60960316839162,  -117.6066893631715,\n       -117.60377555795138, -117.60086175273126, -117.59794794751114,\n       -117.59503414229103, -117.59212033707091, -117.58920653185079,\n       -117.58629272663067,\n       ...\n       -116.10607967481073, -116.10316586959061,  -116.1002520643705,\n       -116.09733825915038, -116.09442445393026, -116.09151064871014,\n       -116.08859684349002,  -116.0856830382699, -116.08276923304979,\n       -116.07985542782967],\n      dtype='float64', name='x', length=527))yPandasIndexPandasIndex(Index([ 33.50466776285139,   33.5017535410965, 33.498839319341606,\n       33.495925097586706, 33.493010875831814,  33.49009665407692,\n        33.48718243232203,  33.48426821056713,  33.48135398881224,\n       33.478439767057345,\n       ...\n        32.55463147075599,   32.5517172490011,  32.54880302724621,\n       32.545888805491316,  32.54297458373642, 32.540060361981524,\n        32.53714614022663,  32.53423191847174,  32.53131769671684,\n        32.52840347496195],\n      dtype='float64', name='y', length=336))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\n\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x2cd7e3af0&gt;\n\n\n\n\n\n\nd.values.max()\n\n49\n\n\n\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x301b7d630&gt;\n\n\n\n\n\n\ntype(d)\n\nxarray.core.dataarray.DataArray\n\n\n\nd.dims\n\n('band', 'y', 'x')\n\n\n\nd.values.mean()\n\n15.630963675792898\n\n\n\nimport numpy\n\n\nnumpy.median(d.values)\n\n17.0\n\n\n\nd.values.shape\n\n(1, 336, 527)\n\n\n\nd.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x301bff700&gt;\n\n\n\n\n\n\nd.plot.hist()\n\n(array([44849., 17108., 10090., 29702., 31657., 23051., 11786.,  6411.,\n         2011.,   407.]),\n array([ 0. ,  4.9,  9.8, 14.7, 19.6, 24.5, 29.4, 34.3, 39.2, 44.1, 49. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\ntype(d)\n\n\nxarray.core.dataarray.DataArray\n\n\n\nimport numpy\nnumpy.random.seed(12345)\nsample_points = county.sample_points(50)\n\n\nm = county.explore()\nsample_points.explore(m=m, color='red')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nEnsure sample points are separated by some threshold\n\ncounty.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- undefined\nDatum: World Geodetic System 1984\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\norig_crs = county.crs\n\n\ncounty_utm = county.to_crs(county.estimate_utm_crs())\n\n\ncounty_utm.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nthreshold = 10000 # no pair of stations within 10000 meters of each other\n\n\nn_points = 25 # number of stations desired\n\n\nnumpy.random.seed(12345)\nthinning = True\nsample_points = county_utm.sample_points(n_points * 4).explode(index_parts=True)\ncandidates = []\nt2 = threshold**2\niter = 0\nwhile thinning:\n    p0 = numpy.random.choice(sample_points,1)[0]\n    #p0 = sample_points[0]\n    d0 = (sample_points.x - p0.x)**2 + (sample_points.y - p0.y)**2\n    candidates.append(p0)\n    if len(candidates) == n_points:\n        thinning=False\n    else:\n        sample_points = sample_points[d0&gt;t2]\n    #print('iter: ', iter, 'shape sp: ', sampled_points.shape)\n    iter += 1\n\n\n\n\ncp = gpd.GeoSeries(candidates)\n\n\ncp.plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nm = county_utm.plot()\ncp.plot(ax=m, color='r');\n\n\n\n\n\ncp.crs = county_utm.crs\ncp = cp.to_crs(county.crs)\n\n\nm = county.explore()\ncp.explore(m=m, color='r')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ncoord_list = [(x, y) for x, y in zip(cp.x, cp.y)]\n\nobservations = [x[0] for x in clipped.sample(coord_list)]\nprecip_gdf = gpd.GeoDataFrame(data=observations, columns=['inches'], geometry=cp)\n\n\nprecip_gdf.plot(column='inches', legend=True);\n\n\n\n\n\nm = county.explore()\nprecip_gdf.explore(column='inches', m=m)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nprecip_gdf.to_file(\"precip_sd.geojson\", driver='GeoJSON')\ncounty.to_file(\"sdcounty.geojson\", driver='GeoJSON')"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#point-pattern-analysis-objectives",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#point-pattern-analysis-objectives",
    "title": "Point Pattern Basics",
    "section": "Point Pattern Analysis Objectives",
    "text": "Point Pattern Analysis Objectives\nGoals\n\nPattern detection\nAssessing the presence of clustering\nIdentification of individual clusters\n\nGeneral Approaches\n\nEstimate intensity of the process\nFormulating an idealized model and investigating deviations from expectations\nFormulating a stochastic model and fitting it to the data"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#point-pattern-analysis-definitions",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#point-pattern-analysis-definitions",
    "title": "Point Pattern Basics",
    "section": "Point Pattern Analysis Definitions",
    "text": "Point Pattern Analysis Definitions\nSpatial Point Pattern: A set of events, irregularly distributed within a region \\(A\\) and presumed to have been generated by some form of stochastic mechanism.\nRepresentation \\(\\left\\{Y(A),  A \\subset \\Re \\right\\}\\), where \\(Y(A)\\) is the number of events occurring in area \\(A\\).\nEvents, points, locations\n\nEvent\n\nan occurrence of interest\n\nPoint\n\nany location in study area\n\nEvent location\n\na particular point where an event occurs"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#point-pattern-analysis-definitions-1",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#point-pattern-analysis-definitions-1",
    "title": "Point Pattern Basics",
    "section": "Point Pattern Analysis Definitions",
    "text": "Point Pattern Analysis Definitions\nRegion: \\(A\\)\n\nMost often planar (two-dimensional Euclidean space)\nOne dimensional applications also possible\nThree-dimensional increasingly popular (space + time)\nPoint processes on networks (non-planar)"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#space-time-point-patterns",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#space-time-point-patterns",
    "title": "Point Pattern Basics",
    "section": "Space-Time Point Patterns",
    "text": "Space-Time Point Patterns"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#space-time-point-patterns-1",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#space-time-point-patterns-1",
    "title": "Point Pattern Basics",
    "section": "Space-Time Point Patterns",
    "text": "Space-Time Point Patterns"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#point-patterns-on-networks",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#point-patterns-on-networks",
    "title": "Point Pattern Basics",
    "section": "Point Patterns on Networks",
    "text": "Point Patterns on Networks"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#point-patterns",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#point-patterns",
    "title": "Point Pattern Basics",
    "section": "Point Patterns",
    "text": "Point Patterns\nUnmarked Point Patterns\n\nOnly location is recorded\nAttribute is binary (presence, absence)\n\nMarked Point Patterns\n\nLocation is recorded\nNon-binary stochastic attribute\ne.g., sales at a retail store, dbh of tree"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#realizations",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#realizations",
    "title": "Point Pattern Basics",
    "section": "Realizations",
    "text": "Realizations\nMapped Point Patterns\n\nAll events are recorded and mapped\nComplete enumeration of events\nFull information on the realization from the process\n\nSampled Point Patterns\n\nSample of events are recorded and mapped\nComplete enumeration of events impossible or intractable\nPartial information on the realization from the process\nPresence/“absence” data (ecology, forestry)"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#research-questions-1",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#research-questions-1",
    "title": "Point Pattern Basics",
    "section": "Research Questions",
    "text": "Research Questions\n\nLocation Only are points randomly located or patterned\nLocation and Value\n\nmarked point pattern\nis combination of location and value random or patterned\n\n\nBoth Cases: What is the Underlying Process?"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#points-on-a-plane-planar-point-pattern-anaysis",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#points-on-a-plane-planar-point-pattern-anaysis",
    "title": "Point Pattern Basics",
    "section": "Points on a Plane (Planar Point Pattern Anaysis)",
    "text": "Points on a Plane (Planar Point Pattern Anaysis)\nClassic Point Pattern Analysis\n\npoints on an isotropic plane\nno effect of translation and rotation\nclassic examples: tree seedlings, rocks, etc\n\nDistance\n\nno directional effects\nno translational effects\nstraight line distance only"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#events-point-map",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#events-point-map",
    "title": "Point Pattern Basics",
    "section": "Events: Point Map",
    "text": "Events: Point Map"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#points-in-context",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#points-in-context",
    "title": "Point Pattern Basics",
    "section": "Points in Context",
    "text": "Points in Context"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#intensity",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#intensity",
    "title": "Point Pattern Basics",
    "section": "Intensity",
    "text": "Intensity\nFirst Moment\n\nnumber of points \\(N\\), area of study \\(|A|\\)\nintensity: \\(\\lambda = N/|A|\\)\narea depends on bounds, often arbitrary\n\nArtificial Boundaries\n\nbounding box (rectangle, square)\nother (city boundary)"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#bounding-box",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#bounding-box",
    "title": "Point Pattern Basics",
    "section": "Bounding Box",
    "text": "Bounding Box"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#district-boundary",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#district-boundary",
    "title": "Point Pattern Basics",
    "section": "District Boundary",
    "text": "District Boundary"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#convex-hull",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#convex-hull",
    "title": "Point Pattern Basics",
    "section": "Convex Hull",
    "text": "Convex Hull\n\nTightest fit various algorithms\nRescaled Convex Hull (Ripley-Rasson)\n\nadjust to properly reflect spatial domain of point process\nuse centroid of convex hull\nrescale by \\(1/[\\sqrt{(1-m/N)}]\\)\n\\(m\\): number of vertices of convex hull"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#convex-hull-1",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#convex-hull-1",
    "title": "Point Pattern Basics",
    "section": "Convex Hull",
    "text": "Convex Hull"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#multiple-boundaries",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#multiple-boundaries",
    "title": "Point Pattern Basics",
    "section": "Multiple Boundaries",
    "text": "Multiple Boundaries"
  },
  {
    "objectID": "lectures/week-09/2023-10-18-point-patterns.html#intensity-estimates",
    "href": "lectures/week-09/2023-10-18-point-patterns.html#intensity-estimates",
    "title": "Point Pattern Basics",
    "section": "Intensity Estimates",
    "text": "Intensity Estimates\n\n\n\n\nArea\nIntensity\n\n\n\n\\(km^2\\)\n\\(cases/km^2\\)\n\n\nDistrict Boundary\n315.155\n3.29\n\n\nBounding Box\n310.951\n3.33\n\n\nConvex Hull\n229.421\n4.52\n\n\n\n\nN=1036"
  },
  {
    "objectID": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html",
    "href": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html",
    "title": "Nearest Neighbor Methods",
    "section": "",
    "text": "Now that we have been introduced to the different statistical models than are used to represent point processes, we turn to the methods that are used to link observed point patterns back to the process that generated the pattern.\nMore specifically, the challenge that we face is as follows. Given an observed point pattern, we wish to make inferences about the process that generated the observed pattern.\nThe general approach that is used is to construct measures that characterise the observed point pattern, and then compare these against the proporties of the theoretical process models we explored previously.\nFor example, if we assume that the underlying process is CSR, we know what kinds of properties the empirical patterns from such a process should exhibit. The critical thing to keep in mind is that we never actually see the underlying process - we only see outcomes of the process (i.e., the pattern).\nThis raises a number of challenges that we will need to address later on, but for now we are going to build up an inituition of the general strategy for analyzing point patterns."
  },
  {
    "objectID": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html#introduction",
    "href": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html#introduction",
    "title": "Nearest Neighbor Methods",
    "section": "",
    "text": "Now that we have been introduced to the different statistical models than are used to represent point processes, we turn to the methods that are used to link observed point patterns back to the process that generated the pattern.\nMore specifically, the challenge that we face is as follows. Given an observed point pattern, we wish to make inferences about the process that generated the observed pattern.\nThe general approach that is used is to construct measures that characterise the observed point pattern, and then compare these against the proporties of the theoretical process models we explored previously.\nFor example, if we assume that the underlying process is CSR, we know what kinds of properties the empirical patterns from such a process should exhibit. The critical thing to keep in mind is that we never actually see the underlying process - we only see outcomes of the process (i.e., the pattern).\nThis raises a number of challenges that we will need to address later on, but for now we are going to build up an inituition of the general strategy for analyzing point patterns."
  },
  {
    "objectID": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html#example-patterns",
    "href": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html#example-patterns",
    "title": "Nearest Neighbor Methods",
    "section": "Example Patterns",
    "text": "Example Patterns\nTo begin we are going to create two different point patterns, one from a CSR process and one from a clustered process. We will use these two patterns to introduce the different statistical methods used to analyze the patterns. Here we are in the rare circumstance in which we actually know what process generated the pattern.\n\nCSR n=60\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(12345)\nn = 60\nxy = np.random.rand(60,2)\ndf = pd.DataFrame(data=xy, columns=['x', 'y'])\nsns.scatterplot(x='x', y='y', data=df);\n\n\n\n\n\nimport pointpats as pp\n\n\ncsr = pp.PointPattern(xy)\n\n\ncsr.summary()\n\nPoint Pattern\n60 points\nBounding rectangle [(0.00838829794155349,0.024676210429265266), (0.9940145858999619,0.9613067360728214)]\nArea of window: 0.9231676681785911\nIntensity estimate for window: 64.99361066054225\n          x         y\n0  0.929616  0.316376\n1  0.183919  0.204560\n2  0.567725  0.595545\n3  0.964515  0.653177\n4  0.748907  0.653570\n\n\n\nw = pp.Window([(0,0), (0,1), (1,1), (1,0), (0,0)])\ndraw = pp.PoissonClusterPointProcess(w, n, 2, 0.05, 1, asPP=True, conditioning=False)\ndraw.realizations[0].plot(window=True, title='Contagion Point Process (2 parents)')\n\n\n\n\n\nclustered = draw.realizations[0]\n\n\nclustered.summary()\n\nPoint Pattern\n60 points\nBounding rectangle [(0.47331760265312733,0.023178703349462502), (0.9696584457277277,0.6150208352748628)]\nArea of window: 1.0\nIntensity estimate for window: 60.0\n          x         y\n0  0.513060  0.541971\n1  0.473318  0.578385\n2  0.508373  0.536200\n3  0.881716  0.060328\n4  0.894221  0.059273"
  },
  {
    "objectID": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html#quadrat-statistics",
    "href": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html#quadrat-statistics",
    "title": "Nearest Neighbor Methods",
    "section": "Quadrat Statistics",
    "text": "Quadrat Statistics\n\nimport pointpats.quadrat_statistics as qs\n\n\ncsr_qr = qs.QStatistic(csr, shape='rectangle', nx=3, ny=3)\ncsr_qr.plot()\n\n&lt;Axes: title={'center': 'Quadrat Count'}&gt;\n\n\n\n\n\n\ncsr_qr.chi2\n\n10.8\n\n\n\ncsr_qr.chi2_pvalue\n\n0.21329101843394052\n\n\n\nclustered_qr = qs.QStatistic(clustered, shape='rectangle', nx=3, ny=3)\nclustered_qr.plot()\n\n&lt;Axes: title={'center': 'Quadrat Count'}&gt;\n\n\n\n\n\n\nclustered_qr.chi2\n\n209.99999999999994\n\n\n\nclustered_qr.chi2_pvalue\n\n4.976940117448032e-41"
  },
  {
    "objectID": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html#nearest-neighbor-distances",
    "href": "lectures/week-11/2023-11-01-nearest-neighbor-Copy1.html#nearest-neighbor-distances",
    "title": "Nearest Neighbor Methods",
    "section": "Nearest Neighbor Distances",
    "text": "Nearest Neighbor Distances\n\nplt.scatter(csr.points.x, csr.points.y);\n\n\n\n\n\nimport networkx as nx\n\n\nG = nx.DiGraph()\nfor idx, point in enumerate(csr.points.values):\n    G.add_node(idx, pos=point)\n    \n\n\npos = nx.get_node_attributes(G, 'pos')\nnx.draw(G, pos, node_size=2)\n\n\n\n\n\nnidx, nnd = csr.knn(1) # here we have the indices of the nearest neighbors (nidx) and the distances (nnd)\n\n\nfor idx, neighbor in enumerate(nidx):\n    edge = (idx, neighbor[0])\n    G.add_edges_from([edge])\n    \n\n\npos = nx.get_node_attributes(G, 'pos')\nnx.draw(G, pos, node_size=2)\n\n\n\n\nHere we draw an arrow towards the nearest neighbor for a given observation.\nIn some cases, an observation is also the nearest neighbor to its nearest neighbor, or so-called “mutual nearest neighbors”. These pairs would appear at the the end of a segment with arrows on both ends.\nIf we look in the extreme northwest three points, we see one pair of mutual nearest neighbors, while the third point is not a mutual nearest neighbor.\nIt is also possible for a point to be a nearest neighbor to more than a single point, as is also seen in this case.\nThe nearest neighbor distances are the lengths of these segments.\n\nMean Nearest Neighbor Distance\nOur first distance based statistic was suggested by Clark and Evans (1954) as the average nearest neighbor distances:\n\\[\\bar{d}_{min} = \\frac{1}{n} \\sum_{i} d_{i, min} \\]\nwhere \\(d_{i, min}\\) is the nearest neighbor distance for observation \\(i\\), and \\(n\\) is the number of observations.\nUnder a CSR process, the expected value of this statistic is:\n\\[E[\\bar{d}_{min}] = \\frac{1}{2 \\sqrt{\\lambda}}\\]\nThe logic of the statistic is to compare the observed mean nearest neighbor distance to this expectation forming their ratio:\n\\[R = \\frac{\\bar{d}_{min}}{\\frac{1}{2 \\sqrt{\\lambda}}} = 2 \\bar{d}_{min} \\sqrt{\\lambda}\\]\nValues of \\(R&lt;1\\) are indicative of a tendancy towards clustering since the observed nearest neighbor distances are smaller than expected under CSR.\nValues of \\(R&gt;1\\) are indicative of a uniform or dispersed pattern.\n\nnnd.mean() # the mean nearest neighbor distance\n\n0.07360281110243255\n\n\n\ncsr.lambda_window # the intensity using the window for the point pattern\n\n64.99361066054225\n\n\n\ndmin = nnd.mean()\nlam = csr.lambda_window\nR = 2 * dmin * lam**(1/2)\nR\n\n1.1867513365512292\n\n\nLet’s compare this to the same statistic based on the mean nearest neighbor distance for the clustered pattern:\n\nnidx, nnd = clustered.knn(1) # here we have the indices of the nearest neighbors (nidx) and the distances (nnd)\n\n\ndmin = nnd.mean()\nlam = clustered.lambda_window\nR = 2 * dmin * lam**(1/2)\nR\n\n0.13087134840769868\n\n\nSo we see that the \\(R\\) value for the clustered pattern is much below 1, while the R value for the CSR pattern is slightly over 1.\nWhat we would like to know is if these values are significantly different from what we would expect if the underlying process that generated the patterns was CSR?\nOne approach is to use theoretical results on the distribution for the \\(R\\) statistic from Petrere (1985). The expected value of \\(R\\) is \\(E[R]=1\\). The variance of the \\(R\\) statistic is: \\[ \\sigma^2_R = \\frac{0.2732}{n}\\]\n\nimport scipy.stats\ndef R_test(pattern):\n    nidx, nnd = pattern.knn(1) # here we have the indices of the nearest neighbors (nidx) and the distances (nnd)\n    lam = pattern.lambda_window\n    R = 2 * nnd.mean() * lam**(1/2)\n    n = nnd.shape[0]\n    var = 0.2732 / n\n    se = var**(1/2)\n    stat = (R - 1 )/ se\n    p_value = scipy.stats.norm.sf(abs(stat)) * 2\n    return R, stat, p_value\n\n\nR_test(csr)\n\n(1.1867513365512292, 2.7675724348891184, 0.005647549379017388)\n\n\n\nR_test(clustered)\n\n(0.13087134840769868, -12.880103258909552, 5.825756575218341e-38)\n\n\n\n\nInference via simulation\n\nimport pointpats\nimport numpy\nsamples = pointpats.PoissonPointProcess(csr.window, n, 99, asPP=True)\n\nr_tests = np.array([R_test(samples.realizations[k]) for k in samples.realizations])\n\nr_tests\n\narray([[ 9.81105197e-01, -2.80012645e-01,  7.79467803e-01],\n       [ 9.28415089e-01, -1.06085681e+00,  2.88754981e-01],\n       [ 1.05435206e+00,  8.05473519e-01,  4.20546481e-01],\n       [ 1.11821360e+00,  1.75187335e+00,  7.97955886e-02],\n       [ 1.12387424e+00,  1.83576157e+00,  6.63929272e-02],\n       [ 1.16724422e+00,  2.47848561e+00,  1.31941433e-02],\n       [ 1.10700813e+00,  1.58581325e+00,  1.12781678e-01],\n       [ 1.09108797e+00,  1.34988348e+00,  1.77053361e-01],\n       [ 9.69002352e-01, -4.59371472e-01,  6.45967431e-01],\n       [ 1.03760497e+00,  5.57289124e-01,  5.77329905e-01],\n       [ 1.02704882e+00,  4.00851546e-01,  6.88529426e-01],\n       [ 1.10508460e+00,  1.55730742e+00,  1.19397514e-01],\n       [ 1.15793416e+00,  2.34051463e+00,  1.92571837e-02],\n       [ 1.04250723e+00,  6.29938355e-01,  5.28734918e-01],\n       [ 1.10758728e+00,  1.59439606e+00,  1.10847354e-01],\n       [ 1.07177650e+00,  1.06369612e+00,  2.87466383e-01],\n       [ 1.02551521e+00,  3.78124168e-01,  7.05338356e-01],\n       [ 9.46840660e-01, -7.87797970e-01,  4.30814888e-01],\n       [ 1.03757582e+00,  5.56857151e-01,  5.77625033e-01],\n       [ 9.90592969e-01, -1.39408044e-01,  8.89127716e-01],\n       [ 1.04856233e+00,  7.19672330e-01,  4.71726767e-01],\n       [ 1.10811705e+00,  1.60224699e+00,  1.09101003e-01],\n       [ 9.54688187e-01, -6.71501077e-01,  5.01901374e-01],\n       [ 1.10995667e+00,  1.62950930e+00,  1.03205247e-01],\n       [ 1.15162238e+00,  2.24697675e+00,  2.46415129e-02],\n       [ 9.31182120e-01, -1.01985062e+00,  3.07799310e-01],\n       [ 9.71814911e-01, -4.17690588e-01,  6.76173355e-01],\n       [ 1.14149435e+00,  2.09688392e+00,  3.60038522e-02],\n       [ 1.07165594e+00,  1.06190948e+00,  2.88276781e-01],\n       [ 1.14437709e+00,  2.13960485e+00,  3.23867145e-02],\n       [ 9.26536025e-01, -1.08870370e+00,  2.76284570e-01],\n       [ 1.00659687e+00,  9.77627322e-02,  9.22120701e-01],\n       [ 1.19240160e+00,  2.85130688e+00,  4.35399261e-03],\n       [ 1.02219381e+00,  3.28902433e-01,  7.42229435e-01],\n       [ 1.17405296e+00,  2.57938804e+00,  9.89755372e-03],\n       [ 1.15649472e+00,  2.31918276e+00,  2.03851289e-02],\n       [ 9.45617539e-01, -8.05924083e-01,  4.20286624e-01],\n       [ 1.05893614e+00,  8.73407565e-01,  3.82440969e-01],\n       [ 1.09368529e+00,  1.38837464e+00,  1.65022993e-01],\n       [ 1.07631454e+00,  1.13094776e+00,  2.58077078e-01],\n       [ 9.78530574e-01, -3.18167420e-01,  7.50357945e-01],\n       [ 1.07064731e+00,  1.04696203e+00,  2.95117090e-01],\n       [ 1.08860485e+00,  1.31308486e+00,  1.89154355e-01],\n       [ 1.19551229e+00,  2.89740597e+00,  3.76262510e-03],\n       [ 1.07442246e+00,  1.10290799e+00,  2.70067126e-01],\n       [ 1.09651586e+00,  1.43032247e+00,  1.52624489e-01],\n       [ 1.10573862e+00,  1.56699976e+00,  1.17114749e-01],\n       [ 9.18296882e-01, -1.21080418e+00,  2.25970464e-01],\n       [ 9.75959250e-01, -3.56273307e-01,  7.21635897e-01],\n       [ 1.12270070e+00,  1.81837021e+00,  6.90075678e-02],\n       [ 1.16124927e+00,  2.38964311e+00,  1.68647518e-02],\n       [ 1.14288622e+00,  2.11751071e+00,  3.42165274e-02],\n       [ 8.95782250e-01, -1.54446109e+00,  1.22476670e-01],\n       [ 1.05289923e+00,  7.83943286e-01,  4.33073389e-01],\n       [ 1.21757563e+00,  3.22437488e+00,  1.26248010e-03],\n       [ 1.03138599e+00,  4.65126525e-01,  6.41840852e-01],\n       [ 1.07305973e+00,  1.08271294e+00,  2.78935859e-01],\n       [ 1.11578890e+00,  1.71594046e+00,  8.61729410e-02],\n       [ 9.58111837e-01, -6.20764093e-01,  5.34754852e-01],\n       [ 9.37476396e-01, -9.26572231e-01,  3.54148678e-01],\n       [ 9.63173031e-01, -5.45759439e-01,  5.85231308e-01],\n       [ 1.03085430e+00,  4.57247129e-01,  6.47493427e-01],\n       [ 1.12588430e+00,  1.86554984e+00,  6.21043732e-02],\n       [ 1.04886527e+00,  7.24161732e-01,  4.68966450e-01],\n       [ 1.11417108e+00,  1.69196511e+00,  9.06526264e-02],\n       [ 9.09443001e-01, -1.34201478e+00,  1.79591204e-01],\n       [ 1.11892848e+00,  1.76246763e+00,  7.79903206e-02],\n       [ 1.14728332e+00,  2.18267389e+00,  2.90598342e-02],\n       [ 9.66649872e-01, -4.94234183e-01,  6.21140802e-01],\n       [ 1.01975820e+00,  2.92807770e-01,  7.69669089e-01],\n       [ 1.06807716e+00,  1.00887343e+00,  3.13035340e-01],\n       [ 8.87332323e-01, -1.66968527e+00,  9.49816494e-02],\n       [ 1.06116818e+00,  9.06485480e-01,  3.64678947e-01],\n       [ 9.73221131e-01, -3.96851022e-01,  6.91477323e-01],\n       [ 1.10831264e+00,  1.60514552e+00,  1.08461783e-01],\n       [ 1.15121609e+00,  2.24095569e+00,  2.50289451e-02],\n       [ 1.06442269e+00,  9.54715861e-01,  3.39721407e-01],\n       [ 1.09045697e+00,  1.34053244e+00,  1.80072304e-01],\n       [ 1.21321146e+00,  3.15969983e+00,  1.57931759e-03],\n       [ 1.03753327e+00,  5.56226553e-01,  5.78055989e-01],\n       [ 9.92381824e-01, -1.12898015e-01,  9.10111410e-01],\n       [ 9.89597771e-01, -1.54156441e-01,  8.77486386e-01],\n       [ 8.69471915e-01, -1.93436865e+00,  5.30678183e-02],\n       [ 1.07456759e+00,  1.10505883e+00,  2.69134096e-01],\n       [ 1.01501782e+00,  2.22557427e-01,  8.23879974e-01],\n       [ 1.06339883e+00,  9.39542766e-01,  3.47452146e-01],\n       [ 1.04251892e+00,  6.30111598e-01,  5.28621572e-01],\n       [ 1.01595310e+00,  2.36417893e-01,  8.13108413e-01],\n       [ 9.76030325e-01, -3.55220016e-01,  7.22424770e-01],\n       [ 1.02700974e+00,  4.00272457e-01,  6.88955852e-01],\n       [ 1.05167838e+00,  7.65850808e-01,  4.43765079e-01],\n       [ 1.03268084e+00,  4.84315702e-01,  6.28161834e-01],\n       [ 1.07237329e+00,  1.07254022e+00,  2.83477458e-01],\n       [ 1.17864464e+00,  2.64743472e+00,  8.11050176e-03],\n       [ 1.00495856e+00,  7.34836527e-02,  9.41421252e-01],\n       [ 9.57599581e-01, -6.28355512e-01,  5.29771074e-01],\n       [ 1.01309195e+00,  1.94016950e-01,  8.46162610e-01],\n       [ 1.06834012e+00,  1.01277043e+00,  3.11169828e-01],\n       [ 1.04123161e+00,  6.11034274e-01,  5.41176890e-01]])\n\n\n\nR_csr = R_test(csr)\n\n\nR_csr[0]\n\n1.1867513365512292\n\n\n\n(r_tests[:,0] &gt;= R_csr[0]).sum()\n\n4\n\n\n\nR_clustered = R_test(clustered)\n\n\nimport pandas\n\nimport seaborn as sns\n\n\ndf = pandas.DataFrame(data=r_tests, columns=['R', 'z', 'p'])\n\nsns.displot(df, kind='kde', x=\"R\")\nplt.axvline(R_csr[0], 0, 0.1, color='g');\nplt.axvline(R_clustered[0], 0, 0.1, color='r');\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nR\nz\np\n\n\n\n\ncount\n99.000000\n99.000000\n99.000000\n\n\nmean\n1.052286\n0.774857\n0.357230\n\n\nstd\n0.078631\n1.165280\n0.282182\n\n\nmin\n0.869472\n-1.934369\n0.001262\n\n\n25%\n0.991487\n-0.126153\n0.099093\n\n\n50%\n1.054352\n0.805474\n0.295117\n\n\n75%\n1.108215\n1.603696\n0.581644\n\n\nmax\n1.217576\n3.224375\n0.941421\n\n\n\n\n\n\n\n\n\nsamples = pointpats.PoissonPointProcess(csr.window, n, 999, asPP=True)\n\nr_tests = np.array([R_test(samples.realizations[k]) for k in samples.realizations])\n\nr_tests\n\narray([[ 1.04108274,  0.60882811,  0.54263838],\n       [ 1.08726785,  1.2932711 ,  0.19591731],\n       [ 0.96240692, -0.5571128 ,  0.57745036],\n       ...,\n       [ 1.18467435,  2.73679237,  0.00620414],\n       [ 1.05998867,  0.88900566,  0.37400004],\n       [ 0.92024492, -1.18193507,  0.23723146]])\n\n\n\ndf = pandas.DataFrame(data=r_tests, columns=['R', 'z', 'p'])\n\nsns.displot(df, kind='kde', x=\"R\")\nplt.axvline(R_csr[0], 0, 0.1, color='g');\nplt.axvline(R_clustered[0], 0, 0.1, color='r');\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nR\nz\np\n\n\n\n\ncount\n999.000000\n999.000000\n999.000000\n\n\nmean\n1.057947\n0.858756\n0.357294\n\n\nstd\n0.077832\n1.153438\n0.296165\n\n\nmin\n0.805327\n-2.884969\n0.000001\n\n\n25%\n1.004814\n0.071334\n0.092286\n\n\n50%\n1.061867\n0.916842\n0.283196\n\n\n75%\n1.111235\n1.648461\n0.577714\n\n\nmax\n1.328334\n4.865765\n0.998563"
  },
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "",
    "text": "Week 01\n\n08-21 Course Introduction\n08-23 Jupyter Introduction (ipynb)\n\nWeek 02\n\n08-28 Python Introduction (ipynb)\n08-30 Functions and Scripts (ipynb) (temp_converter.py)\n\nWeek 03\n\n09-06 Python Introduction to Data Analysis (ipynb)\n\nWeek 04\n\n09-11 Spatial Data\n09-13 GeoPandas (ipynb)\n\nWeek 05\n\n09-18 GeoPandas Spatial Queries (ipynb)\n09-20 Geovisualization (ipynb)\n\nWeek 06\n\n09-25 Spatial Weights (ipynb)\n09-27 Spatial Dependence\n\nWeek 07\n\n10-02 Join Counts (ipynb)\n10-04 Moran’s I (ipynb)\n\nWeek 08\n\n10-09 Studio (Exercise 2 collaboration)\n10-11 Local Autocorrelation (ipynb)\n\nWeek 09\n\n10-16 Local Analysis of Educational Achievment\n10-18 Point Patterns\n\nWeek 10\n\n10-23 Centrography\n10-25 Point Processes\n\nWeek 11\n\n10-30 Quadrat Statistics\n11-01 Nearest Neighbor Statistics\n\nWeek 12\n\n11-06 Distance Based Statistics\n11-08 Geostatistics\n\nWeek 13\n\n11-13 Spatial Interpolation: Deterministic Methods\n11-15 PySAL@NARSC\n\nWeek 14\n\n11-20 Spatial Interpolation: Kriging\n11-22 Thanksgiving Holiday Observed (No class)\n\nWeek 15\n\n11-27 Spatial Disparities Studio: Overview\n11-29 Spatial Disparities Studio: Measuring Spatial Inequality\n\nWeek 16\n\n12-04 Spatial Disparities Studio: Geoprocessing for Spatial Inequality Anaysis\n12-06 Spatial Disparities Studio: Integration Geoprocessing and Statistical Analysis of Disparities\n\nWeek 17\n\n12-11 Final Review"
  },
  {
    "objectID": "lectures/index.html#lectures",
    "href": "lectures/index.html#lectures",
    "title": "Geography 385 Spatial Data Analysis",
    "section": "",
    "text": "Week 01\n\n08-21 Course Introduction\n08-23 Jupyter Introduction (ipynb)\n\nWeek 02\n\n08-28 Python Introduction (ipynb)\n08-30 Functions and Scripts (ipynb) (temp_converter.py)\n\nWeek 03\n\n09-06 Python Introduction to Data Analysis (ipynb)\n\nWeek 04\n\n09-11 Spatial Data\n09-13 GeoPandas (ipynb)\n\nWeek 05\n\n09-18 GeoPandas Spatial Queries (ipynb)\n09-20 Geovisualization (ipynb)\n\nWeek 06\n\n09-25 Spatial Weights (ipynb)\n09-27 Spatial Dependence\n\nWeek 07\n\n10-02 Join Counts (ipynb)\n10-04 Moran’s I (ipynb)\n\nWeek 08\n\n10-09 Studio (Exercise 2 collaboration)\n10-11 Local Autocorrelation (ipynb)\n\nWeek 09\n\n10-16 Local Analysis of Educational Achievment\n10-18 Point Patterns\n\nWeek 10\n\n10-23 Centrography\n10-25 Point Processes\n\nWeek 11\n\n10-30 Quadrat Statistics\n11-01 Nearest Neighbor Statistics\n\nWeek 12\n\n11-06 Distance Based Statistics\n11-08 Geostatistics\n\nWeek 13\n\n11-13 Spatial Interpolation: Deterministic Methods\n11-15 PySAL@NARSC\n\nWeek 14\n\n11-20 Spatial Interpolation: Kriging\n11-22 Thanksgiving Holiday Observed (No class)\n\nWeek 15\n\n11-27 Spatial Disparities Studio: Overview\n11-29 Spatial Disparities Studio: Measuring Spatial Inequality\n\nWeek 16\n\n12-04 Spatial Disparities Studio: Geoprocessing for Spatial Inequality Anaysis\n12-06 Spatial Disparities Studio: Integration Geoprocessing and Statistical Analysis of Disparities\n\nWeek 17\n\n12-11 Final Review"
  },
  {
    "objectID": "lectures/week-05/2023-09-18.html",
    "href": "lectures/week-05/2023-09-18.html",
    "title": "GeoPandas",
    "section": "",
    "text": "GeoPandas Structure\nWorking with GeoDataFrames and GeoSeriesf\nCarrying out project\nBasic spatial queries and attribute construction\nimport geopandas\n\n/tmp/ipykernel_3120570/1616829109.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas\ngdf = geopandas.read_file(\"./data/shared/covid/gz_2010_us_040_00_500k.json\")\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\ngdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\ngdf.columns.values\n\narray(['GEO_ID', 'STATE', 'NAME', 'LSAD', 'CENSUSAREA', 'geometry'],\n      dtype=object)\ngdf.plot()\n\n&lt;AxesSubplot:&gt;\ntype(gdf)\n\ngeopandas.geodataframe.GeoDataFrame"
  },
  {
    "objectID": "lectures/week-05/2023-09-18.html#geometry",
    "href": "lectures/week-05/2023-09-18.html#geometry",
    "title": "GeoPandas",
    "section": "Geometry",
    "text": "Geometry\n\ngdf.geometry\n\n0     MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n1     MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n2     MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n3     POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n4     POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n5     POLYGON ((-75.52684 39.65571, -75.52634 39.656...\n6     MULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n7     MULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n8     MULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n9     POLYGON ((-75.41504 39.80179, -75.42804 39.809...\n10    MULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n11    POLYGON ((-81.67754 36.58812, -81.68014 36.585...\n12    MULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n13    POLYGON ((-114.05060 37.00040, -114.05175 37.0...\n14    MULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n15    MULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n16    MULTIPOLYGON (((-65.58733 18.38199, -65.59122 ...\n17    MULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n18    MULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n19    MULTIPOLYGON (((-164.97620 54.13459, -164.9377...\n20    POLYGON ((-109.04522 36.99908, -109.04524 36.9...\n21    POLYGON ((-94.55929 36.49950, -94.51948 36.499...\n22    MULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n23    POLYGON ((-102.04224 36.99308, -102.05450 36.9...\n24    MULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n25    MULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n26    POLYGON ((-77.03860 38.79151, -77.03890 38.800...\n27    MULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n28    POLYGON ((-81.44412 30.70971, -81.44872 30.709...\n29    MULTIPOLYGON (((-171.73761 25.79210, -171.7223...\n30    POLYGON ((-111.04669 42.00157, -111.41587 42.0...\n31    POLYGON ((-87.53233 39.99778, -87.53254 39.987...\n32    POLYGON ((-88.02803 37.79922, -88.02938 37.803...\n33    POLYGON ((-95.76565 40.58521, -95.75889 40.588...\n34    POLYGON ((-94.61808 36.99813, -94.62522 36.998...\n35    MULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n36    MULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n37    POLYGON ((-91.37161 43.50095, -91.37695 43.500...\n38    MULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n39    POLYGON ((-89.53910 36.49820, -89.53452 36.491...\n40    POLYGON ((-95.76565 40.58521, -95.76853 40.583...\n41    MULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n42    POLYGON ((-109.05004 31.33250, -109.05017 31.4...\n43    POLYGON ((-96.56328 45.93524, -96.57690 45.935...\n44    POLYGON ((-94.61792 36.49941, -94.61531 36.484...\n45    POLYGON ((-117.22007 44.30138, -117.22245 44.2...\n46    POLYGON ((-78.54109 33.85111, -78.55394 33.847...\n47    POLYGON ((-96.44341 42.48949, -96.45971 42.486...\n48    POLYGON ((-72.04008 44.15575, -72.04271 44.152...\n49    MULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n50    POLYGON ((-81.96830 37.53780, -81.96540 37.541...\n51    POLYGON ((-109.05008 41.00066, -109.17368 41.0...\nName: geometry, dtype: geometry\n\n\n\ngdf.iloc[0].geometry\n\n\n\n\n\ngdf.shape\n\n(52, 6)\n\n\n\ngdf.head(52)\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n\n\n5\n0400000US34\n34\nNew Jersey\n\n7354.220\nPOLYGON ((-75.52684 39.65571, -75.52634 39.656...\n\n\n6\n0400000US36\n36\nNew York\n\n47126.399\nMULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n\n\n7\n0400000US37\n37\nNorth Carolina\n\n48617.905\nMULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n\n\n8\n0400000US39\n39\nOhio\n\n40860.694\nMULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n\n\n9\n0400000US42\n42\nPennsylvania\n\n44742.703\nPOLYGON ((-75.41504 39.80179, -75.42804 39.809...\n\n\n10\n0400000US44\n44\nRhode Island\n\n1033.814\nMULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n\n\n11\n0400000US47\n47\nTennessee\n\n41234.896\nPOLYGON ((-81.67754 36.58812, -81.68014 36.585...\n\n\n12\n0400000US48\n48\nTexas\n\n261231.711\nMULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n\n\n13\n0400000US49\n49\nUtah\n\n82169.620\nPOLYGON ((-114.05060 37.00040, -114.05175 37.0...\n\n\n14\n0400000US53\n53\nWashington\n\n66455.521\nMULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n\n\n15\n0400000US55\n55\nWisconsin\n\n54157.805\nMULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n\n\n16\n0400000US72\n72\nPuerto Rico\n\n3423.775\nMULTIPOLYGON (((-65.58733 18.38199, -65.59122 ...\n\n\n17\n0400000US24\n24\nMaryland\n\n9707.241\nMULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n\n\n18\n0400000US01\n01\nAlabama\n\n50645.326\nMULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n\n\n19\n0400000US02\n02\nAlaska\n\n570640.950\nMULTIPOLYGON (((-164.97620 54.13459, -164.9377...\n\n\n20\n0400000US04\n04\nArizona\n\n113594.084\nPOLYGON ((-109.04522 36.99908, -109.04524 36.9...\n\n\n21\n0400000US05\n05\nArkansas\n\n52035.477\nPOLYGON ((-94.55929 36.49950, -94.51948 36.499...\n\n\n22\n0400000US06\n06\nCalifornia\n\n155779.220\nMULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n\n\n23\n0400000US08\n08\nColorado\n\n103641.888\nPOLYGON ((-102.04224 36.99308, -102.05450 36.9...\n\n\n24\n0400000US09\n09\nConnecticut\n\n4842.355\nMULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n\n\n25\n0400000US10\n10\nDelaware\n\n1948.543\nMULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n\n\n26\n0400000US11\n11\nDistrict of Columbia\n\n61.048\nPOLYGON ((-77.03860 38.79151, -77.03890 38.800...\n\n\n27\n0400000US12\n12\nFlorida\n\n53624.759\nMULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n\n\n28\n0400000US13\n13\nGeorgia\n\n57513.485\nPOLYGON ((-81.44412 30.70971, -81.44872 30.709...\n\n\n29\n0400000US15\n15\nHawaii\n\n6422.628\nMULTIPOLYGON (((-171.73761 25.79210, -171.7223...\n\n\n30\n0400000US16\n16\nIdaho\n\n82643.117\nPOLYGON ((-111.04669 42.00157, -111.41587 42.0...\n\n\n31\n0400000US17\n17\nIllinois\n\n55518.930\nPOLYGON ((-87.53233 39.99778, -87.53254 39.987...\n\n\n32\n0400000US18\n18\nIndiana\n\n35826.109\nPOLYGON ((-88.02803 37.79922, -88.02938 37.803...\n\n\n33\n0400000US19\n19\nIowa\n\n55857.130\nPOLYGON ((-95.76565 40.58521, -95.75889 40.588...\n\n\n34\n0400000US20\n20\nKansas\n\n81758.717\nPOLYGON ((-94.61808 36.99813, -94.62522 36.998...\n\n\n35\n0400000US21\n21\nKentucky\n\n39486.338\nMULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n\n\n36\n0400000US22\n22\nLouisiana\n\n43203.905\nMULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n\n\n37\n0400000US27\n27\nMinnesota\n\n79626.743\nPOLYGON ((-91.37161 43.50095, -91.37695 43.500...\n\n\n38\n0400000US28\n28\nMississippi\n\n46923.274\nMULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n\n\n39\n0400000US29\n29\nMissouri\n\n68741.522\nPOLYGON ((-89.53910 36.49820, -89.53452 36.491...\n\n\n40\n0400000US31\n31\nNebraska\n\n76824.171\nPOLYGON ((-95.76565 40.58521, -95.76853 40.583...\n\n\n41\n0400000US33\n33\nNew Hampshire\n\n8952.651\nMULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n\n\n42\n0400000US35\n35\nNew Mexico\n\n121298.148\nPOLYGON ((-109.05004 31.33250, -109.05017 31.4...\n\n\n43\n0400000US38\n38\nNorth Dakota\n\n69000.798\nPOLYGON ((-96.56328 45.93524, -96.57690 45.935...\n\n\n44\n0400000US40\n40\nOklahoma\n\n68594.921\nPOLYGON ((-94.61792 36.49941, -94.61531 36.484...\n\n\n45\n0400000US41\n41\nOregon\n\n95988.013\nPOLYGON ((-117.22007 44.30138, -117.22245 44.2...\n\n\n46\n0400000US45\n45\nSouth Carolina\n\n30060.696\nPOLYGON ((-78.54109 33.85111, -78.55394 33.847...\n\n\n47\n0400000US46\n46\nSouth Dakota\n\n75811.000\nPOLYGON ((-96.44341 42.48949, -96.45971 42.486...\n\n\n48\n0400000US50\n50\nVermont\n\n9216.657\nPOLYGON ((-72.04008 44.15575, -72.04271 44.152...\n\n\n49\n0400000US51\n51\nVirginia\n\n39490.086\nMULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n\n\n50\n0400000US54\n54\nWest Virginia\n\n24038.210\nPOLYGON ((-81.96830 37.53780, -81.96540 37.541...\n\n\n51\n0400000US56\n56\nWyoming\n\n97093.141\nPOLYGON ((-109.05008 41.00066, -109.17368 41.0...\n\n\n\n\n\n\n\n\ndrop_states = ['15', '02', '72'] # HA, AK, PR\n\n\ngdf[gdf.STATE.isin(drop_states)].plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf[~gdf.STATE.isin(drop_states)].plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf = gdf[~gdf.STATE.isin(drop_states)]\n\n\ngdf.shape\n\n(49, 6)\n\n\n\ngdf.index\n\nIndex([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 17, 18,\n       20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51],\n      dtype='int64')\n\n\n\ngdf.reset_index(inplace=True)\n\n\ngdf.index\n\nRangeIndex(start=0, stop=49, step=1)\n\n\n\ngdf.head(49)\n\n\n\n\n\n\n\n\nindex\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n\n\n5\n5\n0400000US34\n34\nNew Jersey\n\n7354.220\nPOLYGON ((-75.52684 39.65571, -75.52634 39.656...\n\n\n6\n6\n0400000US36\n36\nNew York\n\n47126.399\nMULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n\n\n7\n7\n0400000US37\n37\nNorth Carolina\n\n48617.905\nMULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n\n\n8\n8\n0400000US39\n39\nOhio\n\n40860.694\nMULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n\n\n9\n9\n0400000US42\n42\nPennsylvania\n\n44742.703\nPOLYGON ((-75.41504 39.80179, -75.42804 39.809...\n\n\n10\n10\n0400000US44\n44\nRhode Island\n\n1033.814\nMULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n\n\n11\n11\n0400000US47\n47\nTennessee\n\n41234.896\nPOLYGON ((-81.67754 36.58812, -81.68014 36.585...\n\n\n12\n12\n0400000US48\n48\nTexas\n\n261231.711\nMULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n\n\n13\n13\n0400000US49\n49\nUtah\n\n82169.620\nPOLYGON ((-114.05060 37.00040, -114.05175 37.0...\n\n\n14\n14\n0400000US53\n53\nWashington\n\n66455.521\nMULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n\n\n15\n15\n0400000US55\n55\nWisconsin\n\n54157.805\nMULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n\n\n16\n17\n0400000US24\n24\nMaryland\n\n9707.241\nMULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n\n\n17\n18\n0400000US01\n01\nAlabama\n\n50645.326\nMULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n\n\n18\n20\n0400000US04\n04\nArizona\n\n113594.084\nPOLYGON ((-109.04522 36.99908, -109.04524 36.9...\n\n\n19\n21\n0400000US05\n05\nArkansas\n\n52035.477\nPOLYGON ((-94.55929 36.49950, -94.51948 36.499...\n\n\n20\n22\n0400000US06\n06\nCalifornia\n\n155779.220\nMULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n\n\n21\n23\n0400000US08\n08\nColorado\n\n103641.888\nPOLYGON ((-102.04224 36.99308, -102.05450 36.9...\n\n\n22\n24\n0400000US09\n09\nConnecticut\n\n4842.355\nMULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n\n\n23\n25\n0400000US10\n10\nDelaware\n\n1948.543\nMULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n\n\n24\n26\n0400000US11\n11\nDistrict of Columbia\n\n61.048\nPOLYGON ((-77.03860 38.79151, -77.03890 38.800...\n\n\n25\n27\n0400000US12\n12\nFlorida\n\n53624.759\nMULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n\n\n26\n28\n0400000US13\n13\nGeorgia\n\n57513.485\nPOLYGON ((-81.44412 30.70971, -81.44872 30.709...\n\n\n27\n30\n0400000US16\n16\nIdaho\n\n82643.117\nPOLYGON ((-111.04669 42.00157, -111.41587 42.0...\n\n\n28\n31\n0400000US17\n17\nIllinois\n\n55518.930\nPOLYGON ((-87.53233 39.99778, -87.53254 39.987...\n\n\n29\n32\n0400000US18\n18\nIndiana\n\n35826.109\nPOLYGON ((-88.02803 37.79922, -88.02938 37.803...\n\n\n30\n33\n0400000US19\n19\nIowa\n\n55857.130\nPOLYGON ((-95.76565 40.58521, -95.75889 40.588...\n\n\n31\n34\n0400000US20\n20\nKansas\n\n81758.717\nPOLYGON ((-94.61808 36.99813, -94.62522 36.998...\n\n\n32\n35\n0400000US21\n21\nKentucky\n\n39486.338\nMULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n\n\n33\n36\n0400000US22\n22\nLouisiana\n\n43203.905\nMULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n\n\n34\n37\n0400000US27\n27\nMinnesota\n\n79626.743\nPOLYGON ((-91.37161 43.50095, -91.37695 43.500...\n\n\n35\n38\n0400000US28\n28\nMississippi\n\n46923.274\nMULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n\n\n36\n39\n0400000US29\n29\nMissouri\n\n68741.522\nPOLYGON ((-89.53910 36.49820, -89.53452 36.491...\n\n\n37\n40\n0400000US31\n31\nNebraska\n\n76824.171\nPOLYGON ((-95.76565 40.58521, -95.76853 40.583...\n\n\n38\n41\n0400000US33\n33\nNew Hampshire\n\n8952.651\nMULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n\n\n39\n42\n0400000US35\n35\nNew Mexico\n\n121298.148\nPOLYGON ((-109.05004 31.33250, -109.05017 31.4...\n\n\n40\n43\n0400000US38\n38\nNorth Dakota\n\n69000.798\nPOLYGON ((-96.56328 45.93524, -96.57690 45.935...\n\n\n41\n44\n0400000US40\n40\nOklahoma\n\n68594.921\nPOLYGON ((-94.61792 36.49941, -94.61531 36.484...\n\n\n42\n45\n0400000US41\n41\nOregon\n\n95988.013\nPOLYGON ((-117.22007 44.30138, -117.22245 44.2...\n\n\n43\n46\n0400000US45\n45\nSouth Carolina\n\n30060.696\nPOLYGON ((-78.54109 33.85111, -78.55394 33.847...\n\n\n44\n47\n0400000US46\n46\nSouth Dakota\n\n75811.000\nPOLYGON ((-96.44341 42.48949, -96.45971 42.486...\n\n\n45\n48\n0400000US50\n50\nVermont\n\n9216.657\nPOLYGON ((-72.04008 44.15575, -72.04271 44.152...\n\n\n46\n49\n0400000US51\n51\nVirginia\n\n39490.086\nMULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n\n\n47\n50\n0400000US54\n54\nWest Virginia\n\n24038.210\nPOLYGON ((-81.96830 37.53780, -81.96540 37.541...\n\n\n48\n51\n0400000US56\n56\nWyoming\n\n97093.141\nPOLYGON ((-109.05008 41.00066, -109.17368 41.0..."
  },
  {
    "objectID": "lectures/week-05/2023-09-18.html#centroids-and-geometry",
    "href": "lectures/week-05/2023-09-18.html#centroids-and-geometry",
    "title": "GeoPandas",
    "section": "Centroids and geometry",
    "text": "Centroids and geometry\n\ngdf.centroid\n\n/tmp/ipykernel_3120570/2017122361.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf.centroid\n\n\n0      POINT (-69.22532 45.36948)\n1      POINT (-71.79546 42.25229)\n2      POINT (-85.43751 44.35323)\n3     POINT (-109.64507 47.03355)\n4     POINT (-116.65540 39.35646)\n5      POINT (-74.66099 40.18393)\n6      POINT (-75.50198 42.93930)\n7      POINT (-79.35542 35.53980)\n8      POINT (-82.79018 40.29333)\n9      POINT (-77.79953 40.87382)\n10     POINT (-71.55250 41.67619)\n11     POINT (-86.34329 35.84299)\n12     POINT (-99.35528 31.49051)\n13    POINT (-111.67820 39.32379)\n14    POINT (-120.45017 47.38108)\n15     POINT (-90.01113 44.63829)\n16     POINT (-76.76446 39.03041)\n17     POINT (-86.82843 32.78969)\n18    POINT (-111.66458 34.29326)\n19     POINT (-92.43928 34.89974)\n20    POINT (-119.61077 37.24612)\n21    POINT (-105.54782 38.99855)\n22     POINT (-72.72576 41.62055)\n23     POINT (-75.50018 38.99178)\n24     POINT (-77.01630 38.90473)\n25     POINT (-82.50162 28.64096)\n26     POINT (-83.44606 32.64908)\n27    POINT (-114.65933 44.38912)\n28     POINT (-89.19828 40.06474)\n29     POINT (-86.27529 39.90853)\n30     POINT (-93.50004 42.07462)\n31     POINT (-98.38022 38.48470)\n32     POINT (-85.29049 37.52666)\n33     POINT (-91.98048 31.05264)\n34     POINT (-94.30870 46.31645)\n35     POINT (-89.66510 32.75040)\n36     POINT (-92.47742 38.36762)\n37     POINT (-99.81080 41.52715)\n38     POINT (-71.57760 43.68569)\n39    POINT (-106.10838 34.42137)\n40    POINT (-100.46931 47.44634)\n41     POINT (-97.50844 35.58350)\n42    POINT (-120.55529 43.93673)\n43     POINT (-80.89550 33.90710)\n44    POINT (-100.23049 44.43615)\n45     POINT (-72.66272 44.07518)\n46     POINT (-78.80562 37.51539)\n47     POINT (-80.61385 38.64251)\n48    POINT (-107.55145 42.99964)\ndtype: geometry\n\n\n\ngdf.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf.to_crs(gdf.estimate_utm_crs()).plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf = gdf.to_crs(gdf.estimate_utm_crs())\n\n\ngdf.centroid\n\n0      POINT (2359392.452 5305169.775)\n1      POINT (2253174.905 4901469.336)\n2      POINT (1108124.800 4935453.192)\n3      POINT (-765955.563 5348067.290)\n4     POINT (-1548246.254 4630302.336)\n5      POINT (2065285.016 4612318.097)\n6      POINT (1930049.196 4904992.641)\n7      POINT (1742974.165 4021487.098)\n8      POINT (1368634.294 4510266.503)\n9      POINT (1784134.621 4638257.315)\n10     POINT (2289313.142 4842131.086)\n11     POINT (1102080.849 3988935.477)\n12     POINT (-103892.519 3500028.226)\n13    POINT (-1115997.209 4520861.031)\n14    POINT (-1565541.918 5625943.989)\n15      POINT (738620.228 4945674.977)\n16     POINT (1910429.070 4447760.368)\n17     POINT (1078425.172 3643488.570)\n18    POINT (-1230212.721 3954699.221)\n19      POINT (550933.347 3861451.888)\n20    POINT (-1870826.275 4466915.330)\n21     POINT (-590190.342 4392344.096)\n22     POINT (2192677.610 4811150.640)\n23     POINT (2020359.310 4464239.060)\n24     POINT (1889584.643 4429716.358)\n25     POINT (1535090.511 3210901.406)\n26     POINT (1399285.825 3651528.582)\n27    POINT (-1223231.299 5144300.473)\n28      POINT (824539.285 4439350.669)\n29     POINT (1074696.801 4437810.568)\n30      POINT (458939.639 4658274.632)\n31       POINT (29865.949 4274404.492)\n32     POINT (1181577.693 4182456.487)\n33      POINT (598555.176 3435096.661)\n34      POINT (399444.786 5126179.145)\n35      POINT (812334.714 3626923.994)\n36      POINT (547395.056 4245527.933)\n37      POINT (-68074.613 4621136.163)\n38     POINT (2227578.113 5064712.956)\n39     POINT (-710483.211 3886170.809)\n40      POINT (-63437.281 5282398.408)\n41       POINT (91977.904 3947894.482)\n42    POINT (-1716081.976 5249621.330)\n43     POINT (1623336.735 3818285.055)\n44      POINT (-76107.494 4946849.414)\n45     POINT (2128860.270 5085004.659)\n46     POINT (1758189.886 4249101.361)\n47     POINT (1579306.753 4350654.685)\n48     POINT (-688676.799 4864507.482)\ndtype: geometry\n\n\n\ngdf.crs\n\n&lt;Projected CRS: EPSG:32615&gt;\nName: WGS 84 / UTM zone 15N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 96°W and 90°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - Manitoba; Nunavut; Ontario. Ecuador -Galapagos. Guatemala. Mexico. United States (USA).\n- bounds: (-96.0, 0.0, -90.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 15N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf['centroid'] = gdf.centroid\n\n\ngdf.head()\n\n\n\n\n\n\n\n\nindex\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\ncentroid\n\n\n\n\n0\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((2516172.424 5253443.650, 25164...\nPOINT (2359392.452 5305169.775)\n\n\n1\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((2351724.597 4850457.653, 23526...\nPOINT (2253174.905 4901469.336)\n\n\n2\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((821167.988 5338182.388, 821794...\nPOINT (1108124.800 4935453.192)\n\n\n3\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-371533.418 5042503.702, -386687.255...\nPOINT (-765955.563 5348067.290)\n\n\n4\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-1384104.733 4308747.816, -1385163.9...\nPOINT (-1548246.254 4630302.336)\n\n\n\n\n\n\n\n\nbase = gdf.plot()\ngdf.centroid.plot(ax=base, color='r')\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "lectures/week-05/2023-09-18.html#geometry-attribute",
    "href": "lectures/week-05/2023-09-18.html#geometry-attribute",
    "title": "GeoPandas",
    "section": "Geometry Attribute",
    "text": "Geometry Attribute\n\ngdf.geometry\n\n0     MULTIPOLYGON (((2516172.424 5253443.650, 25164...\n1     MULTIPOLYGON (((2351724.597 4850457.653, 23526...\n2     MULTIPOLYGON (((821167.988 5338182.388, 821794...\n3     POLYGON ((-371533.418 5042503.702, -386687.255...\n4     POLYGON ((-1384104.733 4308747.816, -1385163.9...\n5     POLYGON ((2003056.105 4538350.496, 2003083.601...\n6     MULTIPOLYGON (((2267648.358 4790204.443, 22689...\n7     MULTIPOLYGON (((1438177.023 4038770.483, 14383...\n8     MULTIPOLYGON (((1347764.520 4669512.453, 13480...\n9     POLYGON ((2009412.336 4556637.291, 2008122.809...\n10    MULTIPOLYGON (((2312801.421 4845029.912, 23131...\n11    POLYGON ((1514737.024 4109398.060, 1514537.561...\n12    MULTIPOLYGON (((92911.351 3092597.856, 92990.0...\n13    POLYGON ((-1384104.733 4308747.816, -1381904.7...\n14    MULTIPOLYGON (((-1682205.672 5879501.588, -167...\n15    MULTIPOLYGON (((693374.944 5210974.031, 693159...\n16    MULTIPOLYGON (((1987068.430 4366526.262, 19890...\n17    MULTIPOLYGON (((1264668.058 3457276.553, 12625...\n18    POLYGON ((-932699.253 4217338.169, -933275.202...\n19    POLYGON ((360351.380 4040482.637, 363916.663 4...\n20    MULTIPOLYGON (((-2114916.187 4624534.270, -211...\n21    POLYGON ((-305537.860 4132535.682, -306632.528...\n22    MULTIPOLYGON (((2273709.589 4796000.071, 22727...\n23    MULTIPOLYGON (((2000818.277 4534887.709, 20008...\n24    POLYGON ((1889905.340 4416707.434, 1889692.665...\n25    MULTIPOLYGON (((1260129.872 3309151.351, 12619...\n26    POLYGON ((1610127.048 3455093.398, 1609687.082...\n27    POLYGON ((-996830.450 4810504.086, -1027552.17...\n28    POLYGON ((966857.622 4441852.185, 966910.074 4...\n29    POLYGON ((937850.748 4195199.124, 937705.699 4...\n30    POLYGON ((265938.864 4496389.554, 266523.302 4...\n31    POLYGON ((356020.762 4095889.131, 355386.048 4...\n32    MULTIPOLYGON (((1335046.637 4091312.569, 13350...\n33    MULTIPOLYGON (((899967.144 3298554.678, 897713...\n34    POLYGON ((631645.161 4817734.258, 631214.291 4...\n35    MULTIPOLYGON (((912838.208 3354370.602, 918103...\n36    POLYGON ((810004.565 4044781.403, 810441.658 4...\n37    POLYGON ((265938.864 4496389.554, 265688.248 4...\n38    MULTIPOLYGON (((2183943.003 4939995.423, 21838...\n39    POLYGON ((-1036062.528 3580182.026, -1033579.7...\n40    POLYGON ((223770.899 5093027.974, 222715.445 5...\n41    POLYGON ((355100.062 4040560.166, 355306.819 4...\n42    POLYGON ((-1431865.838 5199448.159, -1432139.7...\n43    POLYGON ((1842940.161 3841258.252, 1841789.432...\n44    POLYGON ((217005.965 4709874.192, 215650.454 4...\n45    POLYGON ((2176439.257 5107755.756, 2176332.418...\n46    MULTIPOLYGON (((1994538.922 4338939.700, 19949...\n47    POLYGON ((1476144.128 4212196.934, 1476356.502...\n48    POLYGON ((-852129.655 4664855.469, -862571.421...\nName: geometry, dtype: geometry\n\n\n\ngdf.set_geometry('centroid').plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf = gdf.set_geometry('centroid')\n\n\ngdf.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf = gdf.set_geometry('geometry')\ngdf.plot()\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "lectures/week-05/2023-09-18.html#basic-spatial-queries-and-operations",
    "href": "lectures/week-05/2023-09-18.html#basic-spatial-queries-and-operations",
    "title": "GeoPandas",
    "section": "Basic Spatial Queries and Operations",
    "text": "Basic Spatial Queries and Operations\n\nCentroid of the lower 49\n\ngdf['dissolve_var'] = 1 # all observations have same value for this variable\nus = gdf.dissolve(by='dissolve_var')\n\n\nus.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nus.head()\n\n\n\n\n\n\n\n\ngeometry\nindex\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ncentroid\n\n\ndissolve_var\n\n\n\n\n\n\n\n\n\n\n\n\n1\nMULTIPOLYGON (((-1922552.872 3962305.133, -192...\n0\n0400000US23\n23\nMaine\n\n30842.923\nPOINT (2359392.452 5305169.775)\n\n\n\n\n\n\n\n\nus_cent = us.centroid\nbase = gdf.plot()\nus_cent.plot(ax=base, color='r')\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "lectures/week-05/2023-09-18.html#which-states-are-within-500-miles-804.672-kilometers-of-the-centroid",
    "href": "lectures/week-05/2023-09-18.html#which-states-are-within-500-miles-804.672-kilometers-of-the-centroid",
    "title": "GeoPandas",
    "section": "Which states are within 500 miles (804.672 kilometers) of the centroid?",
    "text": "Which states are within 500 miles (804.672 kilometers) of the centroid?\n\nbuffer = us_cent.buffer(804.672*1000)\nbuffer.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nbase = gdf.plot()\nbuffer.plot(ax=base, color='grey')\nus_cent.plot(ax=base, color='r')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nbase = gdf.plot()\nbuffer.plot(ax=base, color='grey', alpha=0.4)\nus_cent.plot(ax=base, color='r')\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf.sindex.query(buffer, predicate='intersects')\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [19, 36, 12, 39, 41, 31, 21, 37, 28, 30, 15, 34, 48, 44, 40,  3]])\n\n\n\nrfirst, states_intersecting = gdf.sindex.query(buffer, predicate='intersects')\n\n\ngdf.iloc[states_intersecting].plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf[~gdf.index.isin(states_intersecting)].plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nWhich states are on the border of the lower 48?\n\nus.envelope\n\ndissolve_var\n1    POLYGON ((-2186034.766 2758545.507, 2557956.87...\ndtype: geometry\n\n\n\nbb = us.envelope\nbb.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nbase = bb.plot(color='b')\nus.plot(ax=base, color='w') # 2 layers\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nbb_less_49 = bb.difference(us.geometry) # 1 layer\nbb_less_49.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf.sindex.query(bb_less_49, predicate='touches')\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0],\n       [25, 33, 35, 17, 26, 43, 12, 39, 18, 20,  7, 46, 16, 23,  5,  9,\n        22, 10, 28, 29,  8, 15,  2, 34,  1,  6, 45, 38,  0, 27, 42, 40,\n         3, 14]])\n\n\n\n_,border_states = gdf.sindex.query(bb_less_49, predicate='touches')\ngdf[gdf.index.isin(border_states)].plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n\nCreating new attributes\n\nimport numpy as np\n\nborder_dummy = np.zeros(49, 'int')\nborder_dummy[border_states]=1\nborder_dummy\n\narray([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n       0, 1, 1, 0, 0])\n\n\n\ngdf['border'] = border_dummy\ngdf.plot(column='border', categorical=True, legend=True)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf.explore(column='border', categorical=True, legend=False)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngdf.explore()\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngdf.explore(column='border', categorical=True)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngdf.explore?\n\n\nSignature: gdf.explore(*args, **kwargs)\nDocstring:\nInteractive map based on GeoPandas and folium/leaflet.js\nGenerate an interactive leaflet map based on :class:`~geopandas.GeoDataFrame`\nParameters\n----------\ncolumn : str, np.array, pd.Series (default None)\n    The name of the dataframe column, :class:`numpy.array`,\n    or :class:`pandas.Series` to be plotted. If :class:`numpy.array` or\n    :class:`pandas.Series` are used then it must have same length as dataframe.\ncmap : str, matplotlib.Colormap, branca.colormap or function (default None)\n    The name of a colormap recognized by ``matplotlib``, a list-like of colors,\n    :class:`matplotlib.colors.Colormap`, a :class:`branca.colormap.ColorMap` or\n    function that returns a named color or hex based on the column\n    value, e.g.::\n        def my_colormap(value):  # scalar value defined in 'column'\n            if value &gt; 1:\n                return \"green\"\n            return \"red\"\ncolor : str, array-like (default None)\n    Named color or a list-like of colors (named or hex).\nm : folium.Map (default None)\n    Existing map instance on which to draw the plot.\ntiles : str, xyzservices.TileProvider (default 'OpenStreetMap Mapnik')\n    Map tileset to use. Can choose from the list supported by folium, query a\n    :class:`xyzservices.TileProvider` by a name from ``xyzservices.providers``,\n    pass :class:`xyzservices.TileProvider` object or pass custom XYZ URL.\n    The current list of built-in providers (when ``xyzservices`` is not available):\n    ``[\"OpenStreetMap\", \"Stamen Terrain\", “Stamen Toner\", “Stamen Watercolor\"\n    \"CartoDB positron\", “CartoDB dark_matter\"]``\n    You can pass a custom tileset to Folium by passing a Leaflet-style URL\n    to the tiles parameter: ``http://{s}.yourtiles.com/{z}/{x}/{y}.png``.\n    Be sure to check their terms and conditions and to provide attribution with\n    the ``attr`` keyword.\nattr : str (default None)\n    Map tile attribution; only required if passing custom tile URL.\ntooltip : bool, str, int, list (default True)\n    Display GeoDataFrame attributes when hovering over the object.\n    ``True`` includes all columns. ``False`` removes tooltip. Pass string or list of\n    strings to specify a column(s). Integer specifies first n columns to be\n    included. Defaults to ``True``.\npopup : bool, str, int, list (default False)\n    Input GeoDataFrame attributes for object displayed when clicking.\n    ``True`` includes all columns. ``False`` removes popup. Pass string or list of\n    strings to specify a column(s). Integer specifies first n columns to be\n    included. Defaults to ``False``.\nhighlight : bool (default True)\n    Enable highlight functionality when hovering over a geometry.\ncategorical : bool (default False)\n    If ``False``, ``cmap`` will reflect numerical values of the\n    column being plotted. For non-numerical columns, this\n    will be set to True.\nlegend : bool (default True)\n    Plot a legend in choropleth plots.\n    Ignored if no ``column`` is given.\nscheme : str (default None)\n    Name of a choropleth classification scheme (requires ``mapclassify`` &gt;= 2.4.0).\n    A :func:`mapclassify.classify` will be used\n    under the hood. Supported are all schemes provided by ``mapclassify`` (e.g.\n    ``'BoxPlot'``, ``'EqualInterval'``, ``'FisherJenks'``, ``'FisherJenksSampled'``,\n    ``'HeadTailBreaks'``, ``'JenksCaspall'``, ``'JenksCaspallForced'``,\n    ``'JenksCaspallSampled'``, ``'MaxP'``, ``'MaximumBreaks'``,\n    ``'NaturalBreaks'``, ``'Quantiles'``, ``'Percentiles'``, ``'StdMean'``,\n    ``'UserDefined'``). Arguments can be passed in ``classification_kwds``.\nk : int (default 5)\n    Number of classes\nvmin : None or float (default None)\n    Minimum value of ``cmap``. If ``None``, the minimum data value\n    in the column to be plotted is used.\nvmax : None or float (default None)\n    Maximum value of ``cmap``. If ``None``, the maximum data value\n    in the column to be plotted is used.\nwidth : pixel int or percentage string (default: '100%')\n    Width of the folium :class:`~folium.folium.Map`. If the argument\n    m is given explicitly, width is ignored.\nheight : pixel int or percentage string (default: '100%')\n    Height of the folium :class:`~folium.folium.Map`. If the argument\n    m is given explicitly, height is ignored.\ncategories : list-like\n    Ordered list-like object of categories to be used for categorical plot.\nclassification_kwds : dict (default None)\n    Keyword arguments to pass to mapclassify\ncontrol_scale : bool, (default True)\n    Whether to add a control scale on the map.\nmarker_type : str, folium.Circle, folium.CircleMarker, folium.Marker (default None)\n    Allowed string options are ('marker', 'circle', 'circle_marker'). Defaults to\n    folium.CircleMarker.\nmarker_kwds: dict (default {})\n    Additional keywords to be passed to the selected ``marker_type``, e.g.:\n    radius : float (default 2 for ``circle_marker`` and 50 for ``circle``))\n        Radius of the circle, in meters (for ``circle``) or pixels\n        (for ``circle_marker``).\n    fill : bool (default True)\n        Whether to fill the ``circle`` or ``circle_marker`` with color.\n    icon : folium.map.Icon\n        the :class:`folium.map.Icon` object to use to render the marker.\n    draggable : bool (default False)\n        Set to True to be able to drag the marker around the map.\nstyle_kwds : dict (default {})\n    Additional style to be passed to folium ``style_function``:\n    stroke : bool (default True)\n        Whether to draw stroke along the path. Set it to ``False`` to\n        disable borders on polygons or circles.\n    color : str\n        Stroke color\n    weight : int\n        Stroke width in pixels\n    opacity : float (default 1.0)\n        Stroke opacity\n    fill : boolean (default True)\n        Whether to fill the path with color. Set it to ``False`` to\n        disable filling on polygons or circles.\n    fillColor : str\n        Fill color. Defaults to the value of the color option\n    fillOpacity : float (default 0.5)\n        Fill opacity.\n    style_function : callable\n        Function mapping a GeoJson Feature to a style ``dict``.\n        * Style properties :func:`folium.vector_layers.path_options`\n        * GeoJson features :class:`GeoDataFrame.__geo_interface__`\n        e.g.::\n            lambda x: {\"color\":\"red\" if x[\"properties\"][\"gdp_md_est\"]&lt;10**6\n                                         else \"blue\"}\n    Plus all supported by :func:`folium.vector_layers.path_options`. See the\n    documentation of :class:`folium.features.GeoJson` for details.\nhighlight_kwds : dict (default {})\n    Style to be passed to folium highlight_function. Uses the same keywords\n    as ``style_kwds``. When empty, defaults to ``{\"fillOpacity\": 0.75}``.\ntooltip_kwds : dict (default {})\n    Additional keywords to be passed to :class:`folium.features.GeoJsonTooltip`,\n    e.g. ``aliases``, ``labels``, or ``sticky``.\npopup_kwds : dict (default {})\n    Additional keywords to be passed to :class:`folium.features.GeoJsonPopup`,\n    e.g. ``aliases`` or ``labels``.\nlegend_kwds : dict (default {})\n    Additional keywords to be passed to the legend.\n    Currently supported customisation:\n    caption : string\n        Custom caption of the legend. Defaults to the column name.\n    Additional accepted keywords when ``scheme`` is specified:\n    colorbar : bool (default True)\n        An option to control the style of the legend. If True, continuous\n        colorbar will be used. If False, categorical legend will be used for bins.\n    scale : bool (default True)\n        Scale bins along the colorbar axis according to the bin edges (True)\n        or use the equal length for each bin (False)\n    fmt : string (default \"{:.2f}\")\n        A formatting specification for the bin edges of the classes in the\n        legend. For example, to have no decimals: ``{\"fmt\": \"{:.0f}\"}``. Applies\n        if ``colorbar=False``.\n    labels : list-like\n        A list of legend labels to override the auto-generated labels.\n        Needs to have the same number of elements as the number of\n        classes (`k`). Applies if ``colorbar=False``.\n    interval : boolean (default False)\n        An option to control brackets from mapclassify legend.\n        If True, open/closed interval brackets are shown in the legend.\n        Applies if ``colorbar=False``.\n    max_labels : int, default 10\n        Maximum number of colorbar tick labels (requires branca&gt;=0.5.0)\nmap_kwds : dict (default {})\n    Additional keywords to be passed to folium :class:`~folium.folium.Map`,\n    e.g. ``dragging``, or ``scrollWheelZoom``.\n**kwargs : dict\n    Additional options to be passed on to the folium object.\nReturns\n-------\nm : folium.folium.Map\n    folium :class:`~folium.folium.Map` instance\nExamples\n--------\n&gt;&gt;&gt; import geodatasets\n&gt;&gt;&gt; df = geopandas.read_file(\n...     geodatasets.get_path(\"geoda.chicago_health\")\n... )\n&gt;&gt;&gt; df.head(2)  # doctest: +SKIP\n   ComAreaID  ...                                           geometry\n0         35  ...  POLYGON ((-87.60914 41.84469, -87.60915 41.844...\n1         36  ...  POLYGON ((-87.59215 41.81693, -87.59231 41.816...\n[2 rows x 87 columns]\n&gt;&gt;&gt; df.explore(\"Pop2012\", cmap=\"Blues\")  # doctest: +SKIP\nFile:      /opt/tljh/user/lib/python3.9/site-packages/geopandas/geodataframe.py\nType:      method"
  },
  {
    "objectID": "lectures/week-15/2023-11-29-spatial-disparities.html#concepts",
    "href": "lectures/week-15/2023-11-29-spatial-disparities.html#concepts",
    "title": "Spatial Disparities",
    "section": "Concepts",
    "text": "Concepts\n\nEquality\nEquity\nSpatial Disparities"
  },
  {
    "objectID": "lectures/week-15/2023-11-29-spatial-disparities.html#equality-and-inequality",
    "href": "lectures/week-15/2023-11-29-spatial-disparities.html#equality-and-inequality",
    "title": "Spatial Disparities",
    "section": "Equality and Inequality",
    "text": "Equality and Inequality\n\nSource"
  },
  {
    "objectID": "lectures/week-15/2023-11-29-spatial-disparities.html#equality-and-inequality-1",
    "href": "lectures/week-15/2023-11-29-spatial-disparities.html#equality-and-inequality-1",
    "title": "Spatial Disparities",
    "section": "Equality and Inequality",
    "text": "Equality and Inequality\n\nSource"
  },
  {
    "objectID": "lectures/week-15/2023-11-29-spatial-disparities.html#inequality-and-inequity",
    "href": "lectures/week-15/2023-11-29-spatial-disparities.html#inequality-and-inequity",
    "title": "Spatial Disparities",
    "section": "Inequality and Inequity",
    "text": "Inequality and Inequity\n\nInequality implies differences between individuals or groups of individuals\nInequity refers to differences which are considered unfair and unjust\nNot all inequalities are unjust\nAll inequities are the product of unjust inequalities"
  },
  {
    "objectID": "lectures/week-15/2023-11-29-spatial-disparities.html#inequality-of",
    "href": "lectures/week-15/2023-11-29-spatial-disparities.html#inequality-of",
    "title": "Spatial Disparities",
    "section": "Inequality of",
    "text": "Inequality of\n\nincome\nwealth\nhealth burdens\naccessibility to resources\neducational outcomes"
  },
  {
    "objectID": "lectures/week-15/2023-11-29-spatial-disparities.html#why-is-analyzing-inequality-important",
    "href": "lectures/week-15/2023-11-29-spatial-disparities.html#why-is-analyzing-inequality-important",
    "title": "Spatial Disparities",
    "section": "Why is analyzing inequality important?",
    "text": "Why is analyzing inequality important?\n\nScience and understanding\nInequality may impose unfair burdens on members of society\nInequality may reduce the productive capacity of the system\nInequality can lead to increased polarization and fragmentation"
  },
  {
    "objectID": "lectures/week-15/2023-11-29-spatial-disparities.html#spatial-disparities-defined",
    "href": "lectures/week-15/2023-11-29-spatial-disparities.html#spatial-disparities-defined",
    "title": "Spatial Disparities",
    "section": "Spatial Disparities Defined",
    "text": "Spatial Disparities Defined\n\nSpatial disparities occur when socio-economic outcomes differ across places.\n\n\nSpatial Disparities"
  },
  {
    "objectID": "lectures/week-15/2023-11-29-spatial-disparities.html#environmental-justice-literature",
    "href": "lectures/week-15/2023-11-29-spatial-disparities.html#environmental-justice-literature",
    "title": "Spatial Disparities",
    "section": "Environmental Justice Literature",
    "text": "Environmental Justice Literature\n\nMaantay, J. (2002) “Mapping environmental injustices: pitfalls and potential of geographic information systems in assessing environmental health and equity” Environmental Health Perspectives"
  },
  {
    "objectID": "lectures/week-04/2023-09-13.html",
    "href": "lectures/week-04/2023-09-13.html",
    "title": "GeoPandas",
    "section": "",
    "text": "GeoPandas Structure\nWorking with GeoDataFrames and GeoSeriesf\nCarrying out project\nBasic spatial queries and attribute construction\nimport geopandas\n\n/tmp/ipykernel_2579345/1616829109.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas\ngdf = geopandas.read_file(\"./data/shared/covid/gz_2010_us_040_00_500k.json\")\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\ngdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\ngdf.columns.values\n\narray(['GEO_ID', 'STATE', 'NAME', 'LSAD', 'CENSUSAREA', 'geometry'],\n      dtype=object)\ngdf.plot()\n\n&lt;AxesSubplot:&gt;\ntype(gdf)\n\ngeopandas.geodataframe.GeoDataFrame"
  },
  {
    "objectID": "lectures/week-04/2023-09-13.html#geometry",
    "href": "lectures/week-04/2023-09-13.html#geometry",
    "title": "GeoPandas",
    "section": "Geometry",
    "text": "Geometry\n\ngdf.geometry\n\n0     MULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n1     MULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n2     MULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n3     POLYGON ((-104.05770 44.99743, -104.25015 44.9...\n4     POLYGON ((-114.05060 37.00040, -114.04999 36.9...\n5     POLYGON ((-75.52684 39.65571, -75.52634 39.656...\n6     MULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n7     MULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n8     MULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n9     POLYGON ((-75.41504 39.80179, -75.42804 39.809...\n10    MULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n11    POLYGON ((-81.67754 36.58812, -81.68014 36.585...\n12    MULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n13    POLYGON ((-114.05060 37.00040, -114.05175 37.0...\n14    MULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n15    MULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n16    MULTIPOLYGON (((-65.58733 18.38199, -65.59122 ...\n17    MULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n18    MULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n19    MULTIPOLYGON (((-164.97620 54.13459, -164.9377...\n20    POLYGON ((-109.04522 36.99908, -109.04524 36.9...\n21    POLYGON ((-94.55929 36.49950, -94.51948 36.499...\n22    MULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n23    POLYGON ((-102.04224 36.99308, -102.05450 36.9...\n24    MULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n25    MULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n26    POLYGON ((-77.03860 38.79151, -77.03890 38.800...\n27    MULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n28    POLYGON ((-81.44412 30.70971, -81.44872 30.709...\n29    MULTIPOLYGON (((-171.73761 25.79210, -171.7223...\n30    POLYGON ((-111.04669 42.00157, -111.41587 42.0...\n31    POLYGON ((-87.53233 39.99778, -87.53254 39.987...\n32    POLYGON ((-88.02803 37.79922, -88.02938 37.803...\n33    POLYGON ((-95.76565 40.58521, -95.75889 40.588...\n34    POLYGON ((-94.61808 36.99813, -94.62522 36.998...\n35    MULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n36    MULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n37    POLYGON ((-91.37161 43.50095, -91.37695 43.500...\n38    MULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n39    POLYGON ((-89.53910 36.49820, -89.53452 36.491...\n40    POLYGON ((-95.76565 40.58521, -95.76853 40.583...\n41    MULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n42    POLYGON ((-109.05004 31.33250, -109.05017 31.4...\n43    POLYGON ((-96.56328 45.93524, -96.57690 45.935...\n44    POLYGON ((-94.61792 36.49941, -94.61531 36.484...\n45    POLYGON ((-117.22007 44.30138, -117.22245 44.2...\n46    POLYGON ((-78.54109 33.85111, -78.55394 33.847...\n47    POLYGON ((-96.44341 42.48949, -96.45971 42.486...\n48    POLYGON ((-72.04008 44.15575, -72.04271 44.152...\n49    MULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n50    POLYGON ((-81.96830 37.53780, -81.96540 37.541...\n51    POLYGON ((-109.05008 41.00066, -109.17368 41.0...\nName: geometry, dtype: geometry\n\n\n\ngdf.iloc[0].geometry\n\n\n\n\n\ngdf.shape\n\n(52, 6)\n\n\n\ngdf.head(52)\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n\n\n5\n0400000US34\n34\nNew Jersey\n\n7354.220\nPOLYGON ((-75.52684 39.65571, -75.52634 39.656...\n\n\n6\n0400000US36\n36\nNew York\n\n47126.399\nMULTIPOLYGON (((-71.94356 41.28668, -71.92680 ...\n\n\n7\n0400000US37\n37\nNorth Carolina\n\n48617.905\nMULTIPOLYGON (((-82.60288 36.03983, -82.60074 ...\n\n\n8\n0400000US39\n39\nOhio\n\n40860.694\nMULTIPOLYGON (((-82.81349 41.72347, -82.81049 ...\n\n\n9\n0400000US42\n42\nPennsylvania\n\n44742.703\nPOLYGON ((-75.41504 39.80179, -75.42804 39.809...\n\n\n10\n0400000US44\n44\nRhode Island\n\n1033.814\nMULTIPOLYGON (((-71.28157 41.64821, -71.27817 ...\n\n\n11\n0400000US47\n47\nTennessee\n\n41234.896\nPOLYGON ((-81.67754 36.58812, -81.68014 36.585...\n\n\n12\n0400000US48\n48\nTexas\n\n261231.711\nMULTIPOLYGON (((-97.13436 27.89633, -97.13360 ...\n\n\n13\n0400000US49\n49\nUtah\n\n82169.620\nPOLYGON ((-114.05060 37.00040, -114.05175 37.0...\n\n\n14\n0400000US53\n53\nWashington\n\n66455.521\nMULTIPOLYGON (((-123.09055 49.00198, -123.0353...\n\n\n15\n0400000US55\n55\nWisconsin\n\n54157.805\nMULTIPOLYGON (((-90.45525 47.02400, -90.45713 ...\n\n\n16\n0400000US72\n72\nPuerto Rico\n\n3423.775\nMULTIPOLYGON (((-65.58733 18.38199, -65.59122 ...\n\n\n17\n0400000US24\n24\nMaryland\n\n9707.241\nMULTIPOLYGON (((-76.07147 38.20350, -76.04879 ...\n\n\n18\n0400000US01\n01\nAlabama\n\n50645.326\nMULTIPOLYGON (((-85.00237 31.00068, -85.02411 ...\n\n\n19\n0400000US02\n02\nAlaska\n\n570640.950\nMULTIPOLYGON (((-164.97620 54.13459, -164.9377...\n\n\n20\n0400000US04\n04\nArizona\n\n113594.084\nPOLYGON ((-109.04522 36.99908, -109.04524 36.9...\n\n\n21\n0400000US05\n05\nArkansas\n\n52035.477\nPOLYGON ((-94.55929 36.49950, -94.51948 36.499...\n\n\n22\n0400000US06\n06\nCalifornia\n\n155779.220\nMULTIPOLYGON (((-122.44632 37.86105, -122.4385...\n\n\n23\n0400000US08\n08\nColorado\n\n103641.888\nPOLYGON ((-102.04224 36.99308, -102.05450 36.9...\n\n\n24\n0400000US09\n09\nConnecticut\n\n4842.355\nMULTIPOLYGON (((-71.85957 41.32240, -71.86823 ...\n\n\n25\n0400000US10\n10\nDelaware\n\n1948.543\nMULTIPOLYGON (((-75.55945 39.62981, -75.55910 ...\n\n\n26\n0400000US11\n11\nDistrict of Columbia\n\n61.048\nPOLYGON ((-77.03860 38.79151, -77.03890 38.800...\n\n\n27\n0400000US12\n12\nFlorida\n\n53624.759\nMULTIPOLYGON (((-85.15641 29.67963, -85.13740 ...\n\n\n28\n0400000US13\n13\nGeorgia\n\n57513.485\nPOLYGON ((-81.44412 30.70971, -81.44872 30.709...\n\n\n29\n0400000US15\n15\nHawaii\n\n6422.628\nMULTIPOLYGON (((-171.73761 25.79210, -171.7223...\n\n\n30\n0400000US16\n16\nIdaho\n\n82643.117\nPOLYGON ((-111.04669 42.00157, -111.41587 42.0...\n\n\n31\n0400000US17\n17\nIllinois\n\n55518.930\nPOLYGON ((-87.53233 39.99778, -87.53254 39.987...\n\n\n32\n0400000US18\n18\nIndiana\n\n35826.109\nPOLYGON ((-88.02803 37.79922, -88.02938 37.803...\n\n\n33\n0400000US19\n19\nIowa\n\n55857.130\nPOLYGON ((-95.76565 40.58521, -95.75889 40.588...\n\n\n34\n0400000US20\n20\nKansas\n\n81758.717\nPOLYGON ((-94.61808 36.99813, -94.62522 36.998...\n\n\n35\n0400000US21\n21\nKentucky\n\n39486.338\nMULTIPOLYGON (((-83.67541 36.60081, -83.67561 ...\n\n\n36\n0400000US22\n22\nLouisiana\n\n43203.905\nMULTIPOLYGON (((-88.86507 29.75271, -88.88975 ...\n\n\n37\n0400000US27\n27\nMinnesota\n\n79626.743\nPOLYGON ((-91.37161 43.50095, -91.37695 43.500...\n\n\n38\n0400000US28\n28\nMississippi\n\n46923.274\nMULTIPOLYGON (((-88.71072 30.25080, -88.65680 ...\n\n\n39\n0400000US29\n29\nMissouri\n\n68741.522\nPOLYGON ((-89.53910 36.49820, -89.53452 36.491...\n\n\n40\n0400000US31\n31\nNebraska\n\n76824.171\nPOLYGON ((-95.76565 40.58521, -95.76853 40.583...\n\n\n41\n0400000US33\n33\nNew Hampshire\n\n8952.651\nMULTIPOLYGON (((-72.45852 42.72685, -72.45849 ...\n\n\n42\n0400000US35\n35\nNew Mexico\n\n121298.148\nPOLYGON ((-109.05004 31.33250, -109.05017 31.4...\n\n\n43\n0400000US38\n38\nNorth Dakota\n\n69000.798\nPOLYGON ((-96.56328 45.93524, -96.57690 45.935...\n\n\n44\n0400000US40\n40\nOklahoma\n\n68594.921\nPOLYGON ((-94.61792 36.49941, -94.61531 36.484...\n\n\n45\n0400000US41\n41\nOregon\n\n95988.013\nPOLYGON ((-117.22007 44.30138, -117.22245 44.2...\n\n\n46\n0400000US45\n45\nSouth Carolina\n\n30060.696\nPOLYGON ((-78.54109 33.85111, -78.55394 33.847...\n\n\n47\n0400000US46\n46\nSouth Dakota\n\n75811.000\nPOLYGON ((-96.44341 42.48949, -96.45971 42.486...\n\n\n48\n0400000US50\n50\nVermont\n\n9216.657\nPOLYGON ((-72.04008 44.15575, -72.04271 44.152...\n\n\n49\n0400000US51\n51\nVirginia\n\n39490.086\nMULTIPOLYGON (((-76.04653 37.95359, -76.04169 ...\n\n\n50\n0400000US54\n54\nWest Virginia\n\n24038.210\nPOLYGON ((-81.96830 37.53780, -81.96540 37.541...\n\n\n51\n0400000US56\n56\nWyoming\n\n97093.141\nPOLYGON ((-109.05008 41.00066, -109.17368 41.0...\n\n\n\n\n\n\n\n\ndrop_states = ['15', '02', '72'] # HA, AK, PR\n\n\ngdf[gdf.STATE.isin(drop_states)].plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf[~gdf.STATE.isin(drop_states)].plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf = gdf[~gdf.STATE.isin(drop_states)]\n\n\ngdf.shape\n\n(49, 6)"
  },
  {
    "objectID": "lectures/week-04/2023-09-13.html#centroids-and-geometry",
    "href": "lectures/week-04/2023-09-13.html#centroids-and-geometry",
    "title": "GeoPandas",
    "section": "Centroids and geometry",
    "text": "Centroids and geometry\n\ngdf.centroid\n\n/tmp/ipykernel_2579345/2017122361.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  gdf.centroid\n\n\n0      POINT (-69.22532 45.36948)\n1      POINT (-71.79546 42.25229)\n2      POINT (-85.43751 44.35323)\n3     POINT (-109.64507 47.03355)\n4     POINT (-116.65540 39.35646)\n5      POINT (-74.66099 40.18393)\n6      POINT (-75.50198 42.93930)\n7      POINT (-79.35542 35.53980)\n8      POINT (-82.79018 40.29333)\n9      POINT (-77.79953 40.87382)\n10     POINT (-71.55250 41.67619)\n11     POINT (-86.34329 35.84299)\n12     POINT (-99.35528 31.49051)\n13    POINT (-111.67820 39.32379)\n14    POINT (-120.45017 47.38108)\n15     POINT (-90.01113 44.63829)\n17     POINT (-76.76446 39.03041)\n18     POINT (-86.82843 32.78969)\n20    POINT (-111.66458 34.29326)\n21     POINT (-92.43928 34.89974)\n22    POINT (-119.61077 37.24612)\n23    POINT (-105.54782 38.99855)\n24     POINT (-72.72576 41.62055)\n25     POINT (-75.50018 38.99178)\n26     POINT (-77.01630 38.90473)\n27     POINT (-82.50162 28.64096)\n28     POINT (-83.44606 32.64908)\n30    POINT (-114.65933 44.38912)\n31     POINT (-89.19828 40.06474)\n32     POINT (-86.27529 39.90853)\n33     POINT (-93.50004 42.07462)\n34     POINT (-98.38022 38.48470)\n35     POINT (-85.29049 37.52666)\n36     POINT (-91.98048 31.05264)\n37     POINT (-94.30870 46.31645)\n38     POINT (-89.66510 32.75040)\n39     POINT (-92.47742 38.36762)\n40     POINT (-99.81080 41.52715)\n41     POINT (-71.57760 43.68569)\n42    POINT (-106.10838 34.42137)\n43    POINT (-100.46931 47.44634)\n44     POINT (-97.50844 35.58350)\n45    POINT (-120.55529 43.93673)\n46     POINT (-80.89550 33.90710)\n47    POINT (-100.23049 44.43615)\n48     POINT (-72.66272 44.07518)\n49     POINT (-78.80562 37.51539)\n50     POINT (-80.61385 38.64251)\n51    POINT (-107.55145 42.99964)\ndtype: geometry\n\n\n\ngdf.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf.to_crs(gdf.estimate_utm_crs()).plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf = gdf.to_crs(gdf.estimate_utm_crs())\n\n\ngdf.centroid\n\n0      POINT (2359392.452 5305169.775)\n1      POINT (2253174.905 4901469.336)\n2      POINT (1108124.800 4935453.192)\n3      POINT (-765955.563 5348067.290)\n4     POINT (-1548246.254 4630302.336)\n5      POINT (2065285.016 4612318.097)\n6      POINT (1930049.196 4904992.641)\n7      POINT (1742974.165 4021487.098)\n8      POINT (1368634.294 4510266.503)\n9      POINT (1784134.621 4638257.315)\n10     POINT (2289313.142 4842131.086)\n11     POINT (1102080.849 3988935.477)\n12     POINT (-103892.519 3500028.226)\n13    POINT (-1115997.209 4520861.031)\n14    POINT (-1565541.918 5625943.989)\n15      POINT (738620.228 4945674.977)\n17     POINT (1910429.070 4447760.368)\n18     POINT (1078425.172 3643488.570)\n20    POINT (-1230212.721 3954699.221)\n21      POINT (550933.347 3861451.888)\n22    POINT (-1870826.275 4466915.330)\n23     POINT (-590190.342 4392344.096)\n24     POINT (2192677.610 4811150.640)\n25     POINT (2020359.310 4464239.060)\n26     POINT (1889584.643 4429716.358)\n27     POINT (1535090.511 3210901.406)\n28     POINT (1399285.825 3651528.582)\n30    POINT (-1223231.299 5144300.473)\n31      POINT (824539.285 4439350.669)\n32     POINT (1074696.801 4437810.568)\n33      POINT (458939.639 4658274.632)\n34       POINT (29865.949 4274404.492)\n35     POINT (1181577.693 4182456.487)\n36      POINT (598555.176 3435096.661)\n37      POINT (399444.786 5126179.145)\n38      POINT (812334.714 3626923.994)\n39      POINT (547395.056 4245527.933)\n40      POINT (-68074.613 4621136.163)\n41     POINT (2227578.113 5064712.956)\n42     POINT (-710483.211 3886170.809)\n43      POINT (-63437.281 5282398.408)\n44       POINT (91977.904 3947894.482)\n45    POINT (-1716081.976 5249621.330)\n46     POINT (1623336.735 3818285.055)\n47      POINT (-76107.494 4946849.414)\n48     POINT (2128860.270 5085004.659)\n49     POINT (1758189.886 4249101.361)\n50     POINT (1579306.753 4350654.685)\n51     POINT (-688676.799 4864507.482)\ndtype: geometry\n\n\n\ngdf.crs\n\n&lt;Projected CRS: EPSG:32615&gt;\nName: WGS 84 / UTM zone 15N\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 96°W and 90°W, northern hemisphere between equator and 84°N, onshore and offshore. Canada - Manitoba; Nunavut; Ontario. Ecuador -Galapagos. Guatemala. Mexico. United States (USA).\n- bounds: (-96.0, 0.0, -90.0, 84.0)\nCoordinate Operation:\n- name: UTM zone 15N\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ngdf['centroid'] = gdf.centroid\n\n\nbase = gdf.plot()\ngdf.centroid.plot(ax=base, color='r')\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "lectures/week-06/2023-09-25.html",
    "href": "lectures/week-06/2023-09-25.html",
    "title": "Table Join with remote data",
    "section": "",
    "text": "import pandas\nimport geopandas\n\n/tmp/ipykernel_3899390/1529612126.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas\ngdf = geopandas.read_file(\"./data/shared/covid/gz_2010_us_040_00_500k.json\")\n\nERROR 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\ngdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nNAME\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\ngdf.columns.values\n\narray(['GEO_ID', 'STATE', 'NAME', 'LSAD', 'CENSUSAREA', 'geometry'],\n      dtype=object)\ngdf.plot()\n\n&lt;AxesSubplot:&gt;\ngdf.rename(columns={'NAME': 'state'}, inplace=True)\ngdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\ntable = pandas.read_html('https://www.pewresearch.org/religion/religious-landscape-study/compare/party-affiliation/by/state/')\ntable\n\n[                   State Republican/lean Rep. No lean Democrat/lean Dem.  \\\n 0                Alabama                  52%     13%                35%   \n 1                 Alaska                  39%     29%                32%   \n 2                Arizona                  40%     21%                39%   \n 3               Arkansas                  46%     16%                38%   \n 4             California                  30%     21%                49%   \n 5               Colorado                  41%     17%                42%   \n 6            Connecticut                  32%     18%                50%   \n 7               Delaware                  29%     17%                55%   \n 8   District of Columbia                  11%     15%                73%   \n 9                Florida                  37%     19%                44%   \n 10               Georgia                  41%     18%                41%   \n 11                Hawaii                  28%     20%                51%   \n 12                 Idaho                  49%     19%                32%   \n 13              Illinois                  33%     19%                48%   \n 14               Indiana                  42%     20%                37%   \n 15                  Iowa                  41%     19%                40%   \n 16                Kansas                  46%     23%                31%   \n 17              Kentucky                  44%     13%                43%   \n 18             Louisiana                  41%     16%                43%   \n 19                 Maine                  36%     17%                47%   \n 20              Maryland                  31%     14%                55%   \n 21         Massachusetts                  27%     17%                56%   \n 22              Michigan                  34%     19%                47%   \n 23             Minnesota                  39%     15%                46%   \n 24           Mississippi                  44%     14%                42%   \n 25              Missouri                  41%     18%                42%   \n 26               Montana                  49%     21%                30%   \n 27              Nebraska                  47%     17%                36%   \n 28                Nevada                  37%     18%                46%   \n 29         New Hampshire                  35%     20%                44%   \n 30            New Jersey                  30%     19%                51%   \n 31            New Mexico                  37%     15%                48%   \n 32              New York                  28%     19%                53%   \n 33        North Carolina                  41%     17%                43%   \n 34          North Dakota                  50%     18%                33%   \n 35                  Ohio                  42%     18%                40%   \n 36              Oklahoma                  45%     15%                40%   \n 37                Oregon                  32%     21%                47%   \n 38          Pennsylvania                  39%     15%                46%   \n 39          Rhode Island                  30%     22%                48%   \n 40        South Carolina                  43%     18%                39%   \n 41          South Dakota                  53%     10%                37%   \n 42             Tennessee                  48%     15%                36%   \n 43                 Texas                  39%     21%                40%   \n 44                  Utah                  54%     16%                30%   \n 45               Vermont                  29%     14%                57%   \n 46              Virginia                  43%     18%                39%   \n 47            Washington                  33%     23%                44%   \n 48         West Virginia                  43%     16%                41%   \n 49             Wisconsin                  42%     16%                42%   \n 50               Wyoming                  57%     18%                25%   \n \n     Sample\\tsize  \n 0            511  \n 1            310  \n 2            653  \n 3            311  \n 4           3697  \n 5            504  \n 6            377  \n 7            301  \n 8            303  \n 9           2020  \n 10           968  \n 11           312  \n 12           320  \n 13          1326  \n 14           654  \n 15           330  \n 16           307  \n 17           439  \n 18           465  \n 19           303  \n 20           644  \n 21           704  \n 22           982  \n 23           563  \n 24           309  \n 25           642  \n 26           312  \n 27           312  \n 28           314  \n 29           303  \n 30           886  \n 31           312  \n 32          1966  \n 33          1022  \n 34           338  \n 35          1132  \n 36           391  \n 37           419  \n 38          1366  \n 39           305  \n 40           495  \n 41           305  \n 42           661  \n 43          2535  \n 44           315  \n 45           306  \n 46           882  \n 47           714  \n 48           309  \n 49           600  \n 50           316  ]\nlen(table)\n\n1\ndf = table[0]\ndf.head()\n\n\n\n\n\n\n\n\nState\nRepublican/lean Rep.\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\n\n\n\n\n0\nAlabama\n52%\n13%\n35%\n511\n\n\n1\nAlaska\n39%\n29%\n32%\n310\n\n\n2\nArizona\n40%\n21%\n39%\n653\n\n\n3\nArkansas\n46%\n16%\n38%\n311\n\n\n4\nCalifornia\n30%\n21%\n49%\n3697\ndf.columns\n\nIndex(['State', 'Republican/lean Rep.', 'No lean', 'Democrat/lean Dem.',\n       'Sample\\tsize'],\n      dtype='object')\ndf = df.rename(columns={df.columns[1]: 'Rep'})\ndf.head()\n\n\n\n\n\n\n\n\nState\nRep\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\n\n\n\n\n0\nAlabama\n52%\n13%\n35%\n511\n\n\n1\nAlaska\n39%\n29%\n32%\n310\n\n\n2\nArizona\n40%\n21%\n39%\n653\n\n\n3\nArkansas\n46%\n16%\n38%\n311\n\n\n4\nCalifornia\n30%\n21%\n49%\n3697\ndf['rep_int']= df.Rep.str.replace(\"%\",\"\").astype(int)\ndf.head()\n\n\n\n\n\n\n\n\nState\nRep\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\nrep_int\n\n\n\n\n0\nAlabama\n52%\n13%\n35%\n511\n52\n\n\n1\nAlaska\n39%\n29%\n32%\n310\n39\n\n\n2\nArizona\n40%\n21%\n39%\n653\n40\n\n\n3\nArkansas\n46%\n16%\n38%\n311\n46\n\n\n4\nCalifornia\n30%\n21%\n49%\n3697\n30\ndf = df.rename(columns={'State':'state'})\ndf\n\n\n\n\n\n\n\n\nstate\nRep\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\nrep_int\n\n\n\n\n0\nAlabama\n52%\n13%\n35%\n511\n52\n\n\n1\nAlaska\n39%\n29%\n32%\n310\n39\n\n\n2\nArizona\n40%\n21%\n39%\n653\n40\n\n\n3\nArkansas\n46%\n16%\n38%\n311\n46\n\n\n4\nCalifornia\n30%\n21%\n49%\n3697\n30\n\n\n5\nColorado\n41%\n17%\n42%\n504\n41\n\n\n6\nConnecticut\n32%\n18%\n50%\n377\n32\n\n\n7\nDelaware\n29%\n17%\n55%\n301\n29\n\n\n8\nDistrict of Columbia\n11%\n15%\n73%\n303\n11\n\n\n9\nFlorida\n37%\n19%\n44%\n2020\n37\n\n\n10\nGeorgia\n41%\n18%\n41%\n968\n41\n\n\n11\nHawaii\n28%\n20%\n51%\n312\n28\n\n\n12\nIdaho\n49%\n19%\n32%\n320\n49\n\n\n13\nIllinois\n33%\n19%\n48%\n1326\n33\n\n\n14\nIndiana\n42%\n20%\n37%\n654\n42\n\n\n15\nIowa\n41%\n19%\n40%\n330\n41\n\n\n16\nKansas\n46%\n23%\n31%\n307\n46\n\n\n17\nKentucky\n44%\n13%\n43%\n439\n44\n\n\n18\nLouisiana\n41%\n16%\n43%\n465\n41\n\n\n19\nMaine\n36%\n17%\n47%\n303\n36\n\n\n20\nMaryland\n31%\n14%\n55%\n644\n31\n\n\n21\nMassachusetts\n27%\n17%\n56%\n704\n27\n\n\n22\nMichigan\n34%\n19%\n47%\n982\n34\n\n\n23\nMinnesota\n39%\n15%\n46%\n563\n39\n\n\n24\nMississippi\n44%\n14%\n42%\n309\n44\n\n\n25\nMissouri\n41%\n18%\n42%\n642\n41\n\n\n26\nMontana\n49%\n21%\n30%\n312\n49\n\n\n27\nNebraska\n47%\n17%\n36%\n312\n47\n\n\n28\nNevada\n37%\n18%\n46%\n314\n37\n\n\n29\nNew Hampshire\n35%\n20%\n44%\n303\n35\n\n\n30\nNew Jersey\n30%\n19%\n51%\n886\n30\n\n\n31\nNew Mexico\n37%\n15%\n48%\n312\n37\n\n\n32\nNew York\n28%\n19%\n53%\n1966\n28\n\n\n33\nNorth Carolina\n41%\n17%\n43%\n1022\n41\n\n\n34\nNorth Dakota\n50%\n18%\n33%\n338\n50\n\n\n35\nOhio\n42%\n18%\n40%\n1132\n42\n\n\n36\nOklahoma\n45%\n15%\n40%\n391\n45\n\n\n37\nOregon\n32%\n21%\n47%\n419\n32\n\n\n38\nPennsylvania\n39%\n15%\n46%\n1366\n39\n\n\n39\nRhode Island\n30%\n22%\n48%\n305\n30\n\n\n40\nSouth Carolina\n43%\n18%\n39%\n495\n43\n\n\n41\nSouth Dakota\n53%\n10%\n37%\n305\n53\n\n\n42\nTennessee\n48%\n15%\n36%\n661\n48\n\n\n43\nTexas\n39%\n21%\n40%\n2535\n39\n\n\n44\nUtah\n54%\n16%\n30%\n315\n54\n\n\n45\nVermont\n29%\n14%\n57%\n306\n29\n\n\n46\nVirginia\n43%\n18%\n39%\n882\n43\n\n\n47\nWashington\n33%\n23%\n44%\n714\n33\n\n\n48\nWest Virginia\n43%\n16%\n41%\n309\n43\n\n\n49\nWisconsin\n42%\n16%\n42%\n600\n42\n\n\n50\nWyoming\n57%\n18%\n25%\n316\n57\njoin_gdf = gdf.merge(df, on='state')\njoin_gdf.head()\n\n\n\n\n\n\n\n\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nRep\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\nrep_int\n\n\n\n\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((-67.61976 44.51975, -67.61541 ...\n36%\n17%\n47%\n303\n36\n\n\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((-70.83204 41.60650, -70.82373 ...\n27%\n17%\n56%\n704\n27\n\n\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((-88.68443 48.11579, -88.67563 ...\n34%\n19%\n47%\n982\n34\n\n\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-104.05770 44.99743, -104.25015 44.9...\n49%\n21%\n30%\n312\n49\n\n\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-114.05060 37.00040, -114.04999 36.9...\n37%\n18%\n46%\n314\n37\ndrop = ['Alaska', 'Hawaii']\njoin_gdf = join_gdf[~join_gdf['state'].isin(drop)]\njoin_gdf.reset_index(inplace=True)\njoin_gdf.plot(column='rep_int', legend=True)\n\n&lt;AxesSubplot:&gt;\njoin_gdf = join_gdf.to_crs(join_gdf.estimate_utm_crs())\njoin_gdf.plot(column='rep_int', scheme='Quantiles', k=5, legend=True)\n\n&lt;AxesSubplot:&gt;\njoin_gdf.plot(column='rep_int', scheme='Quantiles', k=5, legend=True,\n             figsize=(16,9), legend_kwds={'loc': 'lower right'},\n             cmap='Reds')\n\n&lt;AxesSubplot:&gt;\njoin_gdf.plot(column='rep_int', scheme='Quantiles', k=5, legend=True,\n             figsize=(16,9), legend_kwds={'loc': 'lower right'},\n             edgecolor='lightgrey',\n             cmap='Reds')\n\n&lt;AxesSubplot:&gt;\nimport libpysal\n\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:39: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_dist(x, y):\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:165: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def get_faces(triangle):\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:199: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\n/opt/tljh/user/lib/python3.9/site-packages/libpysal/cg/alpha_shapes.py:261: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_mask_faces(mask, faces):\nw = libpysal.weights.Queen.from_dataframe(join_gdf)\nw.n\n\n49\nw.pct_nonzero\n\n9.079550187421907\nfrom splot.libpysal import plot_spatial_weights\nw.n\n\n49\nw.cardinalities\n\n{0: 1,\n 1: 5,\n 2: 3,\n 3: 4,\n 4: 5,\n 5: 3,\n 6: 5,\n 7: 4,\n 8: 5,\n 9: 6,\n 10: 2,\n 11: 8,\n 12: 4,\n 13: 6,\n 14: 2,\n 15: 4,\n 16: 5,\n 17: 4,\n 18: 5,\n 19: 6,\n 20: 3,\n 21: 7,\n 22: 3,\n 23: 3,\n 24: 2,\n 25: 2,\n 26: 5,\n 27: 6,\n 28: 5,\n 29: 4,\n 30: 6,\n 31: 4,\n 32: 7,\n 33: 3,\n 34: 4,\n 35: 4,\n 36: 8,\n 37: 6,\n 38: 3,\n 39: 5,\n 40: 3,\n 41: 6,\n 42: 4,\n 43: 2,\n 44: 6,\n 45: 3,\n 46: 6,\n 47: 5,\n 48: 6}\nw.histogram\n\n[(1, 1), (2, 5), (3, 9), (4, 10), (5, 10), (6, 10), (7, 2), (8, 2)]\ns = pandas.Series(w.cardinalities)\ns.plot.hist(bins=s.unique().shape[0]);\n_ = plot_spatial_weights(w,join_gdf, figsize=(16,9))\njoin_gdf['queen_neighbors'] = w.cardinalities\njoin_gdf.plot(column='queen_neighbors', legend=True, figsize=(16,9))\n\n&lt;AxesSubplot:&gt;\njoin_gdf.head()\n\n\n\n\n\n\n\n\nindex\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nRep\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\nrep_int\nqueen_neighbors\n\n\n\n\n0\n0\n0400000US23\n23\nMaine\n\n30842.923\nMULTIPOLYGON (((2516172.424 5253443.650, 25164...\n36%\n17%\n47%\n303\n36\n1\n\n\n1\n1\n0400000US25\n25\nMassachusetts\n\n7800.058\nMULTIPOLYGON (((2351724.597 4850457.653, 23526...\n27%\n17%\n56%\n704\n27\n5\n\n\n2\n2\n0400000US26\n26\nMichigan\n\n56538.901\nMULTIPOLYGON (((821167.988 5338182.388, 821794...\n34%\n19%\n47%\n982\n34\n3\n\n\n3\n3\n0400000US30\n30\nMontana\n\n145545.801\nPOLYGON ((-371533.418 5042503.702, -386687.255...\n49%\n21%\n30%\n312\n49\n4\n\n\n4\n4\n0400000US32\n32\nNevada\n\n109781.180\nPOLYGON ((-1384104.733 4308747.816, -1385163.9...\n37%\n18%\n46%\n314\n37\n5\njoin_gdf[join_gdf.state=='California']\n\n\n\n\n\n\n\n\nindex\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nRep\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\nrep_int\nqueen_neighbors\n\n\n\n\n20\n21\n0400000US06\n06\nCalifornia\n\n155779.22\nMULTIPOLYGON (((-2114916.187 4624534.270, -211...\n30%\n21%\n49%\n3697\n30\n3\njoin_gdf[join_gdf.index.isin(w.neighbors[20])].state\n\n4      Nevada\n18    Arizona\n42     Oregon\nName: state, dtype: object\njoin_gdf[join_gdf.index.isin(w.neighbors[18])].state\n\n4         Nevada\n13          Utah\n20    California\n21      Colorado\n39    New Mexico\nName: state, dtype: object"
  },
  {
    "objectID": "lectures/week-06/2023-09-25.html#rook",
    "href": "lectures/week-06/2023-09-25.html#rook",
    "title": "Table Join with remote data",
    "section": "Rook",
    "text": "Rook\n\nwrook = libpysal.weights.Rook.from_dataframe(join_gdf)\n\n\nwrook.n\n\n49\n\n\n\nwrook.pct_nonzero\n\n8.912952936276552\n\n\n\n_ = plot_spatial_weights(wrook,join_gdf, figsize=(16,9))\n\n\n\n\n\njoin_gdf[join_gdf.index.isin(wrook.neighbors[18])].state\n\n4         Nevada\n13          Utah\n20    California\n39    New Mexico\nName: state, dtype: object\n\n\n\nwrook.histogram\n\n[(1, 1), (2, 5), (3, 9), (4, 12), (5, 9), (6, 10), (7, 1), (8, 2)]\n\n\n\ns = pandas.Series(wrook.cardinalities)\ns.plot.hist(bins=s.unique().shape[0]);"
  },
  {
    "objectID": "lectures/week-06/2023-09-25.html#distance-based-weights",
    "href": "lectures/week-06/2023-09-25.html#distance-based-weights",
    "title": "Table Join with remote data",
    "section": "Distance Based Weights",
    "text": "Distance Based Weights\n\nK-Nearest Neighbor\n\nwk4 = libpysal.weights.distance.KNN.from_dataframe(join_gdf, k=4)\n\n\nwk4.histogram\n\n[(4, 49)]\n\n\n\n_ = plot_spatial_weights(wk4,join_gdf, figsize=(16,9))\n\n\n\n\n\nwk4.pct_nonzero\n\n8.16326530612245\n\n\n\n\nDistance Bands\n\nw_bdb = libpysal.weights.distance.DistanceBand.from_dataframe(join_gdf, 1000000, binary=True) # 1 million meters = 621.37 miles\n\n\nw_bdb.histogram\n\n[(4, 1),\n (5, 2),\n (6, 1),\n (7, 3),\n (8, 4),\n (9, 1),\n (10, 4),\n (11, 3),\n (12, 1),\n (13, 4),\n (14, 3),\n (15, 5),\n (16, 2),\n (17, 2),\n (18, 3),\n (19, 1),\n (20, 1),\n (21, 3),\n (22, 4),\n (23, 0),\n (24, 1)]\n\n\n\n_ = plot_spatial_weights(w_bdb,join_gdf, figsize=(16,9))"
  },
  {
    "objectID": "lectures/week-06/2023-09-25.html#uses-of-w",
    "href": "lectures/week-06/2023-09-25.html#uses-of-w",
    "title": "Table Join with remote data",
    "section": "Uses of W",
    "text": "Uses of W\n\nFind largest difference between neighbors\n\nw_adjlist = w.to_adjlist()\n\n\nw_adjlist.head()\n\n\n\n\n\n\n\n\nfocal\nneighbor\nweight\n\n\n\n\n0\n0\n38\n1.0\n\n\n1\n1\n6\n1.0\n\n\n2\n1\n10\n1.0\n\n\n3\n1\n22\n1.0\n\n\n4\n1\n38\n1.0\n\n\n\n\n\n\n\n\ny = join_gdf.rep_int.values\n\n\ny\n\narray([36, 27, 34, 49, 37, 30, 28, 41, 42, 39, 30, 48, 39, 54, 33, 42, 31,\n       52, 40, 46, 30, 41, 32, 29, 11, 37, 41, 49, 33, 42, 41, 46, 44, 41,\n       39, 44, 41, 47, 35, 37, 50, 45, 32, 43, 53, 29, 43, 43, 57])\n\n\n\nimport numpy as np\nw_adjlist['pairdiff'] = np.abs(y[w_adjlist.focal] - y[w_adjlist.neighbor])\n\n\nw_adjlist\n\n\n\n\n\n\n\n\nfocal\nneighbor\nweight\npairdiff\n\n\n\n\n0\n0\n38\n1.0\n1\n\n\n1\n1\n6\n1.0\n1\n\n\n2\n1\n10\n1.0\n3\n\n\n3\n1\n22\n1.0\n5\n\n\n4\n1\n38\n1.0\n8\n\n\n...\n...\n...\n...\n...\n\n\n213\n48\n13\n1.0\n3\n\n\n214\n48\n21\n1.0\n16\n\n\n215\n48\n27\n1.0\n8\n\n\n216\n48\n37\n1.0\n10\n\n\n217\n48\n44\n1.0\n4\n\n\n\n\n218 rows × 4 columns\n\n\n\n\nw_adjlist.pairdiff.max()\n\n32\n\n\n\nw_adjlist[['pairdiff']].idxmax()\n\npairdiff    104\ndtype: int64\n\n\n\nw_adjlist.iloc[w_adjlist[['pairdiff']].idxmax()]\n\n\n\n\n\n\n\n\nfocal\nneighbor\nweight\npairdiff\n\n\n\n\n104\n24\n46\n1.0\n32\n\n\n\n\n\n\n\n\njoin_gdf.iloc[[24, 46]]\n\n\n\n\n\n\n\n\nindex\nGEO_ID\nSTATE\nstate\nLSAD\nCENSUSAREA\ngeometry\nRep\nNo lean\nDemocrat/lean Dem.\nSample\\tsize\nrep_int\nqueen_neighbors\n\n\n\n\n24\n25\n0400000US11\n11\nDistrict of Columbia\n\n61.048\nPOLYGON ((1889905.340 4416707.434, 1889692.665...\n11%\n15%\n73%\n303\n11\n2\n\n\n46\n48\n0400000US51\n51\nVirginia\n\n39490.086\nMULTIPOLYGON (((1994538.922 4338939.700, 19949...\n43%\n18%\n39%\n882\n43\n6\n\n\n\n\n\n\n\n\n\nSpatial Lag\n\nfrom libpysal.weights.spatial_lag import lag_spatial\n\n\nw.transform = 'r'\n\n\nlag_spatial?\n\n\nSignature: lag_spatial(w, y)\nDocstring:\nSpatial lag operator.\nIf w is row standardized, returns the average of each observation's neighbors;\nif not, returns the weighted sum of each observation's neighbors.\nParameters\n----------\nw                   : W\n                      libpysal spatial weightsobject\ny                   : array\n                      numpy array with dimensionality conforming to w (see examples)\nReturns\n-------\nwy                  : array\n                      array of numeric values for the spatial lag\nExamples\n--------\nSetup a 9x9 binary spatial weights matrix and vector of data; compute the\nspatial lag of the vector.\n&gt;&gt;&gt; import libpysal\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; w = libpysal.weights.lat2W(3, 3)\n&gt;&gt;&gt; y = np.arange(9)\n&gt;&gt;&gt; yl = libpysal.weights.lag_spatial(w, y)\n&gt;&gt;&gt; yl\narray([ 4.,  6.,  6., 10., 16., 14., 10., 18., 12.])\nRow standardize the weights matrix and recompute the spatial lag\n&gt;&gt;&gt; w.transform = 'r'\n&gt;&gt;&gt; yl = libpysal.weights.lag_spatial(w, y)\n&gt;&gt;&gt; yl\narray([2.        , 2.        , 3.        , 3.33333333, 4.        ,\n       4.66666667, 5.        , 6.        , 6.        ])\nExplicitly define data vector as 9x1 and recompute the spatial lag\n&gt;&gt;&gt; y.shape = (9, 1)\n&gt;&gt;&gt; yl = libpysal.weights.lag_spatial(w, y)\n&gt;&gt;&gt; yl\narray([[2.        ],\n       [2.        ],\n       [3.        ],\n       [3.33333333],\n       [4.        ],\n       [4.66666667],\n       [5.        ],\n       [6.        ],\n       [6.        ]])\nTake the spatial lag of a 9x2 data matrix\n&gt;&gt;&gt; yr = np.arange(8, -1, -1)\n&gt;&gt;&gt; yr.shape = (9, 1)\n&gt;&gt;&gt; x = np.hstack((y, yr))\n&gt;&gt;&gt; yl = libpysal.weights.lag_spatial(w, x)\n&gt;&gt;&gt; yl\narray([[2.        , 6.        ],\n       [2.        , 6.        ],\n       [3.        , 5.        ],\n       [3.33333333, 4.66666667],\n       [4.        , 4.        ],\n       [4.66666667, 3.33333333],\n       [5.        , 3.        ],\n       [6.        , 2.        ],\n       [6.        , 2.        ]])\nFile:      /opt/tljh/user/lib/python3.9/site-packages/libpysal/weights/spatial_lag.py\nType:      function\n\n\n\n\nlag_rep_int = lag_spatial(w, y)\n\n\nlag_rep_int\n\narray([35.        , 30.8       , 42.        , 52.25      , 41.        ,\n       32.        , 31.4       , 43.75      , 40.4       , 33.83333333,\n       29.5       , 44.        , 42.25      , 43.5       , 40.5       ,\n       36.75      , 33.        , 42.5       , 39.8       , 43.        ,\n       36.33333333, 46.57142857, 28.33333333, 33.33333333, 37.        ,\n       46.5       , 44.2       , 43.66666667, 42.        , 38.25      ,\n       42.5       , 43.5       , 41.71428571, 43.        , 46.5       ,\n       46.75      , 43.75      , 46.5       , 30.66666667, 43.8       ,\n       47.        , 41.66666667, 37.25      , 41.        , 47.16666667,\n       30.        , 36.33333333, 39.8       , 48.83333333])\n\n\n\njoin_gdf['lag_rep_int'] = lag_rep_int\n\n\nimport matplotlib.pyplot as plt\n\n\nf, axs = plt.subplots(1, 2, figsize=(16, 9))\nax1, ax2 = axs\n\ngdf = join_gdf\n\ngdf.plot(column='rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican\")\n\n\ngdf.plot(column='lag_rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\nax2.set_title(\"Spatial Lag Leaning Republican\")\n\nText(0.5, 1.0, 'Spatial Lag Leaning Republican')\n\n\n\n\n\n\ngdf.plot.scatter(x='rep_int', y='lag_rep_int')\n\n&lt;AxesSubplot:xlabel='rep_int', ylabel='lag_rep_int'&gt;\n\n\n\n\n\n\nimport seaborn as sns\n_ = sns.regplot(x='rep_int', y='lag_rep_int', data=gdf)\n\n\n\n\n\nimport seaborn as sns\n_ = sns.regplot(x='rep_int', y='lag_rep_int', data=gdf)\nplt.axhline(y=gdf.lag_rep_int.mean(), color='g', linestyle='--')\nplt.axvline(x=gdf.rep_int.mean(), color='g', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7fc5d2669af0&gt;\n\n\n\n\n\n\nf, axs = plt.subplots(1, 2, figsize=(16, 9))\nax1, ax2 = axs\n\ngdf = join_gdf\n\ngdf.plot(column='rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican\")\n\n\ngdf.plot(column='lag_rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\n_= ax2.set_title(\"Spatial Lag Leaning Republican\")\n\n\n\n\n\ngdf.explore(column='rep_int', cmap='Reds')\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ngdf.to_parquet('repub_lean.parquet')"
  },
  {
    "objectID": "lectures/week-12/interpolation_deterministic.html",
    "href": "lectures/week-12/interpolation_deterministic.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (16, 9)\nimport rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nprecip = gpd.read_file(\"precip_sd.geojson\")\nprecip.plot(column='inches', legend=True);\ncounty = gpd.read_file(\"sdcounty.geojson\")\nm = county.explore()\nprecip.explore(column='inches', m=m)"
  },
  {
    "objectID": "lectures/week-12/interpolation_deterministic.html#interpolation-methods",
    "href": "lectures/week-12/interpolation_deterministic.html#interpolation-methods",
    "title": "Spatial Interpolation",
    "section": "Interpolation Methods",
    "text": "Interpolation Methods\n\nVoronoi Partition Interpolation\n\nfrom libpysal.cg import voronoi_frames\npoints = [(10.2, 5.1), (4.7, 2.2), (5.3, 5.7), (2.7, 5.3)]\nregions_df, points_df = voronoi_frames(points)\nregions_df.shape\n(4, 1)\n\n\nregions_df.plot()\n\n\n#libpysal.cg.voronoi_frames(points, radius=None, clip='extent')\npoints = precip.get_coordinates().values\n\n\npoints\n\n\nv_gdf, v_p = voronoi_frames(points)\n\nv_gdf.plot()\n\n\nv_gdf, v_p = voronoi_frames(points, clip=county.geometry[0])\n\nv_gdf.plot()\n\n\nv_gdf['inches'] = precip.inches\n\n\nv_gdf.plot(column='inches', legend=True)\n\n\nbase = v_gdf.plot(column='inches', legend=True,\n                 edgecolor='gray')\nprecip.plot(ax=base, color='w');\n\n\n\nInterpolate to Grids\n\nhex3 the county\nnearest neighbor interpolators\n\n\n\nimport tobler\n\n\ncounty_utm = county.to_crs(county.estimate_utm_crs())\nprecip_utm = precip.to_crs(precip.estimate_utm_crs())\n\n\nfrom tobler.util import h3fy\n\n\ncounty_h3 = h3fy(county_utm)\n\n\ncounty_h3.plot()\n\n\nm = county_h3.plot(color='grey')\nprecip_utm.plot(column='inches', ax=m, legend=True);\n\n\n\nNearest neighbor\n\nhcents = county_h3.centroid\n\n\nm = hcents.plot(color='r')\nprecip_utm.plot(column='inches', ax=m, legend=True);\n\n\ngpd.sjoin_nearest(county_h3, precip_utm, distance_col=\"distances\",\n    lsuffix=\"left\", rsuffix=\"right\", exclusive=True)\n\n\ncounty_h3.shape\n\n\ncounty_h3['nn1_est'] = gpd.sjoin_nearest(county_h3, precip_utm, distance_col=\"distances\",\n    lsuffix=\"left\", rsuffix=\"right\", exclusive=True).inches\n\n\ncounty_h3.plot(column='nn1_est', legend=True);\n\n\n\nKnn5\n\nX = [[0], [1], [2], [3]]\ny = [0, 0, 1, 1]\nfrom sklearn.neighbors import KNeighborsRegressor\nneigh = KNeighborsRegressor(n_neighbors=2)\nneigh.fit(X, y)\nprint(neigh.predict([[1.5]]))\n\n\n\n# Set number of neighbors to use\nneighbors = 5\n\n# Initialize KNN regressor\nknn_regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"uniform\") # no distance decay distance\n\n# Fit to observed locations\nknn_regressor.fit(precip_utm.get_coordinates(), precip_utm.inches)\n\n\nknn_regressor.predict(hcents.get_coordinates())\n\n\ncounty_h3['nn5_est'] = knn_regressor.predict(hcents.get_coordinates())\n\n\ncounty_h3.plot(column='nn5_est', legend=True);\n\n\nimport matplotlib.pyplot as plt\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\ncounty_h3.plot(column='nn1_est',ax=ax1)\ncounty_h3.plot(column='nn5_est',ax=ax2)\nax1.set_title(\"knn=1\")\nax2.set_title(\"knn=5\");\n\n\n\n# Set number of neighbors to use\nneighbors = 5\n\n# Initialize KNN regressor\nknn_regressor = KNeighborsRegressor(n_neighbors = neighbors, weights = \"distance\") # inverse distance weighting\n# Fit to observed locations\nknn_regressor.fit(precip_utm.get_coordinates(), precip_utm.inches)\n\n\ncounty_h3['nn5id_est'] = knn_regressor.predict(hcents.get_coordinates())\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\ncounty_h3.plot(column='nn5_est',ax=ax1)\ncounty_h3.plot(column='nn5id_est',ax=ax2)\nax1.set_title(\"knn=5 (Unweighted)\")\nax2.set_title(\"knn=5 (Inverse Distance Weights)\");"
  },
  {
    "objectID": "lectures/week-12/interpolation_deterministic.html#next",
    "href": "lectures/week-12/interpolation_deterministic.html#next",
    "title": "Spatial Interpolation",
    "section": "Next",
    "text": "Next\n\ntracts = gdf.dissolve(by='TRACTCE20')\n\ntracts.shape\n\n\nSurface to Area Interpolation\n\nSpatial Join on Centroid\n\ncents = tracts.centroid\n\n\ncents.plot()\n\n\ntype(cents)\n\n\ncoord_list = [(x, y) for x, y in zip(cents.x, cents.y)]\ntracts['centest'] = [x[0] for x in clipped.sample(coord_list)]\ntracts.head()\n\n\ntracts['centroid'] = tracts.centroid\ntracts.set_geometry('centroid', inplace=True)\n\n\ntracts.plot(column='centest', legend=True);\n\n\ntracts.set_geometry('geometry', inplace=True)\ntracts.plot(column='centest', legend=True);\n\n\n\nZonal Methods of Surface to Area Interpolation\n\nimport rasterstats\n\n\ngdf.head()\n\n\ntracts.plot()\n\n\nfrom rasterstats import zonal_stats\ntstats = zonal_stats(tracts, \"clipped_example.tif\",\n            stats=\"count min mean max median\")\n\n#elevations2 = zonal_stats(\n#    sd_tracts.to_crs(dem.rio.crs),  # Geotable with zones\n#    \"../data/nasadem/nasadem_sd.tif\",  # Path to surface file\n#)\n#elevations2 = pandas.DataFrame(elevations2)\n\n\ntstats[:5]\n\n\ntstats = pd.DataFrame(tstats)\n\n\ntstats.head()\n\n\ntstats.shape\n\n\ntracts.shape\n\n\ntracts['mean'] = tstats['mean'].values\ntracts.plot(column='mean', legend=True);\n\n\ntracts['median'] = tstats['median'].values\ntracts.plot(column='median', legend=True);\n\n\ntracts['range'] = tstats['max'].values - tstats['min'].values\ntracts.plot(column='range', legend=True);\n\n\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(data=tracts, x='centest', y='mean')\nplt.plot([10, 40], [10, 40]);\n\n\nsns.scatterplot(data=tracts, x='median', y='mean')\nplt.plot([10, 40], [10, 40]);"
  },
  {
    "objectID": "lectures/week-12/interp.html",
    "href": "lectures/week-12/interp.html",
    "title": "Geog385F23",
    "section": "",
    "text": "import rasterio\n\n\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\n\n\nras = 'data/stanford-td754wr4701-geotiff.tiff' # already interpolated but we will treat it as \"data\"\n\n\nshp = 'data/tl_2022_06073_faces.shp'\n\n\ngdf = gpd.read_file(shp)\n\n\ngdf.shape\n\n\ngdf.head()\n\n\ncounty = gdf.dissolve(by='COUNTYFP20')\n\n\ncounty.plot()\n\n\nrast = rasterio.open(ras)\n\n\ncounty = county.to_crs(rast.crs)\n\n\ncounty.plot()\n\n\nrast.crs\n\n\ncoords = gdf.geometry\nsrc = rast\ndf = county\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n\nclipped_array, clipped_transform = mask(dataset=src, shapes=coords, crop=True)\n\ndf = df.to_crs(src.crs)\nout_meta = src.meta.copy()\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": clipped_array.shape[1],\n                 \"width\": clipped_array.shape[2],\n                 \"transform\": clipped_transform})\nout_tif= \"clipped_example.tif\"\nwith rasterio.open(out_tif, \"w\", **out_meta) as dest:\n    dest.write(clipped_array)\n    \nclipped = rasterio.open(out_tif)\nfig, ax = plt.subplots(figsize=(8, 6))\np1 = df.plot(color=None,facecolor='none',edgecolor='red',linewidth = 2,ax=ax)\nshow(clipped, ax=ax)\nax.axis('off');\n\n\nclipped\n\n\nimport rioxarray\n\n\nd = rioxarray.open_rasterio(\"clipped_example.tif\")\n\n\nd\n\n\nd.plot()\n\n\nd.values.max()\n\n\nd.plot()\n\n\ntype(d)\n\n\nd.dims\n\n\nd.values.mean()\n\n\nimport numpy\n\n\nnumpy.median(d.values)\n\n\nd.values.shape\n\n\nd.plot()\n\n\nd.plot.hist()\n\n\ntype(d)\n\n\nimport rasterstats\n\n\ngdf.head()\n\n\ntracts = gdf.dissolve(by='TRACTCE20')\n\n\ntracts.shape\n\n\ntracts.plot()\n\n\nfrom rasterstats import zonal_stats\ntstats = zonal_stats(tracts, \"clipped_example.tif\",\n            stats=\"count min mean max median\")\n\n#elevations2 = zonal_stats(\n#    sd_tracts.to_crs(dem.rio.crs),  # Geotable with zones\n#    \"../data/nasadem/nasadem_sd.tif\",  # Path to surface file\n#)\n#elevations2 = pandas.DataFrame(elevations2)\n\n\ntstats[:5]\n\n\ntstats = pd.DataFrame(tstats)\n\n\ntstats.head()\n\n\ntstats.shape\n\n\ntracts.shape\n\n\ntracts['mean'] = tstats['mean'].values\ntracts.plot(column='mean', legend=True);\n\n\ntracts['median'] = tstats['median'].values\ntracts.plot(column='median', legend=True);\n\n\ntracts['range'] = tstats['max'].values - tstats['min'].values\ntracts.plot(column='range', legend=True);"
  },
  {
    "objectID": "lectures/week-12/interpolation_kriging.html",
    "href": "lectures/week-12/interpolation_kriging.html",
    "title": "Spatial Interpolation: Kriging",
    "section": "",
    "text": "non-deterministic interpolation\nmeasures of uncertainty\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (16, 9)\nimport rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nimport pykrige\nprecip = gpd.read_file(\"precip_sd.geojson\")\nprecip.plot(column='inches', legend=True);\ncounty = gpd.read_file(\"sdcounty.geojson\")\nm = county.explore()\nprecip.explore(column='inches', m=m)"
  },
  {
    "objectID": "lectures/week-12/interpolation_kriging.html#fit",
    "href": "lectures/week-12/interpolation_kriging.html#fit",
    "title": "Spatial Interpolation: Kriging",
    "section": "Fit",
    "text": "Fit\n\nh3_cents = county_h3.centroid\nimport rioxarray\nimport rasterio\n\n\n# get observed values for all grid cells\n\n\nclipped = rasterio.open(\"clipped_example.tif\")\n\n\nclipped.meta\n\n\nh3_cents_4326 = h3_cents.to_crs(clipped.meta['crs'])\ncp = h3_cents_4326\ncoord_list = [(x, y) for x, y in zip(cp.x, cp.y)]\n\n\nobservations = [x[0] for x in clipped.sample(coord_list)]\ncounty_h3['inches'] = observations\n\n\ncounty_h3.plot(column='inches', legend=True)\n\n\n# calculate fit for each approach (MAPE)\ndef mape(est, obs):\n    err = est-obs\n    aerr = numpy.abs(err)\n    den = obs + (obs == 0)\n    paerr = aerr / den\n    paerr *= 100\n    return paerr.mean()\n\n\nmape(county_h3.nn1_est, county_h3.inches)\n\n\nmape(county_h3.nn5_est, county_h3.inches)\n\n\nmape(county_h3.nn5id_est, county_h3.inches)\n\n\nmape(county_h3.ok_est, county_h3.inches)\n\n\n# plot fit\n\n\n# map errors for different models\n\n\nen1 = county_h3.nn1_est - county_h3.inches\n\ncounty_h3['nn1_error'] = en1\n\ncounty_h3.plot(column='nn1_error', legend=True, cmap='coolwarm')\n\n\nen5id = county_h3.nn5id_est - county_h3.inches\n\ncounty_h3['nn5id_error'] = en5id\n\ncounty_h3.plot(column='nn5id_error', legend=True, cmap='coolwarm')\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(x=county_h3.inches, y=county_h3.nn5id_error);\n\nWIP Below Here\n\nSurface to Area Interpolation\n\nSpatial Join on Centroid\n\ncents = tracts.centroid\n\n\ncents.plot()\n\n\ntype(cents)\n\n\ncoord_list = [(x, y) for x, y in zip(cents.x, cents.y)]\ntracts['centest'] = [x[0] for x in clipped.sample(coord_list)]\ntracts.head()\n\n\ntracts['centroid'] = tracts.centroid\ntracts.set_geometry('centroid', inplace=True)\n\n\ntracts.plot(column='centest', legend=True);\n\n\ntracts.set_geometry('geometry', inplace=True)\ntracts.plot(column='centest', legend=True);\n\n\n\nZonal Methods of Surface to Area Interpolation\n\nimport rasterstats\n\n\ngdf.head()\n\n\ntracts.plot()\n\n\nfrom rasterstats import zonal_stats\ntstats = zonal_stats(tracts, \"clipped_example.tif\",\n            stats=\"count min mean max median\")\n\n#elevations2 = zonal_stats(\n#    sd_tracts.to_crs(dem.rio.crs),  # Geotable with zones\n#    \"../data/nasadem/nasadem_sd.tif\",  # Path to surface file\n#)\n#elevations2 = pandas.DataFrame(elevations2)\n\n\ntstats[:5]\n\n\ntstats = pd.DataFrame(tstats)\n\n\ntstats.head()\n\n\ntstats.shape\n\n\ntracts.shape\n\n\ntracts['mean'] = tstats['mean'].values\ntracts.plot(column='mean', legend=True);\n\n\ntracts['median'] = tstats['median'].values\ntracts.plot(column='median', legend=True);\n\n\ntracts['range'] = tstats['max'].values - tstats['min'].values\ntracts.plot(column='range', legend=True);\n\n\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(data=tracts, x='centest', y='mean')\nplt.plot([10, 40], [10, 40]);\n\n\nsns.scatterplot(data=tracts, x='median', y='mean')\nplt.plot([10, 40], [10, 40]);"
  },
  {
    "objectID": "lectures/week-12/geostat_data.html",
    "href": "lectures/week-12/geostat_data.html",
    "title": "Sampling the raster for “observations”",
    "section": "",
    "text": "import rasterio\nfrom rasterio.mask import mask\nimport geopandas as gpd\nimport fiona\nimport pandas as pd\nras = 'data/stanford-td754wr4701-geotiff.tiff' # already interpolated but we will treat it as \"data\"\nshp = 'data/tl_2022_06073_faces.shp'\ngdf = gpd.read_file(shp)\ngdf.shape\ngdf.head()\ncounty = gdf.dissolve(by='COUNTYFP20')\ncounty.plot()\nrast = rasterio.open(ras)\ncounty = county.to_crs(rast.crs)\ncounty.plot()\nrast.crs\ncoords = gdf.geometry\nsrc = rast\ndf = county\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nclipped_array, clipped_transform = mask(dataset=src, shapes=coords, crop=True)\n\ndf = df.to_crs(src.crs)\nout_meta = src.meta.copy()\nout_meta.update({\"driver\": \"GTiff\",\n                 \"height\": clipped_array.shape[1],\n                 \"width\": clipped_array.shape[2],\n                 \"transform\": clipped_transform})\nout_tif= \"clipped_example.tif\"\nwith rasterio.open(out_tif, \"w\", **out_meta) as dest:\n    dest.write(clipped_array)\n    \nclipped = rasterio.open(out_tif)\nfig, ax = plt.subplots(figsize=(8, 6))\np1 = df.plot(color=None,facecolor='none',edgecolor='red',linewidth = 2,ax=ax)\nshow(clipped, ax=ax)\nax.axis('off');\nclipped\nimport rioxarray\nd = rioxarray.open_rasterio(\"clipped_example.tif\")\nd\nd.plot()\nd.values.max()\nd.plot()\ntype(d)\nd.dims\nd.values.mean()\nimport numpy\nnumpy.median(d.values)\nd.values.shape\nd.plot()\nd.plot.hist()\ntype(d)\nimport numpy\nnumpy.random.seed(12345)\nsample_points = county.sample_points(50)\nm = county.explore()\nsample_points.explore(m=m, color='red')"
  },
  {
    "objectID": "lectures/week-12/geostat_data.html#interpolation-methods",
    "href": "lectures/week-12/geostat_data.html#interpolation-methods",
    "title": "Sampling the raster for “observations”",
    "section": "Interpolation Methods",
    "text": "Interpolation Methods\n\ntracts = gdf.dissolve(by='TRACTCE20')\n\ntracts.shape\n\n\nSurface to Area Interpolation\n\nSpatial Join on Centroid\n\ncents = tracts.centroid\n\n\ncents.plot()\n\n\ntype(cents)\n\n\ncoord_list = [(x, y) for x, y in zip(cents.x, cents.y)]\ntracts['centest'] = [x[0] for x in clipped.sample(coord_list)]\ntracts.head()\n\n\ntracts['centroid'] = tracts.centroid\ntracts.set_geometry('centroid', inplace=True)\n\n\ntracts.plot(column='centest', legend=True);\n\n\ntracts.set_geometry('geometry', inplace=True)\ntracts.plot(column='centest', legend=True);\n\n\n\nZonal Methods of Surface to Area Interpolation\n\nimport rasterstats\n\n\ngdf.head()\n\n\ntracts.plot()\n\n\nfrom rasterstats import zonal_stats\ntstats = zonal_stats(tracts, \"clipped_example.tif\",\n            stats=\"count min mean max median\")\n\n#elevations2 = zonal_stats(\n#    sd_tracts.to_crs(dem.rio.crs),  # Geotable with zones\n#    \"../data/nasadem/nasadem_sd.tif\",  # Path to surface file\n#)\n#elevations2 = pandas.DataFrame(elevations2)\n\n\ntstats[:5]\n\n\ntstats = pd.DataFrame(tstats)\n\n\ntstats.head()\n\n\ntstats.shape\n\n\ntracts.shape\n\n\ntracts['mean'] = tstats['mean'].values\ntracts.plot(column='mean', legend=True);\n\n\ntracts['median'] = tstats['median'].values\ntracts.plot(column='median', legend=True);\n\n\ntracts['range'] = tstats['max'].values - tstats['min'].values\ntracts.plot(column='range', legend=True);\n\n\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n\nsns.scatterplot(data=tracts, x='centest', y='mean')\nplt.plot([10, 40], [10, 40]);\n\n\nsns.scatterplot(data=tracts, x='median', y='mean')\nplt.plot([10, 40], [10, 40]);"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html",
    "href": "lectures/week-10/2023-10-23-centrography.html",
    "title": "Centrography for Point Patterns",
    "section": "",
    "text": "Centrography refers to a set of descriptive statistics that provide summary descriptions of point patterns.\nThis notebook introduces three types of centrography analysis for point patterns in pysal.\n\nCentral Tendency\nDispersion and Orientation\nShape Analysis\n\nWe also illustrate centrography analysis using two simulated datasets. See Another Example\n\nCentral Tendency\n\nmean_center: calculate the mean center of the unmarked point pattern.\nweighted_mean_center: calculate the weighted mean center of the marked point pattern.\nmanhattan_median: calculate the manhattan median\neuclidean_median: calculate the Euclidean median\n\nDispersion and Orientation\n\nstd_distance: calculate the standard distance\nstandard deviational ellipse\n\nShape Analysis\n\nhull: calculate the convex hull of the point pattern\nmbr: calculate the minimum bounding box (rectangle)\n\n\nAll of the above functions operate on a series of coordinate pairs. That is, the data type of the first argument should be \\((n,2)\\) array_like. In case that you have a point pattern (PointPattern instance), you need to pass its attribute “points” instead of itself to these functions.\n\nimport numpy as np\nfrom pointpats import PointPattern\n%matplotlib inline\nimport matplotlib.pyplot as plt\npoints = [[66.22, 32.54], [22.52, 22.39], [31.01, 81.21],\n          [9.47, 31.02],  [30.78, 60.10], [75.21, 58.93],\n          [79.26,  7.68], [8.23, 39.93],  [98.73, 77.17],\n          [89.78, 42.53], [65.19, 92.08], [54.46, 8.48]]\npp = PointPattern(points) #create a point pattern \"pp\" from list\npp.points \n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n66.22\n32.54\n\n\n1\n22.52\n22.39\n\n\n2\n31.01\n81.21\n\n\n3\n9.47\n31.02\n\n\n4\n30.78\n60.10\n\n\n5\n75.21\n58.93\n\n\n6\n79.26\n7.68\n\n\n7\n8.23\n39.93\n\n\n8\n98.73\n77.17\n\n\n9\n89.78\n42.53\n\n\n10\n65.19\n92.08\n\n\n11\n54.46\n8.48\n\n\n\n\n\n\n\n\ntype(pp.points)\n\npandas.core.frame.DataFrame\n\n\nWe can use PointPattern class method plot to visualize pp.\n\npp.plot()\n\n\n\n\n\nfrom pointpats.centrography import (hull, mbr, mean_center,\n                                    weighted_mean_center, manhattan_median,\n                                    std_distance,euclidean_median,ellipse)"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#introduction",
    "href": "lectures/week-10/2023-10-23-centrography.html#introduction",
    "title": "Centrography for Point Patterns",
    "section": "",
    "text": "Centrography refers to a set of descriptive statistics that provide summary descriptions of point patterns.\nThis notebook introduces three types of centrography analysis for point patterns in pysal.\n\nCentral Tendency\nDispersion and Orientation\nShape Analysis\n\nWe also illustrate centrography analysis using two simulated datasets. See Another Example\n\nCentral Tendency\n\nmean_center: calculate the mean center of the unmarked point pattern.\nweighted_mean_center: calculate the weighted mean center of the marked point pattern.\nmanhattan_median: calculate the manhattan median\neuclidean_median: calculate the Euclidean median\n\nDispersion and Orientation\n\nstd_distance: calculate the standard distance\nstandard deviational ellipse\n\nShape Analysis\n\nhull: calculate the convex hull of the point pattern\nmbr: calculate the minimum bounding box (rectangle)\n\n\nAll of the above functions operate on a series of coordinate pairs. That is, the data type of the first argument should be \\((n,2)\\) array_like. In case that you have a point pattern (PointPattern instance), you need to pass its attribute “points” instead of itself to these functions.\n\nimport numpy as np\nfrom pointpats import PointPattern\n%matplotlib inline\nimport matplotlib.pyplot as plt\npoints = [[66.22, 32.54], [22.52, 22.39], [31.01, 81.21],\n          [9.47, 31.02],  [30.78, 60.10], [75.21, 58.93],\n          [79.26,  7.68], [8.23, 39.93],  [98.73, 77.17],\n          [89.78, 42.53], [65.19, 92.08], [54.46, 8.48]]\npp = PointPattern(points) #create a point pattern \"pp\" from list\npp.points \n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n66.22\n32.54\n\n\n1\n22.52\n22.39\n\n\n2\n31.01\n81.21\n\n\n3\n9.47\n31.02\n\n\n4\n30.78\n60.10\n\n\n5\n75.21\n58.93\n\n\n6\n79.26\n7.68\n\n\n7\n8.23\n39.93\n\n\n8\n98.73\n77.17\n\n\n9\n89.78\n42.53\n\n\n10\n65.19\n92.08\n\n\n11\n54.46\n8.48\n\n\n\n\n\n\n\n\ntype(pp.points)\n\npandas.core.frame.DataFrame\n\n\nWe can use PointPattern class method plot to visualize pp.\n\npp.plot()\n\n\n\n\n\nfrom pointpats.centrography import (hull, mbr, mean_center,\n                                    weighted_mean_center, manhattan_median,\n                                    std_distance,euclidean_median,ellipse)"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#mean-center-x_mcy_mc",
    "href": "lectures/week-10/2023-10-23-centrography.html#mean-center-x_mcy_mc",
    "title": "Centrography for Point Patterns",
    "section": "Mean Center \\((x_{mc},y_{mc})\\)",
    "text": "Mean Center \\((x_{mc},y_{mc})\\)\n\\[x_{mc}=\\frac{1}{n} \\sum^n_{i=1}x_i\\] \\[y_{mc}=\\frac{1}{n} \\sum^n_{i=1}y_i\\]\n\nmc = mean_center(pp.points)\nmc\n\narray([52.57166667, 46.17166667])\n\n\n\npp.plot()\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff77d9bb150&gt;"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#weighted-mean-center-x_wmcy_wmc",
    "href": "lectures/week-10/2023-10-23-centrography.html#weighted-mean-center-x_wmcy_wmc",
    "title": "Centrography for Point Patterns",
    "section": "Weighted Mean Center \\((x_{wmc},y_{wmc})\\)",
    "text": "Weighted Mean Center \\((x_{wmc},y_{wmc})\\)\n\\[x_{wmc}=\\sum^n_{i=1} \\frac{w_i x_i}{\\sum^n_{i=1}w_i}\\] \\[y_{wmc}=\\sum^n_{i=1} \\frac{w_i y_i}{\\sum^n_{i=1}w_i}\\]\nThe Weighted mean center is meant for marked point patterns. Aside from the first argument which is a series of \\((x,y)\\) coordinates in weighted_mean_center function, we need to specify its second argument which is the weight for each event point.\n\nweights = np.arange(12)\nweights\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\n\n\nwmc = weighted_mean_center(pp.points, weights)\nwmc\n\narray([60.51681818, 47.76848485])\n\n\n\npp.plot() #use class method \"plot\" to visualize point pattern\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center') \nplt.plot(wmc[0], wmc[1], 'gd', label='Weighted Mean Center')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff775806750&gt;"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#manhattan-median-x_mmy_mm",
    "href": "lectures/week-10/2023-10-23-centrography.html#manhattan-median-x_mmy_mm",
    "title": "Centrography for Point Patterns",
    "section": "Manhattan Median \\((x_{mm},y_{mm})\\)",
    "text": "Manhattan Median \\((x_{mm},y_{mm})\\)\n\\[min  f(x_{mm},y_{mm})= \\sum^n_{i=1}(|x_i-x_{mm}|+|y_i-y_{mm}|)\\]\nThe Manhattan median is the location which minimizes the absolute distance to all the event points. It is an extension of the median measure in one-dimensional space to two-dimensional space. Since in one-dimensional space, a median is the number separating the higher half of a dataset from the lower half, we define the Manhattan median as a tuple whose first element is the median of \\(x\\) coordinates and second element is the median of \\(y\\) coordinates.\nThough Manhattan median can be found very quickly, it is not unique if you have even number of points. In this case, pysal handles the Manhattan median the same way as numpy.median: return the average of the two middle values.\n\n#get the number of points in point pattern \"pp\"\npp.n\n\n12\n\n\n\n#Manhattan Median is not unique for \"pp\"\nmm = manhattan_median(pp.points)\nmm\n\n/home/serge/miniforge3/envs/385f23/lib/python3.11/site-packages/pointpats/centrography.py:208: UserWarning: Manhattan Median is not unique for even point patterns.\n  warnings.warn(s)\n\n\narray([59.825, 41.23 ])\n\n\n\npp.plot()\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center')\nplt.plot(wmc[0], wmc[1], 'gd', label='Weighted Mean Center')\nplt.plot(mm[0], mm[1], 'rv', label='Manhattan Median')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff77580df90&gt;"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#euclidean-median-x_emy_em",
    "href": "lectures/week-10/2023-10-23-centrography.html#euclidean-median-x_emy_em",
    "title": "Centrography for Point Patterns",
    "section": "Euclidean Median \\((x_{em},y_{em})\\)",
    "text": "Euclidean Median \\((x_{em},y_{em})\\)\n\\[min  f(x_{em},y_{em})= \\sum^n_{i=1} \\sqrt{(x_i-x_{em})^2+(y_i-y_{em})^2}\\]\nThe Euclidean Median is the location from which the sum of the Euclidean distances to all points in a distribution is a minimum. It is an optimization problem and very important for more general location allocation problems. There is no closed form solution. We can use first iterative algorithm (Kuhn and Kuenne, 1962) to approximate Euclidean Median.\nBelow, we define a function named median_center with the first argument points a series of \\((x,y)\\) coordinates and the second argument crit the convergence criterion.\n\ndef median_center(points, crit=0.0001):\n    points = np.asarray(points)\n    x0, y0 = points.mean(axis=0)\n    dx = np.inf\n    dy = np.inf\n    iteration = 0\n    while np.abs(dx) &gt; crit or np.abs(dy) &gt; crit:\n        xd = points[:, 0] - x0\n        yd = points[:, 1] - y0\n        d = np.sqrt(xd*xd + yd*yd)\n        w = 1./d\n        w = w / w.sum()\n        x1 = w * points[:, 0]\n        x1 = x1.sum()\n        y1 = w * points[:, 1]\n        y1 = y1.sum()\n        dx = x1 - x0\n        dy = y1 - y0\n        iteration +=1 \n        print(x0, x1, dx, dy, d.sum(), iteration)\n        x0 = x1\n        y0 = y1\n               \n    return x1, y1\n\n\nmedian_center(pp.points, crit=.0001)\n\n52.57166666666668 53.178128280602785 0.606461613936105 -0.9290354286335258 466.24479074356606 1\n53.178128280602785 53.56643624463614 0.388307964033352 -0.4199402653980684 465.9311160558993 2\n53.56643624463614 53.80720376806838 0.24076752343224683 -0.1974862190386233 465.84555867343346 3\n53.80720376806838 53.95348076207835 0.1462769940099662 -0.09642613786996179 465.8197750145871 4\n53.95348076207835 54.04117257066307 0.08769180858472225 -0.04872250646902643 465.8115372002813 5\n54.04117257066307 54.09327726928146 0.05210469861838618 -0.025370793047137852 465.80882301324334 6\n54.09327726928146 54.12405125525861 0.030773985977148755 -0.013552246205456697 465.8079149010591 7\n54.12405125525861 54.14215248769505 0.018101232436443127 -0.00739190209046825 465.8076087750224 8\n54.14215248769505 54.15276956049696 0.010617072801906602 -0.0040992658298719675 465.8075052025632 9\n54.15276956049696 54.15898467957115 0.0062151190741914775 -0.0023026998071102867 465.80747009858044 10\n54.15898467957115 54.16261796248172 0.0036332829105703013 -0.0013061853179365812 465.80745819050844 11\n54.16261796248172 54.16473989468326 0.002121932201539778 -0.0007463404183738476 465.80745414933307 12\n54.16473989468326 54.165978319450346 0.00123842476708802 -0.00042875101595285514 465.80745277762423 13\n54.165978319450346 54.166700756153695 0.0007224367033487056 -0.00024727631074483725 465.80745231197506 14\n54.166700756153695 54.16712204754273 0.0004212913890384584 -0.00014302182778891392 465.8074521538953 15\n54.16712204754273 54.16736766581608 0.00024561827334679265 -8.289363293556562e-05 465.8074521002288 16\n54.16736766581608 54.167510839857464 0.0001431740413835314 -4.8115880247223686e-05 465.80745208200943 17\n54.167510839857464 54.167594287646125 8.344778866131719e-05 -2.7959041396741213e-05 465.807452075824 18\n\n\n(54.167594287646125, 44.42430865883205)\n\n\nAfter 18 iterations, the convergence criterion is reached. The Euclidean Median is \\((54.167594287646125,44.424308658832047)\\).\nWe can also call the function euclidean_median in pysal to calculate the Euclidean Median.\n\nem = euclidean_median(pp.points)\nem\n\narray([54.16773427, 44.42425854])\n\n\nThe two results we get from euclidean_median function in pysal and the median_center function we define here are very much the same.\n\npp.plot()\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center')\nplt.plot(wmc[0], wmc[1], 'gd', label='Weighted Mean Center')\nplt.plot(mm[0], mm[1], 'rv', label='Manhattan Median')\nplt.plot(em[0], em[1], 'm+', label='Euclidean Median')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff7756f6690&gt;"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#standard-distance-standard-distance-circle",
    "href": "lectures/week-10/2023-10-23-centrography.html#standard-distance-standard-distance-circle",
    "title": "Centrography for Point Patterns",
    "section": "Standard Distance & Standard Distance Circle",
    "text": "Standard Distance & Standard Distance Circle\n\\[SD = \\displaystyle \\sqrt{\\frac{\\sum^n_{i=1}(x_i-x_{m})^2}{n} + \\frac{\\sum^n_{i=1}(y_i-y_{m})^2}{n}}\\]\nThe Standard distance is closely related to the usual definition of the standard deviation of a data set, and it provides a measure of how dispersed the events are around their mean center \\((x_m,y_m)\\). Taken together, these measurements can be used to plot a summary circle (standard distance circle) for the point pattern, centered at \\((x_m,y_m)\\) with radius \\(SD\\), as shown below.\n\nstdd = std_distance(pp.points)\nstdd\n\n40.14980648908671\n\n\nPlot mean center as well as the standard distance circle.\n\ncircle1=plt.Circle((mc[0], mc[1]),stdd,color='r')\nax = pp.plot(get_ax=True, title='Standard Distance Circle')\nax.add_artist(circle1)\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center')\nax.set_aspect('equal')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff775806690&gt;\n\n\n\n\n\nFrom the above figure, we can observe that there are five points outside the standard distance circle which are potential outliers."
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#standard-deviational-ellipse",
    "href": "lectures/week-10/2023-10-23-centrography.html#standard-deviational-ellipse",
    "title": "Centrography for Point Patterns",
    "section": "Standard Deviational Ellipse",
    "text": "Standard Deviational Ellipse\nCompared with standard distance circle which measures dispersion using a single parameter \\(SD\\), standard deviational ellipse measures dispersion and trend in two dimensions through angle of rotation \\(\\theta\\), dispersion along major axis \\(s_x\\) and dispersion along minor axis \\(s_y\\):\n\nMajor axis defines the direction of maximum spread in the distribution. \\(s_x\\) is the semi-major axis (half the length of the major axis):\n\n\\[ s_x = \\displaystyle \\sqrt{\\frac{2(\\sum_{i=1}^n (x_i-\\bar{x})\\cos(\\theta) - \\sum_{i=1}^n (y_i-\\bar{y})\\sin(\\theta))^2}{n-2}}\\]\n\nMinor axis defines the direction of minimum spread and is orthogonal to major axis. \\(s_y\\) is the semi-minor axis (half the length of the minor axis):\n\n\\[ s_y = \\displaystyle \\sqrt{\\frac{2(\\sum_{i=1}^n (x_i-\\bar{x})\\sin(\\theta) - \\sum_{i=1}^n (y_i-\\bar{y})\\cos(\\theta))^2}{n-2}}\\]\n\nThe ellipse is rotated clockwise through an angle \\(\\theta\\):\n\n\\[\\theta = \\displaystyle \\arctan{\\{ (\\sum_i(x_i-\\bar{x})^2-\\sum_i(y_i-\\bar{y})^2) + \\frac{[(\\sum_i(x_i-\\bar{x})^2-\\sum_i(y_i-\\bar{y})^2)^2 + 4(\\sum_i(x-\\bar{x})(y_i-\\bar{y}))^2]^\\frac{1}{2}}{2\\sum_i(x-\\bar{x})(y_i-\\bar{y})}\\}}\\]\n\nsx, sy, theta = ellipse(pp.points)\nsx, sy, theta\n\n(39.62386788646298, 42.753818949026815, 1.1039268428650906)\n\n\n\ntheta_degree = np.degrees(theta) #need degree of rotation to plot the ellipse\ntheta_degree\n\n63.250348987371304\n\n\nThe Standard Deviational Ellipse for the point pattern is rotated clockwise by \\(63.25^{\\circ}\\).\n\nfrom matplotlib.patches import Ellipse\nfrom pylab import figure, show,rand\nfig = figure()\n#ax = fig.add_subplot(111, aspect='equal')\ne = Ellipse(xy=mean_center(pp.points), width=sx*2, height=sy*2, angle=-theta_degree) #angle is rotation in degrees (anti-clockwise)\nax = pp.plot(get_ax=True, title='Standard Deviational Ellipse')\nax.add_artist(e)\ne.set_clip_box(ax.bbox)\ne.set_facecolor([0.8,0,0])\ne.set_edgecolor([1,0,0])\nax.set_xlim(0,100)\nax.set_ylim(0,100)\nax.set_aspect('equal')\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center')\nplt.legend(numpoints=1)\nshow()\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#convex-hull",
    "href": "lectures/week-10/2023-10-23-centrography.html#convex-hull",
    "title": "Centrography for Point Patterns",
    "section": "Convex Hull",
    "text": "Convex Hull\nThe convex hull of a point pattern pp is the smallest convex set that contains pp. We can call function hull to caculate the convex hull.\n\nhull(pp.points)\n\narray([[31.01, 81.21],\n       [ 8.23, 39.93],\n       [ 9.47, 31.02],\n       [22.52, 22.39],\n       [54.46,  8.48],\n       [79.26,  7.68],\n       [89.78, 42.53],\n       [98.73, 77.17],\n       [65.19, 92.08]])\n\n\nBy specifying “hull” argument True in PointPattern class method plot, we can easily plot convex hull of the point pattern.\n\npp.plot(title='Centers', hull=True ) #plot point pattern \"pp\" as well as its convex hull\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center')\nplt.plot(wmc[0], wmc[1], 'gd', label='Weighted Mean Center')\nplt.plot(mm[0], mm[1], 'rv', label='Manhattan Median')\nplt.plot(em[0], em[1], 'm+', label='Euclidean Median')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff77de86690&gt;"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#minimum-bounding-rectangle",
    "href": "lectures/week-10/2023-10-23-centrography.html#minimum-bounding-rectangle",
    "title": "Centrography for Point Patterns",
    "section": "Minimum Bounding Rectangle",
    "text": "Minimum Bounding Rectangle\nMinimum Bounding Rectangle (Box) is the same as the minimum bounding Rectangle of its convex hull. Thus, it is almost always bigger than convex hull.\nWe can call mbr function to calculate the leftmost, downmost, rightmost, and upmost value of the vertices of minimum bounding rectangle.\n\nmbr(pp.points)\n\n/tmp/ipykernel_352080/2243439823.py:1: FutureWarning: This function will be deprecated in the next release of pointpats.\n  mbr(pp.points)\n\n\n(8.23, 7.68, 98.73, 92.08)\n\n\nThus, four vertices of the minimum bounding rectangle is \\((8.23,7.68),(98.73,7.68),(98.73,92.08),(8.23,92.08)\\).\n\npp.plot(title='Centers', window=True ) #plot point pattern \"pp\" as well as its Minimum Bounding Rectangle\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center')\nplt.plot(wmc[0], wmc[1], 'gd', label='Weighted Mean Center')\nplt.plot(mm[0], mm[1], 'rv', label='Manhattan Median')\nplt.plot(em[0], em[1], 'm+', label='Euclidean Median')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff775686bd0&gt;\n\n\n\n\n\n\npp.plot(title='Centers',  hull=True , window=True )#plot point pattern \"pp\", convex hull, and Minimum Bounding Rectangle\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center')\nplt.plot(wmc[0], wmc[1], 'gd', label='Weighted Mean Center')\nplt.plot(mm[0], mm[1], 'rv', label='Manhattan Median')\nplt.plot(em[0], em[1], 'm+', label='Euclidean Median')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff775531e90&gt;\n\n\n\n\n\nPlot Standard Distance Circle and Convex Hull.\n\ncircle1=plt.Circle((mc[0], mc[1]),stdd,color='r',alpha=0.2)\nax = pp.plot(get_ax=True, title='Standard Distance Circle', hull=True)\nax.add_artist(circle1)\nplt.plot(mc[0], mc[1], 'b^', label='Mean Center')\nax.set_aspect('equal')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff77559ebd0&gt;"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#simulate-a-100-point-dataset-within-va-state-border-from-a-csr-complete-spatial-randomness-process.",
    "href": "lectures/week-10/2023-10-23-centrography.html#simulate-a-100-point-dataset-within-va-state-border-from-a-csr-complete-spatial-randomness-process.",
    "title": "Centrography for Point Patterns",
    "section": "Simulate a 100-point dataset within VA state border from a CSR (complete spatial randomness) process.",
    "text": "Simulate a 100-point dataset within VA state border from a CSR (complete spatial randomness) process.\n\npp = csr(as_window(state), 100, 1, asPP=True).realizations[0]\npp.plot(window=True)\n\n\n\n\n\npp.plot(window=True, hull=True)\n\n\n\n\n\nmc = mean_center(pp.points)\nmm = manhattan_median(pp.points)\nem = euclidean_median(pp.points)\npp.plot(title='Centers',  hull=True , window=True )#plot point pattern \"pp\", convex hull, and Minimum Bounding Rectangle\nplt.plot(mc[0], mc[1], 'c^', label='Mean Center')\nplt.plot(mm[0], mm[1], 'rv', label='Manhattan Median')\nplt.plot(em[0], em[1], 'm+', label='Euclidean Median')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff7751d9190&gt;\n\n\n\n\n\nPlot Standard Distance Circle of the simulated point pattern.\n\nsx, sy, theta = ellipse(pp.points)\nsx, sy, theta\ntheta_degree = np.degrees(theta) #need degree of rotation to plot the ellipse\nfrom matplotlib.patches import Ellipse\nfrom pylab import figure, show,rand\nfig = figure()\n#ax = fig.add_subplot(111, aspect='equal')\ne = Ellipse(xy=mean_center(pp.points), width=sx*2, height=sy*2, angle=-theta_degree)\nax = pp.plot(get_ax=True, title='Standard Deviational Ellipse')\nax.add_artist(e)\ne.set_clip_box(ax.bbox)\ne.set_facecolor([0.8,0,0])\ne.set_edgecolor([1,0,0])\nax.set_xlim(300000,1000000)\nax.set_ylim(4050000,4350000)\n#ax.set_aspect('equal')\nplt.plot(mc[0], mc[1], 'c^', label='Mean Center')\nplt.legend(numpoints=1)\nshow()\n\n&lt;Figure size 672x480 with 0 Axes&gt;"
  },
  {
    "objectID": "lectures/week-10/2023-10-23-centrography.html#simulate-a-500-point-dataset-within-va-state-border-from-a-csr-complete-spatial-randomness-process.",
    "href": "lectures/week-10/2023-10-23-centrography.html#simulate-a-500-point-dataset-within-va-state-border-from-a-csr-complete-spatial-randomness-process.",
    "title": "Centrography for Point Patterns",
    "section": "Simulate a 500-point dataset within VA state border from a CSR (complete spatial randomness) process.",
    "text": "Simulate a 500-point dataset within VA state border from a CSR (complete spatial randomness) process.\n\npp = csr(as_window(state), 500, 1, asPP=True).realizations[0]\npp.plot(window=True)\n\n\n\n\n\npp.plot(window=True, hull=True)\n\n\n\n\n\nmc = mean_center(pp.points)\nmm = manhattan_median(pp.points)\nem = euclidean_median(pp.points)\npp.plot(title='Centers',  hull=True , window=True )#plot point pattern \"pp\", convex hull, and Minimum Bounding Rectangle\nplt.plot(mc[0], mc[1], 'c^', label='Mean Center')\nplt.plot(mm[0], mm[1], 'rv', label='Manhattan Median')\nplt.plot(em[0], em[1], 'm+', label='Euclidean Median')\nplt.legend(numpoints=1)\n\n&lt;matplotlib.legend.Legend at 0x7ff7750fdd90&gt;\n\n\n\n\n\n\nsx, sy, theta = ellipse(pp.points)\nsx, sy, theta\ntheta_degree = np.degrees(theta) #need degree of rotation to plot the ellipse\nfrom matplotlib.patches import Ellipse\nfrom pylab import figure, show,rand\nfig = figure()\n#ax = fig.add_subplot(111, aspect='equal')\ne = Ellipse(xy=mean_center(pp.points), width=sx*2, height=sy*2, angle=-theta_degree)\nax = pp.plot(get_ax=True, title='Standard Deviational Ellipse')\nax.add_artist(e)\ne.set_clip_box(ax.bbox)\ne.set_facecolor([0.8,0,0])\ne.set_edgecolor([1,0,0])\nax.set_xlim(300000,1000000)\nax.set_ylim(4050000,4350000)\n#ax.set_aspect('equal')\nplt.plot(mc[0], mc[1], 'c^', label='Mean Center')\nplt.legend(numpoints=1)\nshow()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\nIf we calculate the Euclidean distances between every event point and Mean Center (Euclidean Median), and sum them up, we can see that Euclidean Median is the optimal point in iterms of minimizing the Euclidean distances to all the event points.\n\nfrom pointpats import dtot\nprint(dtot(mc, pp.points))\nprint(dtot(em, pp.points))\nprint(dtot(mc, pp.points) &gt; dtot(em, pp.points))\n\n72078532.41999626\n71947972.97009483\nTrue"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html",
    "href": "lectures/week-01/2023-08-23.html",
    "title": "Markdown",
    "section": "",
    "text": "3 + 7\n\n10\nprint(\"hello world\")\n\nhello world\nx = 3 + 7\nx\n\n10\nprint(x)\n\n10\nprint(\"hello world\")\n\nhello world\nThis is a markdown cell.\nSo what?\nWell we can do things like bold or italics.\nHow about bold and italics"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#math",
    "href": "lectures/week-01/2023-08-23.html#math",
    "title": "Markdown",
    "section": "Math",
    "text": "Math\nThe formulae for the sample mean is:\n\\[\n\\bar{x} = 1/n \\sum_{i=1}^n x_i\n\\]"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#markdown",
    "href": "lectures/week-01/2023-08-23.html#markdown",
    "title": "Markdown",
    "section": "Markdown",
    "text": "Markdown\nThis is a markdown cell.\nSo what?\nWell we can do things like bold or italics.\nHow about bold and italics\n\nSubsection\nNow we are in a subsection.\n\n\nLists\nHere is stuff .\nWe can do unordered lists:\n\nfirst\nsecond\nthird\n\nOr numbered lists\n\nfirst\nsecond\nthird"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#math-1",
    "href": "lectures/week-01/2023-08-23.html#math-1",
    "title": "Markdown",
    "section": "Math",
    "text": "Math\nThe formulae for the sample mean is:\n\\[\n\\bar{x} = 1/n \\sum_{i=1}^n x_i\n\\]"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#links",
    "href": "lectures/week-01/2023-08-23.html#links",
    "title": "Markdown",
    "section": "Links",
    "text": "Links\nThe big search engine in the sky is google"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#markdown-1",
    "href": "lectures/week-01/2023-08-23.html#markdown-1",
    "title": "Markdown",
    "section": "Markdown",
    "text": "Markdown\nThis is a markdown cell.\nSo what?\nWell we can do things like bold or italics.\nHow about bold and italics\n\nSubsection\nNow we are in a subsection.\n\n\nLists\nHere is stuff .\nWe can do unordered lists:\n\nfirst\nsecond\nthird\n\nOr numbered lists\n\nfirst\nsecond\nthird"
  },
  {
    "objectID": "lectures/week-01/2023-08-23.html#math-2",
    "href": "lectures/week-01/2023-08-23.html#math-2",
    "title": "Markdown",
    "section": "Math",
    "text": "Math\n\n10 * 4\n\n40"
  },
  {
    "objectID": "lectures/week-07/2023-10-02.html",
    "href": "lectures/week-07/2023-10-02.html",
    "title": "Join Counts",
    "section": "",
    "text": "import geopandas\n\n/tmp/ipykernel_482139/1529612126.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n\nimport os\nos.environ['USE_PYGEOS'] = '0'\nimport geopandas\n\nIn the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n  import geopandas\n\n\n\ngdf = geopandas.read_parquet('repub_lean.parquet')\n\n\ngdf.plot()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ngdf.plot(column='rep_int', cmap='Reds', figsize=(16,9))\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nf, axs = plt.subplots(1, 2, figsize=(16, 9))\nax1, ax2 = axs\n\n\n\ngdf.plot(column='rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax1,\n        )\nax1.set_axis_off()\nax1.set_title(\"Leaning Republican\")\n\n\ngdf.plot(column='lag_rep_int',\n         cmap='Reds',\n         scheme='quantiles',\n         k=5,\n         edgecolor='grey',\n         linewidth=0.1,\n         alpha=0.75,\n         legend=True,\n         legend_kwds={'loc': 'lower left'},\n         ax =ax2,\n        )\nax2.set_axis_off()\nax2.set_title(\"Spatial Lag Leaning Republican\")\n\nText(0.5, 1.0, 'Spatial Lag Leaning Republican')\n\n\n\n\n\n\nimport seaborn as sns\n_ = sns.regplot(x='rep_int', y='lag_rep_int', data=gdf)\nplt.axhline(y=gdf.lag_rep_int.mean(), color='g', linestyle='--')\nplt.axvline(x=gdf.rep_int.mean(), color='g', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7ff04fd076a0&gt;\n\n\n\n\n\n\ngdf.rep_int.median()\n\n41.0\n\n\n\nhi_rep = gdf.rep_int &gt;= gdf.rep_int.median()\nhi_rep\n\n0     False\n1     False\n2     False\n3      True\n4     False\n5     False\n6     False\n7      True\n8      True\n9     False\n10    False\n11     True\n12    False\n13     True\n14    False\n15     True\n16    False\n17     True\n18    False\n19     True\n20    False\n21     True\n22    False\n23    False\n24    False\n25    False\n26     True\n27     True\n28    False\n29     True\n30     True\n31     True\n32     True\n33     True\n34    False\n35     True\n36     True\n37     True\n38    False\n39    False\n40     True\n41     True\n42    False\n43     True\n44     True\n45    False\n46     True\n47     True\n48     True\nName: rep_int, dtype: bool\n\n\n\ngdf['hi_rep'] = hi_rep\n\n\ngdf.plot(column='hi_rep', categorical=True, legend=True, figsize=(16,9), cmap='Greys', edgecolor='grey')\n_ = plt.title(\"High Republican Leaning States\")\n\n\n\n\n\ngdf.hi_rep.sum() # number of high republican leaning states\n\n26\n\n\n\ngdf.shape[0] -  gdf.hi_rep.sum() # number of low republican leaning states\n\n23\n\n\n\nimport libpysal\nimport esda\n\n\nimport numpy\nrids = numpy.random.permutation(gdf.index.values)\n\n\nrids\n\narray([ 1,  0, 12, 14, 30, 24,  5,  2, 25, 28, 13, 44, 35, 38, 40, 41, 16,\n       32, 42, 36, 10, 26,  3, 31,  8, 43, 47, 33, 17, 29, 27, 19,  9,  4,\n       48, 21, 23, 37, 20, 45, 22, 15, 34, 46, 18, 11, 39,  7,  6])\n\n\n\ngdf['random_values'] = gdf.hi_rep.values[rids]\n\n\ngdf.plot(column='random_values', categorical=True, legend=True, figsize=(16,9), cmap='Greys', edgecolor='grey')\n_ = plt.title(\"High Republican Leaning States (Randomized)\")\n\n\n\n\n\nw = libpysal.weights.Queen.from_dataframe(gdf)\n\n\nw.n\n\n49\n\n\n\nnumpy.random.seed(12345)\njc = esda.join_counts.Join_Counts(gdf.hi_rep, w)\n\n\njc.bb\n\n52.0\n\n\n\njc.autocorr_pos # BB + WW\n\n78.0\n\n\n\njc.expected\n\n\n\n\n\n\n\nNeighbor\nW\nB\n\n\nFocal\n\n\n\n\n\n\nW\n15.800459\n25.699541\n\n\nB\n25.699541\n41.800459\n\n\n\n\n\n\n\n\njc.crosstab\n\n\n\n\n\n\n\nNeighbor\nW\nB\n\n\nFocal\n\n\n\n\n\n\nW\n26.0\n15.5\n\n\nB\n15.5\n52.0\n\n\n\n\n\n\n\n\njc.p_sim_autocorr_pos\n\n0.001\n\n\n\nimport seaborn\n\n\nax = seaborn.histplot(jc.sim_autocurr_pos)\n_ = plt.axvline(jc.autocorr_pos, 0, 80, color='r')\n\n\n\n\n\njc_r = esda.join_counts.Join_Counts(gdf.random_values, w)\n\n\njc_r.crosstab\n\n\n\n\n\n\n\nNeighbor\nW\nB\n\n\nFocal\n\n\n\n\n\n\nW\n21.0\n30.0\n\n\nB\n30.0\n28.0\n\n\n\n\n\n\n\n\njc_r.expected\n\n\n\n\n\n\n\nNeighbor\nW\nB\n\n\nFocal\n\n\n\n\n\n\nW\n23.862385\n27.137615\n\n\nB\n27.137615\n30.862385\n\n\n\n\n\n\n\n\nax = seaborn.histplot(jc_r.sim_autocurr_pos)\n_ = plt.axvline(jc_r.autocorr_pos, 0, 80, color='r')"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]